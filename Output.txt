<Paper ID = 1>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Efﬁcient sentence embedding using discrete cosine transform. In Proceedings of the 2019 Con- from the amazon dataset, as deemed by our ap- ferenceonEmpiricalMethodsinNaturalLanguage proach, in Table 5. As can be seen, our method Processing and the 9th International Joint Confer- performsquitewellinmatchingcomplexsentences ence on Natural Language Processing (EMNLP- withvaryingtopicsandsentimentstotheirclosest IJCNLP),pages3672–3678,HongKong,China. </Extractive Summary>  </Table ID = 5>  </Paper ID = 1> 

<Paper ID = 2>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Retrieval performance. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Fordetails,seeAppendix. Table 2. For additional results, see Table 4 in Appendix. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Additional retrieval performance using self-critic (Rennie et al., 2017) baseline in the policy gradient, applicabletodatasetswithnooraclequestions. </Abstractive Summary>  <Extractive Summary> =  Table 2. For additional results, see Table 4 in Appendix. 4 RelatedWork Model performance v.s. withthesamecount,werandomlychooseone. Additional experiment results using a different baseline - self-critic (Rennie et al., 2017) - is Algorithm1Top-TFIDF@K shown in Table 4. This shows that our proposed Input: D+,CorpusC modelframeworkiseffectiveevenwithanyofthe Foreachd ∈ D+,retrievetop-KquestionsinC; two policy gradient baselines. </Extractive Summary>  </Table ID = 4>  </Paper ID = 2> 

<Paper ID = 3>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Averagesurprisal(inbits)of EOW vs. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 3> 

<Paper ID = 4>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  This is in line with previous Section4). 4https://dumps.wikimedia.org/other/ ResultsandAnalysis WereportF scoresover 1 kiwix/zim/wikipedia/wikipedia_en_all_ the test set in Table 2. The main ﬁnding is that maxi_2020-06.zim 5https://github.com/openzim/libzim imagesimproveperformanceinallsettings,forall 6Becauseitisquitedifﬁculttoﬁndcorrespondencesbe- languages and both image representations. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Comparison of F1 scores over the full SHINRAdatasetforBERT . </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  Table 3: F scores for pre-trained VL-BERT. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  To 24ktrainingsetusedinSection3. testthis,weperformadditionalexperimentsvary- The results in Table 5 shows that even for ingthetrainingdatasize,rangingfrom4kto24k VL-BERT, a neural-based model that is much traininginstances,instepsof4k. morecomplexthanthelinear-kernelSVM,when Figure2plotstheF performanceasthetraining BERT isnotused,imagesprovideagaininper- 1 pre set size increases. </Extractive Summary>  </Table ID = 5>  </Paper ID = 4> 

<Paper ID = 5>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  format. Table 1 presents an overview of the different 2.2 Modelling datasets and highlights important differences be- KatiyarandCardie(2016)explorejointlyextract- tween them. The fully ﬁne-grained sentiment ingholders,targets,andexpressionswithLSTMs. </Extractive Summary>  </Table ID = 1>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Finally,the have suggested that there is only a weak relation- challenge dataset is by far the largest with over ship between target extraction and polarity classi- 11,000 training targets. Additionally, Table 6 in ﬁcation(Huetal.,2019). Appendix A shows the percentage of unique tar- gets per dataset, as well as the percentage of tar- 3 Data gets shared between the training set and the dev and test sets. </Extractive Summary>  </Table ID = 6>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The experiments on and expression tags (+POL. in Table 3) tends to thetargeteddatasets,ontheotherhand,willshow decreaseperformance. us whether it is possible to improve the targeted 5.2 Polarityclassiﬁcationwithgold modelswithpredictedexpressions. Finally,+fullperformsbetweentheorigi- tions (PRED. in Table 3) leads to mixed results nalinputand+exp. – the model leads to improvements on MPQA + exp. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  us whether it is possible to improve the targeted 5.2 Polarityclassiﬁcationwithgold modelswithpredictedexpressions. annotations 5 Results Table 4 shows the macro F scores for the po- 1 larity classiﬁcation task on the gold targets. The In this section we describe the main results from model performs better than the best reported re- the extraction and two classiﬁcation experiments sultsonChallenge(Jiangetal.,2019),andsimilar describedinSection4. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Macro F scores for polarityclassiﬁcation of gold targets. </Abstractive Summary>  <Extractive Summary> =  the trained expression prediction models. The Augmenting the input text with predicted sen- fact that the dataset that beneﬁts most is the TD- timent expressions leads to losses in 41 out of Parsedatasetsuggeststhatexpressioninformation averaged 56 experiments shown in Figure 3 (or ismostusefulwhentherearemultipletargetswith in 173 out of 280 experiments in Table 5). Cu- multiplepolarities. </Extractive Summary>  </Table ID = 5>  </Paper ID = 5> 

<Paper ID = 6>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Results on the validation set of SuperGLUE longbeensolved,itneverthelessservesasauseful benchmarkdataset,withBert-large-casedmodel. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Ablation of choice of hyper-parameter num- large unlabelled data, and was used by Noisy- berof innersteps k forour methodCBST +Reptile+ Student(Xieetal.,2019)recentlytoachievestate- l2sponSQuADwithBERT-base. </Abstractive Summary>  <Extractive Summary> =  our model. As shows in Table 8, the number of innerupdatesdoesnothaveamajorimpactonthe We further study the effect of the thresholding results,butweadviseitbekeptlessthanorequal fraction f used to select the subset of conﬁdent to 4 as higher inner steps reduce the number of data. We use the pre-trained Bert-base-uncased outerupdates(asthetotalnumberofepochsiskept model,self-trainedonthetrainingsetofNewsQA constant). </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  Table 9: Performance of BERT-base on SQuAD, af- of-the-art,suchasMerityetal.(2018). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 9>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  IEEEComputerSociety. A.1 Signiﬁcancetests Alex Wang, Yada Pruksachatkun, Nikita Nangia, We provide below in Table 10 and Table 12 the AmanpreetSingh, JulianMichael, FelixHill, Omer Levy,andSamuelR.Bowman.2019. Superglue: A P-valuesforone-sampleT-testfortheTable6and stickierbenchmarkforgeneral-purposelanguageun- Table 7, with the null hypothesis that the scores derstanding systems. IEEEComputerSociety. A.1 Signiﬁcancetests Alex Wang, Yada Pruksachatkun, Nikita Nangia, We provide below in Table 10 and Table 12 the AmanpreetSingh, JulianMichael, FelixHill, Omer Levy,andSamuelR.Bowman.2019. Superglue: A P-valuesforone-sampleT-testfortheTable6and stickierbenchmarkforgeneral-purposelanguageun- Table 7, with the null hypothesis that the scores derstanding systems. Asafurthertestofimprovedgeneralization,we E CorrespondingValidationsetresults split the squad dev set in two equal halves, per- forTestSet formedourself-trainingononehalf,andevaluated on the other half. Scores in Table 11 show, self- Our only reported test scores are on MNIST, trainingononehalfimprovedgeneralizationonthe NewsQA,andUbuntuDialogCorpus. ForMNIST, other. </Extractive Summary>  </Table ID = 1>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  A.1 Signiﬁcancetests Alex Wang, Yada Pruksachatkun, Nikita Nangia, We provide below in Table 10 and Table 12 the AmanpreetSingh, JulianMichael, FelixHill, Omer Levy,andSamuelR.Bowman.2019. Superglue: A P-valuesforone-sampleT-testfortheTable6and stickierbenchmarkforgeneral-purposelanguageun- Table 7, with the null hypothesis that the scores derstanding systems. In Advances in Neural Infor- ofourresultshavethesamemeanasthebaseline. </Extractive Summary>  </Table ID = 7>  </Paper ID = 6> 

<Paper ID = 7>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Description for the Persuasion (P4G) (Wang apositiveassociationbetweenworkingknowledge et al., 2019) and Negotiation (CB) (He et al., 2018) and one’s ability to resist persuasion (Wood and datasets Kallgren,1988;LuttrellandSawicki,2020). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Examples of annotation snippets for the Per- giesasasequencelabellingtask. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Here we describe the search-space of all the Inthissection,wedescribethebaselinesandevalu- hyper-parametersusedinourexperimentsanddescribe ationmetrics. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  Table 7: We observe the impact of incorporating se- quence of strategies on conversation outcome predic- Table6: ResultsofRESPERandotherbaselinesonthe tionintermsofMacro-F1andWeighted-F1score. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  </Paper ID = 7> 

<Paper ID = 8>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Ittakesasinputthe STS-B (Cer et al., 2017), in GLUE, we addition- hidden state h and outputs the certainty level u i i allyuseanotherregressiondataset,SICK(Marelli ofthesampleattheith layer: etal.,2014). Statisticsofthesedatasetsarelisted in Table 1. Our implementation is adapted from u = σ(c(cid:62)h +b), (8) i i theHuggingfaceTransformerLibrary(Wolfetal., 2020). </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Test set results comparing baselines (raw BERT/RoBERTa, from the original paper), DistilBERT, and earlyexitingwithAlternatingﬁne-tuning. </Abstractive Summary>  <Extractive Summary> =  iting (smaller average exit layer) is minimized. TakeFigure4asanexample,wereportthearea of one curve above the other as a numerical We visualize the trade-offs in Figures 4 and 5, and also show detailed numbers in Table 2 using metric: JOINT over ALT and ALT over JOINT is respectively (0.4, 13.5) for RTE, (0.9, 8.8) results from the test set. Dots in the ﬁgures and for MRPC, and (0.2, 18.2) for SST-2. </Extractive Summary>  </Table ID = 2>  </Paper ID = 8> 

<Paper ID = 9>  </Paper ID = 9> 

<Paper ID = 10>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thishasbeenshownforseman- areweb-crawledcorporafromthe.ukand.dedo- tic similarity and analogy tasks (Mu et al., 2017; mainrespectively. Resultinginfairlylargecorpora, Artetxeetal.,2018b;Raunaketal.,2019)aswell 2B tokens and 750M tokens (see Table 1). We astermextraction(Ha¨ttyetal.,2020). Theirbiggestdifferenceto PUKWAC already contained in a VSM, rather than adding and SDEWAC is their approximately 10 to 100 additionalinformation. Furthersemanticinforma- timessmallersize,accordingtotokencounts(see tioncanbeintroducedbypre-trainingvectorson to Table 1). The task is to rank the list of target alargerunspeciﬁccollectionoftext(Kutuzovand words according to their word sense divergence, Kuzmenko, 2016) or by training a seperate ma- gradually from 0 (no change) to 1 (total change). </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: max and mean performance on LCSD task (Spearman’s rho) for all alignment methods. </Abstractive Summary>  <Extractive Summary> =  We ﬁnd small mean perfor- 3For a detailed overview on SGNS parameters see Ap- pendixB. mancegainsacrosstheboard(.013forGER+STA, 129Figure 2: Left: max scores from Table 2, middle and right: Performance (Spearman’s rho) of NO alignment methodonLSCDtaskacrossdifferentdimensionalitiesandpre-trainingcorpora. VIwithoutpre-trainingascom- parablebaseline. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Mean of best-performing parameters and Test Statistics. </Abstractive Summary>  <Extractive Summary> =  Parameter range for SOT [-1,1] and MC+PCR PP methods on word embeddings are not limited [0,25] to performance differences in word similarity or LSCD tasks. We use two test statistics to fur- ther analyse vector spaces: (i) isotropy (Mu and Viswanath,2018),i.e.,uniformityofvectordistri- ure 3f, 3d and mean gain in Table 3. A range of butionand(ii)frequencybias(Dubossarskyetal., parametersshowsimprovementswithm=3yield- 2017;Kaiseretal.,2020a),i.e.,correlationbetween ing the highest (.0175). </Extractive Summary>  </Table ID = 3>  </Paper ID = 10> 

<Paper ID = 11>  <Table ID = 1>  <Abstractive Summary> =  Table 11: Random test samples from the Hungarian Gutenberg test set (ﬁrst 5 rows), and Opensubtitles test set (last5rows).ResponsesfromtheGPT2modeltrainedonGutenbergandOpensubtitlesarecompared.EOUmeans “EndOfUtterance”. </Abstractive Summary>  <Extractive Summary> =  effectivenessinthissettingaswell. 2 Background Unfortunately, the Cornell Corpus is relatively small,whiletheOpensubtitlescorpussuffersfrom Open-domaindialoguedatasetsvaryinsize,qual- thefactthattheoriginaldatasetlacksbothdialogue ity, and source, as demonstrated in Table 1. Gen- andturnsegmentation: subtitlelinesaretreatedas erally,smallerdatasetsareconstructedusingcon- turnsanddialoguehistoryconsistsoftheprevious trolledcrowdsourcingenvironments,makingtheir nlines,withlittletonoadditionalpost-processing quality higher (e.g., PersonaChat (Zhang et al., usedtoextractdialoguesinsteadofusingtheraw 2018)). </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Statistics of the ﬁnal dialogue datasets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Metrics computed on the test set of DailyDialog and PersonaChat for Transformer and GPT2 trainings. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Random test samples from PersonaChat. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 9>  </Paper ID = 11> 

<Paper ID = 12>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Relative improvements (higher is better) of R @1 of RA-BERTE and RA-BERTD over the mean of 10 stochastic BERT predictions (S-BERTE and S-BERTD). </Abstractive Summary>  <Extractive Summary> =  E[RD] stands for the input space that only investigate whether ranking with risk aversion is uses the mean of the predictive distribution for more effective than using the predictive distribu- the k candidate responses in R using S-BERTD, tion mean, we select b based on the best value +var[RE] uses both E[RD] and the uncertainties observed on the validation set. Table 3 displays of S-BERTE for the k candidates and +var[RD] the results of this experiment, showing the im- usesboththescoresE[RD]andtheuncertainties provements of RA-BERTD and RA-BERTE over of S-BERTD. Our results show that the uncer- S-BERTD and S-BERTE respectively. </Extractive Summary>  </Table ID = 3>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  crease, moving away from the diagonal. Addi- S-BERTD because it better captures the model tionally, when we challenge the model in cross- uncertaintyinthetrainingprocedure,sinceitcom- domain and cross-NS settings, the calibration er- bines different weights that explain equally well ror increases signiﬁcantly as evident in Table 1. thepredictionofrelevancegiventheinputs. </Extractive Summary>  </Table ID = 1>  </Paper ID = 12> 

<Paper ID = 13>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Adversarial example detection performances for DISP and FGWS when evaluated on attacks against RoBERTa. </Abstractive Summary>  <Extractive Summary> =  Implementation details suggestedbyAlzantotetal.(2018),consistingofa for the models and attacks can be found in Ap- population-basedblack-boxmechanismbasedon pendix B. We report the after-attack accuracies2 geneticsearchthatiterativelyperformsindividual for the RoBERTa model in Table 2 and for the word-level perturbations to an input sequence to CNN/LSTMmodelsinTable3(columnAdv.). We causeamisclassiﬁcation. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  FGWS transforms a given 0 ThehigherBF ,thestrongertheevidenceinfavor sequence X into a sequence X(cid:48) by replacing in- 10 ofthealternativehypothesisH .3 Weadditionally frequent words with more frequent, semantically 1 calculate Cohen’s d effect sizes for all mean fre- similarsubstitutions. Weinitiallydeﬁnethesubset quencycomparisons.4 X := {x ∈ X|φ(x) < δ}ofwordsthatareeligi- E Table 1 shows the loge frequencies (mean µφ bleforsubstitution,whereδ ∈ R>0 isafrequency andstandarddeviationσ )andCohen’sdforthe threshold. FGWS then generates a sequence X(cid:48) φ speciﬁedsamplesgeneratedbytheattacksagainst fromX byreplacingalleligiblewordswithwords theRoBERTamodel(theresultsfortheCNNand thataresemanticallysimilar,buthavehigheroccur- LSTMmodelscanbefoundinAppendixC).We rence frequencies in the model’s training corpus. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Performance results of NWS and FGWS on select γ so that not more than 10% of the unper- attacks against the CNN and LSTM models. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Mean log frequencies of replaced words and their corresponding substitutions by attack, model and e dataset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  theperturbedsequences. G AdditionalFGWSexamples F Varyingfalsepositivethresholds Additional examples of FGWS can be found in Table 6 (true positives), Table 7 (false positives), Therateoffalsepositivespredictedbyadetection Table 8 (true negatives) and Table 9 (false nega- systemiscrucialforitspracticability,andalimited tives). amountoffalsepositivesishencehighlydesirable. </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  theperturbedsequences. G AdditionalFGWSexamples F Varyingfalsepositivethresholds Additional examples of FGWS can be found in Table 6 (true positives), Table 7 (false positives), Therateoffalsepositivespredictedbyadetection Table 8 (true negatives) and Table 9 (false nega- systemiscrucialforitspracticability,andalimited tives). amountoffalsepositivesishencehighlydesirable. </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  theperturbedsequences. G AdditionalFGWSexamples F Varyingfalsepositivethresholds Additional examples of FGWS can be found in Table 6 (true positives), Table 7 (false positives), Therateoffalsepositivespredictedbyadetection Table 8 (true negatives) and Table 9 (false nega- systemiscrucialforitspracticability,andalimited tives). amountoffalsepositivesishencehighlydesirable. </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  theperturbedsequences. G AdditionalFGWSexamples F Varyingfalsepositivethresholds Additional examples of FGWS can be found in Table 6 (true positives), Table 7 (false positives), Therateoffalsepositivespredictedbyadetection Table 8 (true negatives) and Table 9 (false nega- systemiscrucialforitspracticability,andalimited tives). amountoffalsepositivesishencehighlydesirable. </Extractive Summary>  </Table ID = 9>  </Paper ID = 13> 

<Paper ID = 14>  </Paper ID = 14> 

<Paper ID = 15>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  betweenf andtheconcatenationofq witha (h ) i j j j computedbyconsideringnouns,verbs,adjectives andadverbs(1+overlaps,1overlap,0overlaps). ysisbyperforminganablationstudyonthedev-set Table 2 reports the MAP score for each of the (Section4.2). describedcategories. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  asigniﬁcantimprovementovertheRSbaselineis obtainedbyRSBM25+USBM25onbothlexical 4.2 ExplanationAnalysis gluesandcentralexplanationsentences(+6.0and Wepresentanablationstudywiththeaimofunder- +5.6MAPoverRSBM25). standingthecontributionofeachsub-component Regardingthelexicaloverlapscategories(Table to the general performance of the joint RS + US 2b),weobserveasteadyimprovementforallthe model (see Table 1). To this end, a detailed eval- combined RS + US models over the respective uation on the development set of the Worldtree RS baselines. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  On the other dev-set. Table 3 illustrates the ranking assigned hand, the longer the explanation, the higher the byRSandRS+USmodelstoscientiﬁcsentences numberofcorescientiﬁcfacts. Therefore,thede- ofincreasingcomplexity. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  To an- swerthequestionq,themodelselectsthecandidate Acknowledgements answerc suchthata = argmax p . a i i Table 4 reports the accuracy with and without The authors would like to thank the anonymous explanations on the Worldtree test-set for easy reviewersfortheconstructivefeedback. Aspecial and challenge questions (Clark et al., 2018). </Extractive Summary>  </Table ID = 4>  </Paper ID = 15> 

<Paper ID = 16>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Prediction accuracies for gender relational fectsthatareinsigniﬁcantatα < 0.01. </Abstractive Summary>  <Extractive Summary> =  (2018b) WEATandWAT.EspeciallyinWord2Vecandfast- usedasubsetof40instancesassociatedwith2seed Text,almostallbiasesaredebiased. word-pairs,notusedinthetrainingsplit,toevalu- Table 2 shows the percentages where a word- atethegeneralisabilityofadebiasingmethod. For pairiscorrectlyclassiﬁedasDeﬁnition,Stereotype unbiasedwordembeddings,weexpecthighsimilar- or None. </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Performance obtained when using only the ilarity between the two vectors (b − a + c) and dominantgloss(Dom)orallglosses(All)onSemBias. </Abstractive Summary>  <Extractive Summary> =  which demonstrates the effectiveness of our pro- posed method. From the results for Avg, we see 5.3 DominantGlossvsAllGlosses that debiasing is achieved with almost no loss in In Table 5, we investigate the effect of using the performance. In addition, thedebiased scores on dominantgloss(i.e. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  FollowingZhaoetal.(2018b),weevaluateon MSR(Mikolovetal.,2013c)andGoogleanalogy Embeddings Word2Vec GloVe fastText datasets (Mikolov et al., 2013a) as shown in Ta- Org/Deb Org/Deb Org/Deb ble6. WS 62.4/60.3 60.6/68.9 64.4/67.0 From Table 6 we see that for all word embed- SIMLEX 44.7/46.5 39.5/45.1 44.2/47.3 dings, debiased using the proposed method accu- RG 75.4/77.9 68.1/74.1 75.0/79.6 MTurk 63.1/63.6 62.7/69.4 67.2/69.9 rately preserves the semantic information in the RW 75.4/77.9 68.1/74.1 75.0/79.6 originalembeddings. Infact,exceptforWord2Vec MEN 68.1/69.4 67.7/76.7 67.6/71.8 embeddingsonWSdataset,weseethattheaccu- MSR 73.6/72.6 73.8/75.1 83.9/80.5 Google 74.0/73.7 76.8/77.3 87.1/85.7 racy of the embeddings have improved after the debiasingprocess,whichisadesirableside-effect. </Extractive Summary>  </Table ID = 6>  </Paper ID = 16> 

<Paper ID = 17>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Manual Evaluation: Percentage of cases for each approach where the majority of annotators predicted the stance of a generated claim on the given big issue correctly (true), incorrectly (false), or could not decide it (Undec.). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  baseline so. Ithinkthatitshouldbelegalized Table 6 shows the average score of all ap- becauseitisagoodthing proachesregardingtheinformativenessofthegen- eratedclaims. Here,bothversionsofourapproach Table7: Aselectionofclaimsgeneratedbythediffer- achievedbetterscoresthanthebaseline,matching ent evaluated approaches for the different association the ground-truth score. </Extractive Summary>  </Table ID = 6>  </Paper ID = 17> 

<Paper ID = 18>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Results on Gigawords dataset, where b in the transformerbaselinestandsforbeamsearchsize. </Abstractive Summary>  <Extractive Summary> =  2019a).Tomeasuretherelativeinferencespeedup, weincludetransformerasabaselinemodel. 6.1 ExperimentalSetup The results are shown in Table 1, from which WeimplementtheproposedmodelwithPyTorch we can see that, by using length-parallel decod- (Paszkeetal.,2017).TheBERTmodelweuseis ing,theperformanceofallNAGbaselinescanbe theHuggingfaceimplementation(Wolfetal.,2019) notablyimproved.However,suchproceduresignif- (bert-base-uncased).Toapproximatethetransition icantlyincreasestheinferencelatency.Incontrast, 238Models R-1 R-2 R-L Speedup Models F1 R-1 R-2 R-L Speedup Autoregressive Autoregressive Luong-NMT 33.10 14.45 30.71 - Bi-LSTM-Dep 82.3 81.5 74.1 81.3 - Tagger 82.8 81.1 72.4 80.9 - Pointer-Generator 35.98 15.99 33.33 - Tagger+ILP 79.0 76.1 64.6 75.8 - DRGD 36.25 17.61 33.55 - HiSAN-Dep 82.7 82.1 74.9 81.9 - ConceptPointer 36.62 16.40 33.98 - HiSAN 83.2 82.9 75.8 82.7 - Transformer(b=4) 35.74 16.97 33.43 1.00× Transformer(b=4) 82.4 82.0 74.6 81.8 1.00× Non-Autoregressive Non-Autoregressive NAG-NMT 27.20 8.96 25.58 9.31× NAG-NMT 72.5 72.1 59.9 71.8 10.71× +LPD-9 29.76 10.03 28.04 5.28× +LPD-9 73.8 73.6 61.0 73.1 6.09× NAG-REG 73.7 73.1 61.5 73.0 10.00× NAR-REG 28.56 9.79 26.83 8.64× +LPD-9 75.6 75.1 63.4 74.9 5.49× +LPD-9 31.23 11.14 29.55 4.74× NAG-CRF 75.1 74.4 66.8 74.2 9.41× NAG-CRF 30.29 12.61 28.71 8.07× +LPD-9 77.3 76.5 69.0 76.3 5.04× +LPD-9 32.91 14.31 31.03 4.32× BNAG-CRF 77.1 76.2 68.9 76.0 7.21× BNAG-CRF 32.63 14.32 30.82 6.13× +LPD-9 79.3 78.5 71.7 78.2 3.91× +LPD-9 34.56 16.10 32.76 3.21× Ours(α=0.7) 79.5 79.0 72.1 78.7 10.00× Ours(α=0.3) 34.67 16.13 32.81 9.31× Ours(α=1.0) 80.7 80.3 73.6 80.1 8.42× Ours(α=1.0) 35.05 16.48 33.28 6.72× Table2:Resultsonsentencecompressiontask Table 1: Results on Gigawords dataset, where b in the transformerbaselinestandsforbeamsearchsize. thestandardtoken-kept-F1(F1)score.Inaddition, Wealsoreporttheresultsofotherstandardmetrics our model can self-determine the output length includingROUGE-1,ROUGE-2andROUGE-L. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In addition, we compare our model tiveanalysis,weusethemeasurementofsentence- withseveralstrongautoregressivemodels,includ- levelrepetition(Wellecketal.,2020)tocompute ing LSTM-based (Wu et al., 2016), CNN-based theratioofduplicaten-grams(rep-n)inthegener- (Gehringetal.,2017)andtransformermodel. atedresult.Thismetricisdeﬁnedas The results are shown in Table 3, from which |uniquen-grams(Y)| weseethatourmodeloutperformsthebestNAG rep-n(Y) = 100×(1.0− ). baseline(withLPD)intermsofboththegeneration |n-grams(Y)| (13) quality and inference speedup. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  themodel’sROUGE-Lscore.Additionally,wealso BERT & CRF To quantify the importance of showtheresultsfromtransformermodelforadi- eachcomponent(BERT&CRF)ofourmodel,we rectcomparison.Comparingthetwovariantsofour evaluatetheperformanceonGigawordsdatasetby model,weseethattrainingwithcontext-awareob- removingeachcomponentiteratively. jectiveleadstoa42%droponrep-3metric(0.427 The results are shown in Table 4, from which vs 0.741) and a 64% drop on rep-4 metric (0.106 wecanseethatbyremovinganyofthesecompo- vs0.295).TheROUGE-Lresultsalsoindicatethat 240Ours Length-ParallelDecoding Models (α=1.0) LPD-1 LPD-5 LPD-10 BLEU 30.45 27.15 29.62 30.37 Speedup(×) 11.31 11.84 8.92 6.01 Table6:ResultscomparisononIWSLT14dataset the reduction in token repetition can effectively improvethemodelgenerationquality. Dynamic Length Determination Next, we ex- aminetheimportanceofthemodel’sabilitytody- Figure3:ExperimentresultsonGigawordsdatasetus- namically determine the length of the generated ingratio-ﬁrstdecodingwithdifferentα. </Extractive Summary>  </Table ID = 4>  </Paper ID = 18> 

<Paper ID = 19>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Machine translation results on different test sets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Ablation Study of ReZero and Reversible Training. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Training speed comparison between Trans- versible training saves GPU memory. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Model conﬁguration for SD-REV and FD- REV.nrepresentsnumberofsplits. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 19> 

<Paper ID = 20>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Average accuracy and standard deviation for RoBERTa (large) on Yelp, AG’s News, Yahoo and MNLI (m:matched/mm:mismatched)forﬁvetrainingsetsizes|T|. </Abstractive Summary>  <Extractive Summary> =  We do not deﬁneanItalianverbalizerbecausex-stancedoes Table 2: Comparison of PET with two state-of-the-art notcontainanyItaliantrainingexamples. semi-supervisedmethodsusingRoBERTa(base) 4.2 Results weincreasethetrainingsetsize,theperformance English Datasets Table 1 shows results for En- gains of PET and iPET become smaller, but for glishtextclassiﬁcationandlanguageunderstanding both50and100examples, PET continuestocon- tasks; we report mean accuracy and standard de- siderablyoutperformstandardsupervisedtraining viationforthreetrainingruns. Lines1–2(L1–L2) (L8vsL7,L11vsL10)withiPET (L9,L12)still show unsupervised performance, i.e., individual givingconsistentimprovements. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Comparison of PET with two state-of-the-art notcontainanyItaliantrainingexamples. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Minimum (min) and maximum (max) accu- supervised 76.6 76.0 71.0 racyofmodelsbasedonindividualPVPsaswellasPET T ,T De Fr PET 77.9 79.0 73.6 withandwithoutknowledgedistillation(|T|=10). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Results on x-stance intra-target for XLM-R ve 10 o (base) trained on subsets of TDe and TFr and for joint pr training on all data (T +T ). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Hyperparameters for training individual PET models without auxiliary language modeling (PET−LM) andwithlanguagemodeling(PET),theﬁnal PET classiﬁer(C),regularsupervisedtraining(sup.) andin-domain pretraining(In-Dom.PT).WheneverdifferentvaluesareusedfortheEnglishdatasets(En)andx-stance(Xs),both valuesaregivenseparatedbyaslash. </Abstractive Summary>  <Extractive Summary> =  orders of magnitude higher than the latter for all tasks considered. We thus selected a range B.1 HyperparameterChoices {1e−3,1e−4,1e−5}ofreasonablechoicesforα Relevanttraininghyperparametersforbothindivid- and performed preliminary experiments on Yelp ual PET models and the ﬁnal classiﬁer C as well with 100 training examples to ﬁnd the best value as our supervised baseline are listed in Table 5. amongthesecandidates. Asthisresultsin30dif- ferentconﬁgurationsforjustonetaskandtraining Forin-domainpretrainingexperimentsdescribed setsize,weonlyperformthisanalysisonYelpwith in Section 5, we use the language model ﬁnetun- 100examples,forwhichresultscanbeseeninFig- ingscriptoftheTransformerslibrary(Wolfetal., ure 6. For supervised learning, the conﬁguration 2020);allhyperparametersarelistedinthelastcol- usedthroughoutthepaper(LR = 1e−5,250steps) umn of Table 5. Pretraining was performed on a turnsouttoperformbestwhereasfor PET,training totalof3NVIDIAGeForceGTX1080TiGPUs. </Extractive Summary>  </Table ID = 5>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Most probable verbalizers according to AVS l well, thenq (l | x)(i.e., theprobabilityM p[l←t] forYelpwith50trainingexamples assigns to t given P(x)) should be high only for thoseexamples(x,y) ∈ T wherey = l. </Abstractive Summary>  <Extractive Summary> =  Foreachl ∈ L,wethensample whileeliminatingthechallengeofmanuallyﬁnd- k verbalizersv ,...,v usingρ tocompute ing suitable verbalizers. Table 8 shows the most 1 k 0 probableverbalizersfoundusingAVSfortheYelp n k 1 (cid:88)(cid:88) dataset. While most verbalizers for this dataset sk(t) = s (t | (P ,v )) l n·k l i j intuitivelymakesense,wefoundAVStostruggle i=1 j=1 with ﬁnding good verbalizers for three out of ten forallt ∈ V.7 Thesescoresenableustodeﬁnea labelsintheYahoodatasetandforallMNLIlabels. </Extractive Summary>  </Table ID = 8>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Of these tokens, we only to depend strongly on the initial random assign- keepthe10000mostfrequentonesinD. ment, we simply consider multiple such assign- Results are shown in Table 7. As can be seen, ments. </Extractive Summary>  </Table ID = 7>  </Paper ID = 20> 

<Paper ID = 21>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Total individual documents, mentions, co- textual language models such as BERT (Devlin referenceclustersofeachsubsetexcludingsingletons. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: MUC and B3 results from running baseline tityspansfromthemanuallyannotatedCD2CRas modelsonCD2CRtestsubset,BCOSthreshold=0.65 276TestType Co- Pass Rate & Exampletestcaseandoutcomefortestcase referent? TotalTests M1: ...toboostthestrugglinginsect’snumbers... </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The CA-V model demonstratesapromisingabilitytogeneralisebe- yondourcorpustoothertasksandrevealsaninter- estingcorrespondencebetweenCDCRandCD2CR settings. Figure 5: RoBERTa-based mention pair similarity fre- quency distribution for test examples from Table 4. MUC B3 Model ’yes’and’no’for’co-referent’and’not-co-referent’re- P R F1 P R F1 spectively CA 0.86 0.82 0.84 0.63 0.68 0.65 CA-V 0.82 0.81 0.81 0.56 0.53 0.55 suggeststhatdisentanglingthesepairsislikelyto Table5:MUCandB3resultsfromrunningtheCD2CR beachallengingtaskforthedownstreamclassiﬁca- baseline model (CA-V) on ECB+ dataset compared tionlayerintheCA-Vmodel. We collected a balanced set of 30-40 ments of different types and/or themes. We have examples of both co-referent and non-coreferent- constructedaspecialisedCD2CRannotateddataset, but-challengingpairsforeachtypeofrelationship available,alongwithourannotationguidelinesand (exact numbers in Table 4). We then recorded tool,asafreeandopenresourceforfutureresearch. </Extractive Summary>  </Table ID = 4>  </Paper ID = 21> 

<Paper ID = 22>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Full-length ROUGE (RG) F1 evaluation (%) on the CNN/DailyMail test set. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 22> 

<Paper ID = 23>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Results per entrainment target and measure, overall and per gender pair (female, male, mixed) with signiﬁcancelevel(***: α < 0.001,**: α < 0.01,*: α < 0.05,(*): α < 0.1)perfamily(seeFootnote2). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  These lists represent different percentages of ZahraRahimi,AnishKumar,DianeLitman,Susannah all the words uttered by various speaker groups Paletz,andMingzhiYu.2017. EntrainmentinMulti- in the corpus, as detailed in Table 2. We include PartySpokenDialoguesatMultipleLinguisticLev- them here so they may be used to interpret our els. </Extractive Summary>  </Table ID = 2>  </Paper ID = 23> 

<Paper ID = 24>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Safety of utterances, before ﬁltering datasets. </Abstractive Summary>  <Extractive Summary> =  302Model C2 WoW ED BST RetrieveandReﬁne(RetNRef),consistentwithWe- (K =20) (K =100) (K =100) (K =100) stonetal.(2018). 256M 88.55 91.70 62.67 83.45 622M 89.96 93.22 70.15 82.11 Safety Table 4 also shows that ﬁne-tuning on BSTresultsinsaferresponsescomparedtothepre- Table1: Hits@1/Kofﬁne-tunedpoly-encodermodels trainedbaseline,asgaugedbyanunsafewordlist onthevalidationsetforBSTdatasets. Hits@1/K mea- orthesafetyclassiﬁerofDinanetal.(2019b),7 and sures recall@1 when ranking the gold label among a that humans do utter unsafe responses, but much setofK−1otherrandomcandidates. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Average perplexity of the pre-trained and ﬁne-tunedmodelsonthevalidationsetsfortheBST Table 4: Safety of utterances, before ﬁltering datasets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Perplexity of the pre-trained and ﬁne-tuned models on the validation set for BST datasets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 24> 

<Paper ID = 25>  <Table ID = 1>  <Abstractive Summary> =  Table 10: conTest results for 200 random samples from Movie-Dialogs_Corpus.html McDiv includingHDS. </Abstractive Summary>  <Extractive Summary> =  Data for promptGen was gener- Aspect HDS (aspHDS); Identical to absHDS, ex- ated by GPT-2-large (Radford et al., 2019) with- cept we explicitly ask about a speciﬁc aspect of diversity,namelyformandcontent.6 out ﬁne-tuning. We provide examples for how story endings change as a function of tempera- 6.3 DecodingTest ture in Table 1. Examples for all tasks along with additional reproducibility details are in the IndecTestwemeasurethecorrelationbetweendi- Appendix B. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  perature) and evaluated 200 (2 random sets per temperature). For automatic metrics, we repeat Results Table 2 presents results of absHDS, this100times(randomlysampling200outof1K simHDS,andallautomaticmetrics. Ingeneral,n- sets each time), to present the mean and standard gram based metrics capture the diversity induced 4https://github.com/Tiiiger/bert_score by a temperature sweep, beating HDS and neu- 5https://github.com/UKPLab/ ral metrics. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: decTest results for different decoding parameters: absHDS 0.69 0.81 0.79 Spearman’s ρ (mean and standard deviation) of automatic simHDS - 0.74 - metricsforpromptGen. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  div that if m (S ) > η the classiﬁer predicts high div c diversity,andotherwisepredictslowdiversity. Div contains a subset of 3K examples, termed Table 4 shows the results. N-gram-based met- McDiv ,inwhichformdiversitywasneutral- nuggets rics perform poorly, indicating they do not mea- ized, providing a difﬁcult meta-evaluation chal- sure content diversity well. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: decTest ranking results: Spearman’s (ρ) correla- metrics (sent-BERT) and absHDS was increased tionbetweentemperaturedifferencesandeachmetricscore. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: decTestresultsfordifferentdecodingparameters: Spearman’s ρ (mean and standard deviation) of automatic Mizil and Lee, 2011) contains 108K/30K metricsforstoryGen. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  Table 7: decTestresultsfordifferentdecodingparameters: pre-trained generative models for generating re- Spearman’s ρ (mean and standard deviation) of automatic sponsesgiventhecontexts: metricsforrespGen. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  Table 8: conTest results for McDiv; Results for automatic metricsoverallthesamples(2Kpertask). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  </Paper ID = 25> 

<Paper ID = 26>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (2016) to evalu- questionshavebeenmatchedtosubject-predicate- 1 ate the identiﬁed topic entities. Experimental re- objecttriplesinFreebase, andveriﬁedbyhuman sults are shown in Table 1. We can see that our annotators. Experimentalresultsfrom ﬁne-grainedsemanticmatching. Wealsoemployed thismodelareshowninrowswiththepreﬁx“Multi- a BERT model to predict the start and end posi- task” in Table 1, 3, 4, and 6. Although the multi- tions of the topic entity in a question. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Qualitative analysis on entity linking candi- FullFreebase 79.0 88.9 90.3 datesfortheretrievalstep. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  l P P 71.9 84.6 86.3 Re-ranking step: We feed top-100 candidate t l nodes from the retrieval step into our entity link- Multi-taskP 68.1 84.2 85.8 l ing model Pl to re-rank all candidates. Table 3 Multi-taskPtPl 71.7 84.7 86.4 showsresultsontheSimpleQuestionsdataset. The FullFreebase 68.1 81.6 83.8 ﬁrstgroupofnumbersinTable3areresultsfrom previousstate-of-the-artmodels. We can see that our P model on the full Freebase. The last rows of Table 3, 4, r r obtainsverycompetitiveresultsontheSimpleQues- and6showtheresultsofrunningour“Multi-task” tionsdataset,andoutperformspreviousmodelsby model over the full Freebase. Signiﬁcant degra- alargemarginintheFreebaseQAdataset. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  For the SimpleQuestions dataset, since FreebaseQA dataset, we create another ranker to allquestionscanbeansweredwith1-hoprelation- multiplytogetherscoresfromboththetopicentity chains,weonlyretrieve1-hopcandidates. Forthe detectionmodelandentitylinkingmodel,andlist FreebaseQAdataset,followingthemethodinJiang the results in the row P P in Table 45. The P P t l t l etal.(2019),weonlyexpand1-hoprelation-chain ranker gets even better Top-1 accuracy than our candidatesinto2-hopcandidatesiftheobjectnode entitylinkingmodelP alone,whichveriﬁesthat l of a 1-hop relation-chain is a CVT node. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Qualitative analysis for relation-chain candi- Multi-taskP 79.7 47.9 r datesintheretrievalstep,where“Rel/Q”istheaverage Multi-taskP P P 79.7 51.7 t l r numberofrelation-chainsperquestion. </Abstractive Summary>  <Extractive Summary> =  Retrievalresultsfromthebaselinearelisted dateswiththesamesubjectandrelationtype,we inthe“All”columnsinTable5. Theresultsfrom ourIRbasedmethod(proposedinSection4.3)are 5AccuracyoftheP P modelisnotgiveninTable3,be- t l shown in the “IR” columns in Table 5. The last causeonlythebest(top-1)topicentityisusedforretrieving entitycandidatesintheSimpleQuestionsdataset. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Ourmethodingests relation detection model P to re-rank all candi- Freebaseintoinvertedindicesonharddiskstorage, r dates. Table 6 shows the results from previous thus naturally overcomes the memory overhead. state-of-the-art models as well as our relation de- Thisadvantageenablesustoevaluateourmethod tection model P . </Extractive Summary>  </Table ID = 6>  </Paper ID = 26> 

<Paper ID = 27>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  previous classiﬁer (Table 5). Table 6 shows the results. Theclassiﬁerstrainedonpreviousstandard datasets (although much larger) yield a consider- manual frame feature. </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  nenttypeofabuseinthestandarddatasets. Fromtheemotioncategories,disgustcorrelates 5.2 LinguisticAnalysis with abusive comparisons, while trust correlates Table 7 shows the precision of the most predic- withnon-abusiveinstances. Ofthemanualfeatures tivefeaturesforeachclass. </Extractive Summary>  </Table ID = 7>  </Paper ID = 27> 

<Paper ID = 28>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  thatprojectedtweettopredictthebinaryclasslabel y: p(y|M)∝exp(h·1)where1∈{1}n. Thismodel In Table 3, we examine for PMI and our projection-based approach whether the ranking isafeed-forwardnetworktrainedusingStochastic quality can be improved when more tweets are GradientDescent(Rumelhartetal.,1986). Onthe used. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  More speciﬁcally, any amount of tweets beyond 10k, we use 100k we apply the Adsorption label propagation algo- tweets(i.e.thelargestamountoftweetsavailable rithmfromjunto(Talukdaretal.,2008)onaword- tous)insubsequentexperiments. similaritygraphwherethewordsofourvocabulary Table 4 compares further methods. Our gold arenodesandedgesencodecosine-similaritiesof standard has a wide notion of abusive language, their embeddings. on lower ranks). Table 4 shows clearly that LP (2018a),onsomedomains(e.g.Warner),itiseven outperformsprojection onlowerranks. better. </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ouremoji-basedapproach,thatapproachproduces To ensure comparability to the remaining conﬁg- alwaysthesameresultoneachtestset. urations,theseedsareprependedtotheoutputof Table 6 shows that our lexicon performs on a LP (which explains that LP has only an impact parwiththeinductionmethodfromWiegandetal. on lower ranks). </Extractive Summary>  </Table ID = 6>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  we train on one dataset and test on another, we The underlying lexicon was bootstrapped using showthatthechosenconﬁgurationisnotoverﬁtto manual annotation and the induction depends on aparticulardataset. externalresources,suchasWordNetorsentiment Table 5 provides some information on the intensityresources. Ouremoji-basedapproachisa datasets we consider. </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Weoptedforasettingapplicabletomostlanguages. Table 7 shows the results. We also added an Evaluation. </Extractive Summary>  </Table ID = 7>  </Paper ID = 28> 

<Paper ID = 29>  </Paper ID = 29> 

<Paper ID = 30>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Smatchscores(CaiandKnight,2013). Modelper- formancevaluesinthismanuscriptareanaverage 5.3 Results overthebestperformingmodelsacross3random Table 2 compares our different proposed ap- seeds. Lastly, the input to the parser - the vector proaches to the three baseline methods using the representationofeachword-isobtainedbyaver- AMR2.0 and AMR1.0 datasets. </Extractive Summary>  </Table ID = 2>  </Paper ID = 30> 

<Paper ID = 31>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Somestatisticsofthedatasetsasgiven L = −logP(y∗|R ,R ,R ,...,R ;θ) by Ma et al. (2017) are shown in Table 1. We label 0 1 2 T (15) useGlove(Penningtonetal.,2014)embeddingto initialize the word vectors and oppositeness em- 3.5 MainTweetInformationPreservation bedding(deSilvaandDou,2019)toinitializethe TheVeysehetal.(2019)studynotedthatthemodel oppositenessembeddings. </Extractive Summary>  </Table ID = 1>  </Paper ID = 31> 

<Paper ID = 32>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Syntax transfer results on SNLI. </Abstractive Summary>  <Extractive Summary> =  as a proxy for syntactic similarity. As shown in Table 1, we also evaluate three other variants of 3.2 ControlledGenerationandTransfer our model. In polarized-VAE (wo) we use word Wefollowpreviouswork(Chenetal.,2019a;Bao overlap(BLEUscores)asaheuristicproxyfores- etal.,2019)andanalyzetheperformanceofcon- timating semantic similarity, while keeping syn- trolledgenerationbyevaluatingsyntaxtransferin tactic training unchanged. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Comparison of polarized-VAE with different theproximity-basedregularization). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 32> 

<Paper ID = 33>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Example paraphrase pairs in ParaSCI. </Abstractive Summary>  <Extractive Summary> =  ParaphrasepairsinParaSCI-ACLfocuson we propose to calculate the BLEU4 score (Pap- theNLPﬁeld,whileparaphrasepairsinParaSCI- ineni et al., 2002) between the source and target arXiv are more general. Some cases are shown sentencesofeachparaphrasepairandnameitSelf- in Table 1. ParaSCI show three main highlights: BLEU. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ParaSCI show three main highlights: BLEU. In Table 2, ParaSCI, especially ParaSCI- 1) Sentences included are long, nearly 19 words ACL, shows a relatively low Self-BLEU, which per sentence; 2) Sentences are more sententially meanssentencesaresigniﬁcantlychanged. divergent;3)Itprovidesrichscientiﬁcknowledge. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Comparison of paraphrase discovering meth- paraphrasecandidatesnaturally,justasthefollow- ods. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Overall variation of ParaSCI from manual ParaSCI-ACL 0.97 0.14 0.12 0.28 0.57 0.45 evaluation. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Wereporttheaveragenumberofoc- jority vote of other workers. We asked the best currencesofeachparaphrasetypepersentencepair workertolabelmoredatabyrepublishingtheques- for each corpus in Table 5 and visualize that in tions done by workers with low reliability (Co- Figure3. hen’s kappa <0.4). ParaSCI-ACLandParaSCI-arXivshowsimilardis- tributions. Paraphrasesentencesusuallychangea As shown in Table 5 and Figure 3, ParaSCI- lotlexically,becauselexicalvariationiseasierto ACL, ParaSCI-arXiv and Quora share a similar realize. Although sentential variation scores are paraphrasephenomenondistribution. </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Example paraphrase sentences generated and expressions, generation models trained on by the same Transformer model trained on different ParaSCIbringvaluablescientiﬁcknowledgetothe datasets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  As ParaSCI consists of quantities of scientiﬁc terms Table 7: Example paraphrase sentences generated and expressions, generation models trained on by the same Transformer model trained on different ParaSCIbringvaluablescientiﬁcknowledgetothe datasets. Different from Table 6, models here are outputsentence. trained with in-domain data, so the training data and testingdatacomefromthesameﬁeld. </Extractive Summary>  </Table ID = 6>  </Paper ID = 33> 

<Paper ID = 34>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Salient spans extracted using factuality dis- [SEP]+s +[SEP], where [CLS] is a special v criminator. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We conduct human evaluations on Amazon Me- chanical Turk (AMT) considering 4 different ab- Results We ﬁnd that Co-opNet is preferred stractive baseline model variants over 100 ran- acrossallcriteriaforallcomparisons,whenweuse domly sampled AAN test set examples. Given the adjacency discriminator (see Table 7). When agoldintroduction,AMTevaluatorsareaskedto using the the factuality discriminator, Co-opNet compareacorrespondingabstractgeneratedfrom is superior to baselines in all cases except when Co-opNetagainstanabstractgeneratedbyabase- comparedonabstractivenesstothePGenmodel. </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Whilethere tobemoreextractivethanabstractsgeneratedby has been prior work on factual consistency (Cao PGen. et al., 2018; Gao et al., 2019; Krys´cin´ski et al., As shown in Table 8, generations selected by 2020; Zhang et al., 2020b), these works did not the adjacency discriminator more closely match focusonscientiﬁcpapersummarization. the distribution of abstracts, while the generator NeuralAbstractiveSummarization Inthepast, sometimes favors copying from the introduction abstractive summarization models (Rush et al., atthelossofnarrativestructure. </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  Table 9: Automatic Evaluation of discriminator archi- maries from the hypothesis generation pool that tectures containsentenceslongerthanaﬁxedmaxlengthof 200tokens,aclearsignofcoherencedeterioration. </Abstractive Summary>  <Extractive Summary> =  The precision, accuracy and F1. Table 9 provides modelistrainedtominimizethenegativeloglikeli- summary statistics of discriminator performance hoodofthenextwordw givenallprecedingwords: i onthevariousdiscourseandfactualityobjectives. Discourse-Adj denotes the adjacency discrimina- ∣a∣+∣s∣ tors, while Discourse-Abs denotes the discourse Lgen = − ∑ logP(wi∣w0,...wi−1) (10) role label prediction model (Cohan et al., 2019) i=1 and Factuality denotes the token saliency predic- tionmodel. </Extractive Summary>  </Table ID = 9>  </Paper ID = 34> 

<Paper ID = 35>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Results (higher is better) on MetaQA, WebQuestionsSP datasets. </Abstractive Summary>  <Extractive Summary> =  Results. Table 1 shows the results. We evalu- atedourproposedmethodandallbaselinesthrough Acknowledgements Hits@1metric. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Ablation study on MetaQA KG-50 (2-hop) textcorpusonMetaQAKG-50. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Weexploitonly datasets) - KG Full (in which the KG is left un- the given knowledge base structure. Futhermore, touched),andthemorerealisticKG-50settingin as shown in Table 2, baselines such as GraftNet which50%linksarerandomlyremoved. Wecom- andPullNetperformpoorlyintheabsenceoftext. </Extractive Summary>  </Table ID = 2>  </Paper ID = 35> 

<Paper ID = 37>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Summary of the model performances. </Abstractive Summary>  <Extractive Summary> =  Thisdemonstratesthevalue 6 Experimentalresults ofthenewlyintroducedcoarsefeatures4. (Q2) Across the seven coarse input conﬁgura- The results are summarized in Table 4. The key ﬁndings are: (Q1) with the extended inputs, we tions, there are no signiﬁcant differences in the performancefromtheclassiﬁcationtreewhencom- observedstatisticallysigniﬁcantimprovementsin pared to the HAN in six out of seven input con- boththeHANandtreeovertheexistingfullsetof features(one-tailedt-test);(Q2)giventhecoarse- ﬁgurations. Ofallconﬁdenceintervalsthatin- inputs(two-tailedt-test). cluded zero in the fourth column of Table 4, the (Q1)Whencomparedtothefullsetofexisting 4WeperformedadditionaltestsinAppendixDtoobserve 3Weconductedadditionaltuningexperimentsforthetree theimpactoftheadditionstotheﬁnenarratives,andfound inAppendixCtoobservepotentialimprovementsinperfor- smallimprovements(butstatisticallyinsigniﬁcant)inallthree mance. outofthreeinputfamilies(va,vp,vpa). </Extractive Summary>  </Table ID = 4>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Summary of the model performances for the ﬁne narratives. </Abstractive Summary>  <Extractive Summary> =  intelligentvirtualagents,pages218–233.Springer. D Additionaltestsforadditionstotheﬁne Appendices narratives A Tuningprocedure Table 7 reports the additional tests on the impact oftheaddedﬁnefeatures. Weobservethatwhilst WetunedtheSGDoptimizerwithalearningrate all three input conﬁgurations (va, vp, vpa) have between0.003to0.010,batchsizetobebetween smallincreasesinperformance,noneofthemare 4to20,L2regularizationbetween10−6 and10−3, statisticallysigniﬁcant. </Extractive Summary>  </Table ID = 7>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  6. Theverbalcontentisslightlydifferentdueto(1) Table 5 (HAN) and Table 6 (Tree) report the hy- differentmethodstodeterminetalkturnstransitions perparameter conﬁgurations for each of the best- and(2)automaticspeechrecognitionaccuracy. performingmodelreportedinTable4. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  6. Theverbalcontentisslightlydifferentdueto(1) Table 5 (HAN) and Table 6 (Tree) report the hy- differentmethodstodeterminetalkturnstransitions perparameter conﬁgurations for each of the best- and(2)automaticspeechrecognitionaccuracy. performingmodelreportedinTable4. </Extractive Summary>  </Table ID = 6>  </Paper ID = 37> 

<Paper ID = 38>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Typological Blinding and Prediction. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Pearson correlations between α weights and similaritiesinthissetting. </Abstractive Summary>  <Extractive Summary> =  Weexpect measurewecalculateitspairwisePearsoncorrela- thatthistoalargedegreeisduetothetypological tionwiththeαvalueslearnedundereachcondition. similaritiesoflanguagesbeingencodedimplicitly Table 2 shows correlations between α weights basedoncorrelationsbetweenpatternsintheinput andsimilaritiesincreasewhenpredictingtypolog- data. Aslow-resourcelanguagesoftendonoteven icalfeatures,anddecreaseswhenblindedtosuch haveaccesstoanysubstantialamountofrawtext, features. </Extractive Summary>  </Table ID = 2>  </Paper ID = 38> 

<Paper ID = 39>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Mean and standard deviation results (development sets) for each of the 16 datasets and the different architectural setups. </Abstractive Summary>  <Extractive Summary> =  This demonstrates the need for more 5.2 AdapterFusion complexadaptationapproaches. AdapterFusionaimstoimproveperformanceona In Table 1 we show the results for MT-A and given target task m by transferring task speciﬁc ST-Awithareductionfactor16(seetheappendix knowledge from the set of all N task adapters, Table3formoreresults)whichweﬁndhasagood where m ∈ {1, ..., N}. We hypothesize that if trade-offbetweenthenumberofnewlyintroduced thereexistsatleastonetaskthatsupportsthetarget parameters and the task performance. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Performance changes of AdapterFusion com- information between tasks while avoiding the in- pared to ST-A and MT-A. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 39> 

<Paper ID = 40>  </Paper ID = 40> 

<Paper ID = 41>  </Paper ID = 41> 

<Paper ID = 42>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Comparison between reported performance completelymodular: theresearcherisabletopick andreproducedperformanceonOpenSQuAD. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Open SQuAD performance using Wikipedia dumps from different years. </Abstractive Summary>  <Extractive Summary> =  Inthereadertrain- Weconductthelastexperimenttotesttherobust- ing stage, we train the model using BERT-large- nessofthestate-of-the-artsystemagainsttemporal cased model, also with global normalization to shifting. Results are reported in Table 3. We ob- makethespanscorecomparable. </Extractive Summary>  </Table ID = 3>  </Paper ID = 42> 

<Paper ID = 43>  </Paper ID = 43> 

<Paper ID = 44>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  4.2 RelationExtraction Table4: Statisticsofrecognizedentities. As shown in Table 3, we extracted 40.5 million Model /corpus /abstract relations including 29.8 million unique relations. en ner craft md 1.8M 6 Among the relation extraction methods, OpenIE en ner jnlpba md 3.1M 11 outputsthelargestnumber. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  4.3 EntityRecognition KennethWardChurchandPatrickHanks.1990. Word As shown in Table 4, a total of 6.4M entities associationnorms,mutualinformation,andlexicog- were recognized from the corpus with the four raphy. ComputationalLinguistics,16(1):22–29. </Extractive Summary>  </Table ID = 4>  </Paper ID = 44> 

<Paper ID = 45>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  This record the time needed for ES and US to com- complexityiscomparablewiththoseofpublicdi- plete the tasks. Table 2 describes and compares alogue datasets, like Multiwoz or Taskmaster-1 the time taken on the admin task for the two su- (Budzianowskietal.,2018;Byrneetal.,2019). pervisorsacrossthethreelanguagesconsidered. </Extractive Summary>  </Table ID = 2>  </Paper ID = 45> 

<Paper ID = 46>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Generalizability of BERT SHARED-NORM swers. </Abstractive Summary>  <Extractive Summary> =  Theencoderinputisasinglesentence differentcombinationsofthesedatasets. and the decoder output is a question, where the Table 2 shows the results of this experi- inputsentencecontainstheanswertothequestion. ment. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  It’snotclearwhatthisquestion sults for each QG model in terms of answer F1 isasking. accuracy are shown in Table 3, compared along- 3: Questionisstrangelyworded,vague,orcon- sidetheresultforhuman-authoredquestions. tains errors. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Overalltheseresultsagainshowthe questions corresponding to 175 inputs. Table 5 beneﬁtofaugmentingthetrainingdatawithauto- in the appendix shows examples of these items. matically generated questions. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Accordingly, our Participantsreadtheinputsentenceinitsparagraph democurrentlyrunstheAUGMENTEDmodel. 444 GeneratingQ&APairs Table 6 shows an example of a generated Q&A list for one text. We conducted an evaluation of We combined our best-performing QG and QA the informativeness of these pairs with 38 AMT modelsintoasystemthattakesatextasinputand participants. </Extractive Summary>  </Table ID = 6>  </Paper ID = 46> 

<Paper ID = 47>  <Table ID = 2>  <Abstractive Summary> =  Table 2: In-domain type-aware F1 score for test set Weconductthreeexperimentsontheninedatasets on each dataset with current SoTA. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Type-ignored F1 score in cross-domain setting over non-lower-cased English datasets. </Abstractive Summary>  <Extractive Summary> =  We also andXLM-RLARGE modelsalongwithcurrentstate- report the accuracy of the same XLM-R model of-the-art(SoTA).Onecanconﬁrmthatourframe- trainedoveracombineddatasetresultingfromcon- work with XLM-RLARGE achieves a comparable catenationofalltheabovedatasets. SoTAscore,evensurpassingitintheWNUT2017 In Table 3, we present the type-ignored F1 re- dataset. Ingeneral,XLM-RLARGE performsconsis- sultsacrossdatasets. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Cross-lingual type-aware F1 results on vari- thealphabet. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Cross-lingual type-aware F1 score over WikiAnndatasetwithXLM-R . </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 47> 

<Paper ID = 48>  </Paper ID = 48> 

<Paper ID = 49>  </Paper ID = 49> 

<Paper ID = 50>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  of the tasks, particularly for sentence segmenta- tion (+3.24%), POS tagging (+1.44% for UPOS and +1.55% for XPOS), morphological tagging ModelPackage Trankit Stanza (+1.46%), and dependency parsing (+4.0% for MultilingualTransformer 1146.9MB - UASand+5.01%forLAS)whilemaintainingthe Arabic 38.6MB 393.9MB Chinese 40.6MB 225.2MB competitive performance on tokenization, multi- English 47.9MB 383.5MB wordexpansion,andlemmatization. French 39.6MB 561.9MB Spanish 37.3MB 556.1MB 5.3 NERresults Totalsize 1350.9MB 2120.6MB Table 3 compares Trankit with Stanza (v1.1.1), Table5: Modelsizesforfivelanguages. Flair (v0.7), and spaCy (v2.3) on the test sets of 11consideredNERdatasets. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Flair (v0.7), and spaCy (v2.3) on the test sets of 11consideredNERdatasets. FollowingStanza,we reporttheperformanceforothertoolkitswiththeir 5.4 SpeedandMemoryUsage pretrained models on the canonical data splits if Table 4 reports the relative processing time for theyareavailable. Otherwise,theirbestconfigura- UDandNERofthetoolkitscomparedtospaCy’s tionsareusedtotrainthemodelsonthesamedata CPU processing time5. </Extractive Summary>  </Table ID = 4>  </Paper ID = 50> 

<Paper ID = 51>  </Paper ID = 51> 

<Paper ID = 52>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Forbothtasks,we alogisidentiﬁedbyauniqueresourceidentiﬁer employedBidirectionalEncoderRepresentations (URI).Itisconnectedtoamediatornodethatrep- fromTransformers(BERT)(Devlinetal.,2019). resentsthemultiaryrelationassociatedwiththe Table 1 shows some example results of claim entry. For example, Figure 3 shows a question matchingandstancedetection. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Correlation between the percentage of con- willremainatopperformerunderallfourclasses. </Abstractive Summary>  <Extractive Summary> =  Be- tweetsrelativetothetotalnumberoftweetsfrom causeotherrecentstancedetectionmethods(Mo- a country. The Pearson correlation coefﬁcients htarami et al., 2018; Fang et al., 2019) only re- between them are in Table 4. We ﬁnd that the portedmacro-F1scorescalculatedusingallfour numberofmisinformationtweetsmostpositively classesincluding“unrelated”,wecannotreport correlates with the number of conﬁrmed cases. </Extractive Summary>  </Table ID = 4>  </Paper ID = 52> 

<Paper ID = 53>  </Paper ID = 53> 

<Paper ID = 54>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  moresuitablefordeploymentduetoitssmallsize. Theclassiﬁcationresultsappearinatablesothat Table 2 lists performance of SVMs version com- earlierresultscanbereferredto. Werecognizethat paredtousingBERT.WhencomparingtoBERT usersmaywanttoclassifymanytweetsinonego models, we ﬁne-tuned AraBERT(Antoun et al., withouthavingtotypethemoneatatime. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  via POST requests. Table 3 lists available API InProceedingsoftheFifthArabicNaturalLanguage routesandFigure3illustratesexampleusage. Re- ProcessingWorkshop,pages97–110. </Extractive Summary>  </Table ID = 3>  </Paper ID = 54> 

<Paper ID = 55>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Number of concepts linked to ConceptNet rainingcatsanddogs,etc. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 55> 

<Paper ID = 56>  <Table ID = 1>  <Abstractive Summary> =  Table 1: MSA tools implemented in SAFAR 6 Almost all integrated MSA tools have their own license. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: MSA resources implemented in SAFAR Also, a corpus for language identification tasks 3.2 Moroccan Dialect is available with SARAF. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Moroccan dialect resources and tools implemented in SAFAR specifying appropriate parameters according to 3.3 Machine learning models their needs. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 56> 

<Paper ID = 57>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  These examples show- went out to buy groceries. After an hour he got case how the ﬁne-grained analysis enabled by In- up.” (Figure 5b and Table 1). These two exam- terpreT affords a higher level of insight that is ples show how changing a single token (“back” indispensable for interpreting model behavior for became “up”) signiﬁcantly alters the sentence se- complexlanguageunderstandingtasks. </Extractive Summary>  </Table ID = 1>  </Paper ID = 57> 

<Paper ID = 58>  </Paper ID = 58> 

<Paper ID = 59>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 59> 

<Paper ID = 60>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Ascreenshot intotrain/dev/testsplits,weﬁrstcreatechunksof of the output of the system is shown in Figure 1. size 100,000 samples in which all samples of an Moreover, Table 1 shows the glossary extracted acronym are assigned to the same chunk. Since from the text of this paper using the rule-based eachacronymappearsonlyinonechuck,wetrain component of the system. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Fi- precision,recall,andF1scorefortheacronymand 1 2 n nally,theconcatenationofthetextrepresentation, long-formpredictionandalsotheirmacro-averaged i.e., h¯, and the acronym representation, i.e., h , F1 score. The results are shown in Table 2. This a is fed intoa 2-layer feed-forward neural network tableshowsthatourmodeloutperformsbothrule- whose ﬁnal layer dimension is equal to the total based and more advanced feature-based or deep number of long-forms in the dataset (i.e., dataset learningmodels. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Performance of models for acronym disam- biguation(AD) ularExpressions(BADREX)(Gooch,2011)orAb- breviationExpander(ABBREX)(ABBREX2018), unfortunately, they are incapable of acronym dis- andZettlemoyer,2020),DECBAE(Jinetal.,2019) ambiguation. </Abstractive Summary>  <Extractive Summary> =  To our knowledge, the most simi- andGAD(PouranBenVeysehetal.,2020d). The lar work to ours is proposed by Ciosici and As- results are shown in Table 3. This table demon- sent(2018). </Extractive Summary>  </Table ID = 3>  </Paper ID = 60> 

<Paper ID = 61>  </Paper ID = 61> 

<Paper ID = 62>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Scores of single task models on test data for ingcanbeenabledinthehyperparametersconﬁgu- threepopulardatasetsandavarietyoftasks. </Abstractive Summary>  <Extractive Summary> =  Forthesedatasets, EN(Bojaretal.,2014),IWSLT15EN-VI(Cettolo thecombinationofsmoothinganddatasetembed- etal.,2014))usingmBERTasourembeddings.10 dings are the most promising settings. Perhaps Table 2 reports our results on the test sets com- surprisingly, the zero-shot datasets (<1k) have a paredtopreviouswork. ForallUDtasks,wescore higherLASascomparedtothesmalldatasetsand slightlyhigher,whereasforGLUEtaskswescore usingaseparatedecoderbasedontheproxytree- consistentlylowercomparedtothereferences. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Average LAS scores on test splits of UD Table 3: Average results over all development sets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Average results over all development sets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: The scores (accuracy) per dataset on the GLUE tasks (dev) for a variety of multi-task settings (ordered by size, indicated in number of sentences in trainingdata). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 62> 

<Paper ID = 63>  </Paper ID = 63> 

<Paper ID = 64>  </Paper ID = 64> 

<Paper ID = 65>  </Paper ID = 65> 

<Paper ID = 66>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Language coverage per category of the ser­ ready being ingested together with two reposito­ vicestointegrateinELG ries related to ELG, LINDAT/CLARIAH­CZ and ELRA­SHARE­LRs(LRspublishedatLREC). </Abstractive Summary>  <Extractive Summary> =  Our goal is to provide ser­ Ourmaingroupsofusersare: (1)LT/LRproviders vicesofallclassesforallofficialEUlanguagesand – companies or research organisations with tools, forotherEUandnon­EUlanguagesthatareofso­ services or data that can be provided through the cial or strategic interest in the EU. Table 1 shows ELG;(2)Developersandintegrators–companies theoveralllanguagecoverageofeachcategoryof andresearchinstitutionsinterestedinusingLT;(3) servicesacrossallconsortiumpartners;languages General LT information seekers; (4) Stakeholders have been divided into four groups: (A) EU offi­ whowishtoprovideinformationabouteventsetc.; ciallanguages;(B)otherEUlanguageswithoutof­ (5) Casual visitors. We provide three ways of ac­ ficial status, plus languages from candidate coun­ cess: RESTAPIs,webUIs,Pythonpackage. </Extractive Summary>  </Table ID = 1>  </Paper ID = 66> 

<Paper ID = 67>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Evaluation results of rescaled 5-scale scores (%). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 67> 

<Paper ID = 68>  </Paper ID = 68> 

<Paper ID = 69>  </Paper ID = 69> 

<Paper ID = 70>  </Paper ID = 70> 

<Paper ID = 71>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Subse- quently,weremovedthedocumentswithlessthan 5 words for 20 Newsgroups and BBC News and lessthan3wordsfortheotherdatasets. Table 1 reports some statistics about the cur- rentlyavailablepre-processed. Figure1: WorkﬂowoftheOCTISframework Avg# #Unique Dataset Domain #Docs words Theframeworkcanbeusedbothasapythonli- words indocs braryandasadashboard. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  portantly,thehyper-parameterselection. Inthefol- Table 2 summarizes the main features of the 266existingtopicmodelingframeworksandcompares byallowingagivenruntobeexecutedbeforeoth- themwithOCTIS. ers. </Extractive Summary>  </Table ID = 2>  </Paper ID = 71> 

<Paper ID = 72>  </Paper ID = 72> 

<Paper ID = 73>  </Paper ID = 73> 

<Paper ID = 74>  </Paper ID = 74> 

<Paper ID = 75>  </Paper ID = 75> 

<Paper ID = 76>  </Paper ID = 76> 

<Paper ID = 77>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Germanisaninﬂectedlan- ofTrepresentsamoreevendistributionbetween guagei.ethewordorderchangesaccordingtothe thetasks,whenTishighenough,thevalueofλ function in the sentence. Most word orders are j 316 5Model Language TextMode Period Comma Question Overall Spoken 84.8 47.0 47.6 59.6 Hindi Written 89.2 34.7 55.2 59.7 Joint-BILSTMNCRF+FastText Spoken 59.8 40.3 19.4 39.8 Tamil Written 47.2 24.9 14.9 29.0 Spoken 88.7 67.3 41.1 65.6 Hindi Written 92.3 70.8 43.4 68.5 Joint-XLMRobertaNCRF Spoken 75.9 58.7 20.3 56.6 Tamil Written 70.5 43.6 20.3 44.8 Spoken 90.6 66.6 68.8 75.3 Hindi Written 93.7 74.9 59.6 76.1 Joint-MultilingualBERTNCRF Spoken 85.3 71.8 66.6 74.6 Tamil Written 74.8 50.1 43.1 56.0 Table3: Resultsonlowresourcelanguages deﬁnedintermsofﬁniteverb(V),incombination Table 3 . We obtained the best result using the withSubject(S),andobject(O).InGerman, this Joint-Multilingual BERT NCRF model. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Ablation Study on our dataset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 77> 

<Paper ID = 78>  </Paper ID = 78> 

<Paper ID = 79>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Weselectthethree The datasets we used are widely adopted in the tasksforadiverseyetpracticalbenchmarkforpre- NLP community. Quantitative details of datasets trainedmodelswithoutconstrainingthemodelson can be found in Table 2. The selected tasks are sentence-level classiﬁcation tasks. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  There- costbecausetheyvarydramaticallybetweentasks. fore, during model pretraining, after every thou- To simplify the process, we compute the ratio of sand pretraining steps, we use the current pre- BERT ’stimeandcosttothatofeachmodel LARGE trained model for ﬁne-tuning and see if the ﬁne- as the normalized measure, as shown in Table 3 tuned model can reach our cut-off performance. andTable4. extensive experiments on given hardware (RTX Here, our cut-offs were selected by observing 2080Ti GPU) with different model settings as therecentstate-of-the-artmodel’sperformanceon the selected dataset for the task4. A wise choice shown in Table 3 and Table 4. We also collect thedevelopmentsetperformancewithtimeinﬁne- wouldbechoosingtheperformanceofsomeclassic tuningtoinvestigatehowthemodelisﬁne-tuned methodslikeLSTM-CRForBi-LSTMmodelsas fordifferenttasks. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  extensive experiments on given hardware (RTX Here, our cut-offs were selected by observing 2080Ti GPU) with different model settings as therecentstate-of-the-artmodel’sperformanceon the selected dataset for the task4. A wise choice shown in Table 3 and Table 4. We also collect thedevelopmentsetperformancewithtimeinﬁne- wouldbechoosingtheperformanceofsomeclassic tuningtoinvestigatehowthemodelisﬁne-tuned methodslikeLSTM-CRForBi-LSTMmodelsas fordifferenttasks. </Extractive Summary>  </Table ID = 4>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In the MNLI task, such a trend does 333notapplybecauseoftheincreaseddifﬁcultylevel andthenumberoftraininginstances,whichfavors alargermodelcapacity. Even though ALBERT model has a lot fewer BERT-BASE RoBERTa-LARGE ALBERT-BASE BERT-LARGE XLNet-BASE ALBERT-LARGE parameters than BERT, according to Table 1, the RoBERTa-BASE XLNet-LARGE ALBERTmodel’sﬁne-tuningtimeissigniﬁcantly more than BERT models because ALBERT uses 0.8 largehiddensizeandmoreexpensivematrixcom- putation. Theparametersharingtechniquemakes 0.6 1(Dev) it harder to ﬁne-tune the model. </Extractive Summary>  </Table ID = 1>  </Paper ID = 79> 

<Paper ID = 80>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  periments in different settings. Since the main difference between the shared and the language- speciﬁc encoders-decoders lies in whether they 4.2 SharedvsLanguage-speciﬁc retrain the entire system when adding new lan- Table 1 and 2 show comparisons between the guages,weaccordinglydesignourexperimentsto sharedandlanguage-speciﬁcencoders-decoders. comparethesystemsunderthiscondition. Previous works, e.g. (Firat et al., 2016a; Lu Initial Training Table 1 shows that the et al., 2018), suffered from attention mismatch language-speciﬁc encoder-decoders outperforms which was solved by sharing intermediate layers. the shared approach in all cases. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  On average, Inourcase,webelievethatwedonotsufferfrom our proposed approach is better than the shared thisproblembecause,withinaninitialsetofNlan- approachwithadifferenceof0.40BLEUpoints. guages,wetrainN∗N−1systemsusingpair-wise Adding New Languages Table 2 shows that, corpus(withoutrequiringmulti-parallelcorpusas when adding a new language into the system, previous works (Escolano et al., 2019)). Due to the language-speciﬁc encoder-decoders outper- our proposed joint training, once we have trained formthesharedarchitectureby2.92BLEUpoints our initial system, we end up with only N en- for Russian-to-English and by 3.64 BLEU in the codersandN decoders(2∗N). </Extractive Summary>  </Table ID = 2>  </Paper ID = 80> 

<Paper ID = 81>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Accuracy rates of HISK, BOWE-BERT with Inthispaper,(i)weintroducedLaRoSeDa,alarge k-means, BOWE-BERT with SOMs and their combi- datasetforpolarityclassiﬁcationofRomanianre- nations on MOROCO, for the Romanian intra-dialect views,and(ii)weemployedself-organizingmaps, multi-classcategorizationbytopictask. </Abstractive Summary>  <Extractive Summary> =  Acknowledgments Results on MOROCO. In Table 3, we present theresultsonMOROCO,fortheRomanianintra- The authors thank reviewers for their useful re- dialect multi-class categorization by topic task. marks. </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  forthecross-validationprocedureand90.90%on Results on LaRoSeDa. In Table 2, we present the test set. We note that SOMs had a signiﬁ- the results on LaRoSeDa. </Extractive Summary>  </Table ID = 2>  </Paper ID = 81> 

<Paper ID = 82>  </Paper ID = 82> 

<Paper ID = 83>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Method TrueNewsasPositive FakeNewsasPositive Methods Types AUC F1Macro F1Micro F1 Precision Recall F1 Precision Recall BERT 0.60852 0.56096 0.69806 0.31574 0.40318 0.26050 0.80618 0.76011 0.85839 Usingonly LSTM-Avg 0.69124 0.62100 0.71877 0.42953 0.48415 0.39692 0.81246 0.79139 0.83671 claims’text LSTM-Last 0.70142 0.63122 0.72415 0.44650 0.48935 0.41412 0.81594 0.79594 0.83776 TextCNN 0.70537 0.63081 0.72005 0.45001 0.48164 0.43035 0.81160 0.79882 0.82622 Usingboth HAN 0.70365 0.62510 0.72800 0.42884 0.49192 0.38161 0.82136 0.79058 0.85490 claims’text& NSMN 0.77270 0.68006 0.76127 0.51954 0.57558 0.48182 0.84058 0.82011 0.86364 articles’text DeClare 0.81036 0.72445 0.78813 0.59250 0.61235 0.58096 0.85640 0.85023 0.86399 Ours MAC 0.88715 0.78660 0.83316 0.68738 0.69975 0.68601 0.88581 0.88617 0.88706 Imprv.overthebestbaseline 9.47% 8.58% 5.71% 16.01% 14.27% 18.08% 3.43% 4.23% 2.67% 5.3 ExperimentalSettings 5.4 PerformanceofMACandbaselines We show experimental results of our model and Foreachdataset,werandomlyselect10%number baselines in Tables 2 and 3. In Table 2, MAC ofclaimsfromeachclasstoformavalidationset, outperforms all baselines with signiﬁcance level whichisusedfortuninghyper-parameters. Were- p < 0.05 by using one-sided paired Wilcoxon port5-foldstratiﬁedcrossvalidationresultsonthe test on Snopes dataset. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Wetrainourmodeland result when h = 5,h = 2,H = 300 and 1 2 baselineson4-foldsandtestthemontheremaining D = D = 128. In Table 3, MAC also signif- 1 2 fold. WeuseAUC,macro/microF1,class-speciﬁc icantly outperforms all baselines with p < 0.05 F1,PrecisionandRecallasevaluationmetrics. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Impact of word attention and evidence atten- tiononourMACintwodatasets 0.89 0.75 Snopes PolitiFact 0.885 Methods AUC F1Macro AUC F1Macro AUC00.8.8785 AUC0.74 OnlyWordAtt 0.87278 0.77831 0.74483 0.67818 0.87 0.73 OnlyEvidenceAtt 0.82531 0.72885 0.71790 0.65187 0.865 Word&DocAtt 0.88715 0.78660 0.75756 0.68642 1 2 5 1 2 4 5 Table 5: Impact of speakers and publishers on perfor- h2 3 4 5 1 2 3h14 h23 4 5 1 2 3h1 manceofMACintwodatasets (a) Snopes (b) PolitiFact Snopes PolitiFact Figure 2: Sensitivity of MAC with respect to number Methods AUC F1Macro AUC F1Macro ofheadsinword-levelattentionh1 andthenumberof TextOnly 0.88186 0.77146 0.72401 0.66844 headsindocument-levelattentionh2 Text+Publishers 0.88715 0.78660 0.72645 0.66984 Text+Speakers 0.75202 0.68483 Text+Pubs+Spkrs 0.75756 0.68642 mative words in evidence, irrelevant information can be captured, leading to low quality represen- tations of evidence. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Impact of speakers and publishers on perfor- h2 3 4 5 1 2 3h14 h23 4 5 1 2 3h1 manceofMACintwodatasets (a) Snopes (b) PolitiFact Snopes PolitiFact Figure 2: Sensitivity of MAC with respect to number Methods AUC F1Macro AUC F1Macro ofheadsinword-levelattentionh1 andthenumberof TextOnly 0.88186 0.77146 0.72401 0.66844 headsindocument-levelattentionh2 Text+Publishers 0.88715 0.78660 0.72645 0.66984 Text+Speakers 0.75202 0.68483 Text+Pubs+Spkrs 0.75756 0.68642 mative words in evidence, irrelevant information can be captured, leading to low quality represen- tations of evidence. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 83> 

<Paper ID = 84>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Theevaluationmetricistoken- wheretisthepositionofthetoken,hf andhb are t t levelmicroF ,asthisisreﬂectiveofourevaluation forward and backward LSTM features at t, h is 1 t setupwhereeachtokenisevaluatedseparately. thenewoutputfeatureattanda istheattention t The top section of Table 1 shows the results of weightforthebackwardfeaturesatpositiont. The the different variants of the base architecture in attentionweightsa arecomputedasfollows: t Section5.2. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Entity identiﬁcation performance. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Results of the proposed wait-one policy. </Abstractive Summary>  <Extractive Summary> =  9 Conclusions a threshold on the tag output. Table 2 shows the This paper introduced and motivated the as-you- resultsofseveralthresholdvaluesandtheirimpact typeexperimentalsetupforthepopularNERtask. onthetotalF . </Extractive Summary>  </Table ID = 2>  </Paper ID = 84> 

<Paper ID = 85>  </Paper ID = 85> 

<Paper ID = 86>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Randomly sampled test-train overlapping questions in Open NaturalQuestions. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Exact Match scores on our dataset splits. </Abstractive Summary>  <Extractive Summary> =  4.4 Results Examiningwhatbehavioursarelearntbymodels has received attention in language understanding QuestionMemorization Earlier,wefoundthat tasks, such as GLUE (Wang et al., 2018), which ∼30%oftestsetquestionsoverlapwiththetrain includestoolsforprobingfordifferentreasoning set. The “Question overlap” columns in Table 4 types. There has also been critical and careful shows performance on Question Memorization. titles,andprovidebaselinesfor“long-answer”QA. Vergaetal.(2020)observeansweroverlapeffects Answer Classiﬁcation The “Answer overlap inarelatedmodality(knowledgebaseQA),butno only” column in Table 4 shows performance on notconsiderquestionoverlap. answerclassiﬁcation. </Extractive Summary>  </Table ID = 4>  </Paper ID = 86> 

<Paper ID = 87>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  . 1010Domain intents slots utterances 5 Zero-shotperformance Weather 2 4 3692 Bottom part of Table 3 shows the CS test accu- Device 17 6 2112 racywhenusingonlythein-domainmonolingual Table1: CSTOPStatistics data. OurENdatasetisthetask-orientedparsing dataset(Schusteretal.,2018)describedinthepre- vioussection. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Zero-shot accuracy for simple LSTM model EN+ES 88.2 87.8 91.2 whenusingdifferentmonolingualcorporaanddifferent embeddingstrategies. </Abstractive Summary>  <Extractive Summary> =  R and XLM, which is around 50% faster during We have shown the effects of using the afore- theinferencethanXLM-Rduringinference. The mentioned embeddings in the zero-shot setting trainingdetailsforallourmodelsandthevalidation in Table 4. We can see that by having monolin- resultsarelistedintheAppendix. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  readyavailable. TheﬁrstcolumninTable6shows In Table 5, we show the CS zero-shot accu- theCSFew-Shot(FS)performancealongsidethe racywhenﬁne-tuningonthenewlygeneratedES ﬁne-tuning on the EN data and the aligned trans- data(calledES∗.) alongsidetheoriginalENdata. lated data, when average over three sampling of Wecanseethatunsupervisedalignmentresultsin CSTOP100. </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  1019A Appendix Here,wedescribethedetailsregardingthetraining asthevalidationresults. A.1 ModelandTrainingParameters In Table 7, we have shown the training details for all our models. We use ADAM (Kingma and Ba,2014)withLearningRate(LR),WeightDecay (WD),andBatchSize(BSz)thatislistedforeach model. </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Validation Accuracy when only a few CS instances (FS) are available during training. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  </Paper ID = 87> 

<Paper ID = 88>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Human evaluation on a three-point scale (0 Paraphrases generatedby 54.1 73.4 51.7 62.3 = not a paraphrase, 1 = ungrammatical paraphrase, 2 targettemplates =grammaticalparaphrase). </Abstractive Summary>  <Extractive Summary> =  and SIVAE. All models are given syntactic spec- Theresultsofhumanevaluationarereportedin iﬁcations; however, without the disentanglement, Table 3. If paraphrases rated 1 or 2 are consid- Seq2seq-SynandSIVAEtendtocopythesource eredmeaningful,wenoticethatSynPGgenerates sentenceastheoutputandthereforegetlowtem- meaningful paraphrases at a similar frequency to platematchingscores. </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  If paraphrases rated 1 or 2 are consid- Seq2seq-SynandSIVAEtendtocopythesource eredmeaningful,wenoticethatSynPGgenerates sentenceastheoutputandthereforegetlowtem- meaningful paraphrases at a similar frequency to platematchingscores. thatofSIVAE.However,SynPGtendstogenerate Table 2 lists some paraphrase examples gener- more ungrammatical paraphrases (those rated 1). atedbyallmodels. </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  targettemplatesisthatwecaneasilygeneratealot Furthermore, wecalculatethehitrate, theper- of syntactically different paraphrases by feeding centage of generated paraphrases getting 2 and the model with different templates. Table 5 lists matchingthetargetparseatthesametime. Thehit someparaphrasesgeneratedbySynPGwithdiffer- ratemeasureshowoftenthegeneratedparaphrases enttemplates. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Training on larger dataset improves the performance of SynPG. </Abstractive Summary>  <Extractive Summary> =  accuracyofthegeneratedparsep˜. From Table 6, We observe that enlarging the Weﬁndthatmostofgeneratedparsesp˜ indeed trainingdatasetindeedimprovestheperformance. followthetargettemplates,whichmeansthatthe Also, with the ﬁne-tuning, the performance of parse generator usually generates good parses p˜. </Extractive Summary>  </Table ID = 6>  </Paper ID = 88> 

<Paper ID = 89>  </Paper ID = 89> 

<Paper ID = 90>  </Paper ID = 90> 

<Paper ID = 91>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Ablation study of our framework, reporting Categorical 11 43 supervisedJGAontheMultiWOZ2.1testset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Supervised DST performance on MultiWOZ 2.1ofourmodel(underlined)comparedtopriormeth- Table 4: Supervised DST performance on MultiWOZ ods capable of zero-shot inference. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Supervised DST performance on MultiWOZ ods capable of zero-shot inference. </Abstractive Summary>  <Extractive Summary> =  Inthissettingwe malize domains and slots corresponding to the samedomain(e.g.Bus 1, Bus 2)foratotalof compare our approach against prior methods ca- pableofzero-shotinferenceinTable3—TRADE, 19 domains and 124 slot types in DSTC8. We STARC,SUMBT,andMA-DST—andthoseinca- further manually annotate each dataset with slot pable of doing so in Table 4, including DSTQA value types: open-valued (e.g. Hotel Name), nu- (Zhou and Small, 2019), DS-DST (Zhang et al., meric(e.g.RestaurantGuests),temporal(e.g.Taxi 2019),SOM-DST(Kimetal.,2020),SST(Chen LeaveAt), and categorical (e.g. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Zero-shot domain adaptation JGA (%) on MultiWOZ 2.1 test set for recent works and our mod- domain-slottuplesviaBERT(Devlinetal.,2019) els with question loss, on the (Rest)aurant, (Hot)el, andanRNNencoder,respectively. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 91> 

<Paper ID = 92>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Zero-shot ad-hoc retrieval for Natural Ques- Table 5: MAP for zero-shot models (above dashed tions. </Abstractive Summary>  <Extractive Summary> =  WecanseeherethatwhileQGen substantially. The last row of Table 4 shows a still signiﬁcantly outperform other baselines, the model trained on the NaturalQuestions training gap between QGen and QA is smaller. Unlike data,whichisnearly2-3timesmoreaccuratethan BioASQ and Forum datasets, NaturalQuestions thebest zero-shotmodels. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: MAP for zero-shot models (above dashed tions. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Furthermore,froman measured on BIOASQ 7 and generation quality efﬁciencystandpoint,howmanysynthetictraining with a held out set of the community question- examplesarerequiredtoachievemaximumperfor- answer data set. Results are shown in Table 6. mance. </Extractive Summary>  </Table ID = 6>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  More Trainingbatchsizeforthe“lite”,“base”and“large” detailsofhyperparametervaluesofeachtaskare modelsare256,128and32respectively. Allmod- listed in Table 8. Note on Forum data, the maxi- elsaretrainedona“4x4”sliceofv3GoogleCloud mumbatchsizeforQGenismuchlargerthanother TPU.Atinference,resultsfromusingbeamsearch tasks. </Extractive Summary>  </Table ID = 8>  </Paper ID = 92> 

<Paper ID = 93>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Weevaluateourmethodwithautomaticevaluation metrics-ROUGEF1scores(Lin,2004). ROUGE- The ﬁrst blocks in Table 3,4 include the lead 1 and ROUGE-2 compute unigram and bigram andtheoraclebaselines. Thesecondandthethird overlapsbetweensystemsummariesandreference blocksinthetablespresenttheresultsofsupervised summaries,whileROUGE-Lcomputesthelongest abstractive models, and of supervised extractive commonsub-sequenceofthetwo. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thisoverlapwithgoldstandardsummaries readerevenifitisn’tintheabstract(LinandHovy, suggestsouruseofdiscoursestructureandhierar- 1997)). chyplaysasigniﬁcantroleinourmethod’sperfor- Table 5 presents the human evaluation results. mance. </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  againsuggestthatourmethod’simprovementcan 6 AblationStudies indeed be attributed to the use of hierarchy and discoursestructure,ratherthantothethechoiceof 6.1 Component-wiseAnalysis representations. Table 6 presents the ablation study to assess the relativecontributionsoftheboundaryfunctionand 6.3 StabilityofHyperparameters the hierarchical information. We keep all the hy- perparametersunchangedwithrespecttothebest settinginSection4.2andeithervarythepositional function or the hierarchical structures. We also found that the improvement of each components arestableacrossallthehyperparameterswetested (moredetailsintheappendix). The ﬁrst block of Table 6 reports the ablation resultswithdifferentpositionalfunctions: noposi- tionalfunction(ErkanandRadev,2004;Mihalcea andTarau,2004),leadbiasfunction(ZhengandLa- pata,2019),andourproposedboundaryfunction. Wecanseethatusingthewrongpositionalfunction hurtsthemodel’sperformancewhencomparingno positional function with lead bias function. </Extractive Summary>  </Table ID = 6>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Our empirical results indicate the hierarchy- multiplymodelalwaysoutperformsno-hierarchy models ((Figure 4 (a)) but under performs hierarchy-add. Nevertheless, Table 8 shows that addinganyhierarchicalstructureresultsinperfor- manceimprovementbywidemarginswhencom- paredtotheno-hierarchymodel. (b) Section-section hierarchical multiplica- tion(hierarchy-multiply,ours) (c) Section-sentence hierarchical addition (hierarchy-add,ours) Figure4: Comparisonoftheﬂatfully-connectedgraphused inErkanandRadev(2004);MihalceaandTarau(2004);Zheng andLapata(2019)tothehierarchicalgraphusedinourmod- els(b)and(c). </Extractive Summary>  </Table ID = 8>  </Paper ID = 93> 

<Paper ID = 94>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Comparison of different major dialog act humanconversations.Inthispaper,wepresent schemes. </Abstractive Summary>  <Extractive Summary> =  Weshowacompar- alogsystemunderstanding. Wealsobuildadialog ison to major dialog act schemes in Table 1. We actpredictorbasedontheannotatedcorpus. </Extractive Summary>  </Table ID = 1>  </Paper ID = 94> 

<Paper ID = 95>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Illustrations of input-output pairs for typical thedown-streamtargettask. Table 10: Average BLEU-2/BLEU-3 scores for the oftheresponsesare“um - hum”forSwitchboard. </Abstractive Summary>  <Extractive Summary> =  ourmodels. In Table 10 we show the knowledge transfer resultsfortheCornellMoviedataset. B DetailsonDatasets InTable11weshowcontextsensitivityresults fortheCornellMoviedataset. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Perplexity on test set for different training process on the three dialogue datasets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Average BLEU-2/BLEU-3 scores for the model’s samples w.r.t. </Abstractive Summary>  <Extractive Summary> =  strategieschangethemodel’sbehavior. Inpartic- The BLEU scores are shown in Table 4. Note ular,weaimtoanswerthecrucialquestionabout that for the pretrained model we feed news trig- whetherthemodelforgetspreciouslanguagegener- gers,whilefortheotherdialoguemodelsdialogue ationskillsduringstandardﬁnetuning,andwhether triggers are used. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Weshowcasesam- ysisbelowontheNSpretraining. ples from different models in Table 3. To save Comparingtostandardﬁnetuning, mix-review space, we manually select and show the most re- furthergivessolidimprovement. </Extractive Summary>  </Table ID = 3>  <Table ID = 6>  <Abstractive Summary> =  Table 6: The model’s relative PPL drop when word- highlightthepretrainedmodel’sperformancefornews shufﬂe/drop is applied to input. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  possibility of a data-driven knowledgable chat- Weobservethatthestandardﬁnetunedmodels bot. In Table 7, we show multi-turn and single- arenotclosetotheclusterofthepretrainedmodels, turninteractionexampleswiththemodeltrained which suggests the models’ generative behavior by mix-review. For demonstration purpose, we issubstantiallydifferentfromthepretrainedones. </Extractive Summary>  </Table ID = 7>  <Table ID = 9>  <Abstractive Summary> =  Table 9: Samples of different models on the Dailydialog/Switchboard/Cornell-Movie test-set. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 9>  </Paper ID = 95> 

<Paper ID = 96>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  rationprocessastheE2E-Conformermodel. Table 2 shows the comparison between DNN- Table 3 compares different end-to-end ASR HMMsystemsandourend-to-endsystemonYMC- architectures on the YMC-EXP corpus.8 The EXP(-CS).Itindicatesthatevenwithoutanexter- E2E-Conformer obtained the best results, obtain- nallanguagelexicontheend-to-endsystemsigniﬁ- ing signiﬁcant WER improvement as compared cantlyoutperformsboththeDNN-HMMbaseline to the E2E-RNN and the E2E-Transformer mod- modelsandtheCNN-HMM-basedstate-of-the-art els. TheE2E-Conformer’sWERonYMC-EXP(- model. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  rationprocessastheE2E-Conformermodel. Table 2 shows the comparison between DNN- Table 3 compares different end-to-end ASR HMMsystemsandourend-to-endsystemonYMC- architectures on the YMC-EXP corpus.8 The EXP(-CS).Itindicatesthatevenwithoutanexter- E2E-Conformer obtained the best results, obtain- nallanguagelexicontheend-to-endsystemsigniﬁ- ing signiﬁcant WER improvement as compared cantlyoutperformsboththeDNN-HMMbaseline to the E2E-RNN and the E2E-Transformer mod- modelsandtheCNN-HMM-basedstate-of-the-art els. TheE2E-Conformer’sWERonYMC-EXP(- model. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: E2E-Conformer Results for Two Transcrip- tionLevels(Underlyingrepresentsmorphologicaldivi- Table6: E2E-ConformerResultsonanotherEL:High- sionsandunderlyingphonemesbeforetheapplication landPueblaNahuatl of phonological rules; Surface is reﬂective of spoken formsandlacksmorphologicalparsing) CER WER Corpus CER WER dev/test dev/test Corpus dev/test dev/test 10h 18.3/17.5 44.7/43.3 20h 14.2/12.9 34.8/33.3 10h 19.4/19.5 39.1/39.2 50h 11.0/10.2 27.0/24.9 20h 12.6/12.7 26.2/26.2 Whole(120h) 9.9/8.6 23.5/21.7 50h 8.6/8.7 18.0/18.0 Whole(92h) 7.7/7.7 16.0/16.1 Table7: E2E-ConformerResultsonanotherEL:High- landPueblaNahuatl(DifferentCorpusSize) Table5: E2E-ConformerResultsonDifferentCorpus Size signiﬁcantly. </Abstractive Summary>  <Extractive Summary> =  We (e) The Framework Generalizability: To test alsotrainedseparateRNNlanguagemodelsforfu- theend-to-endASRsystems’generalizationability, sionandunigramlanguagemodelstoextractword weconductedthesameend-to-endtrainingandtest piecesfordifferenttranscriptionlevels. proceduresonanotherendangeredlanguage: High- Table 4 shows the E2E-Conformer results for landPueblaNahuatl(high1278;azz). Thiscorpus both underlying and surface transcription levels. </Extractive Summary>  </Table ID = 4>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  For this language the E2E-Conformer (d) Comparison with Different Corpus Sizes: again offers better performances over the other As introduced in Section 1, most ELs are consid- models. Table 7 shows the E2E-Conformer per- eredlow-resourceforASRpurposes. Tomeasure formances on different amounts of training data the impact of resource availability on ASR accu- forHighlandPueblaNahuatl. </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Character Error-type Distribution of Novice Character andASR(bynumberoferrors) Alignment Character Rules 4 NoviceTranscriptionCorrection HybridTranscription Finally, this paper presents novice transcription correction(NTC)asataskforELdocumentation. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Compared to the paired inedforthefusion: avoting-basedsystem(Fiscus, experttranscription,thenoviceachievedaCERof 1997)andarule-basedsystem. 6.0% on clean-dev, deﬁned in Table 1. However, Thevoting-basedsystemfollowsthedeﬁnition it is not feasible to spend many months training in (Fiscus, 1997) that combines hypotheses from speakerswithnoliteracyskillstoacquirethetran- differentASRmodelswithnovicetranscription. </Extractive Summary>  </Table ID = 1>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  basedmethodthatcombineshypothesesfromthe novice and end-to-end ASR systems. Empirical 5.2 Results studiesontheYMC-NTcorpusindicatethatboth As shown in Table 9, voting-based methods and methodssigniﬁcantlyreducetheCER/WERofthe rule-based methods all signiﬁcantly reduce the novicetranscriptionforcleanspeech. novice errors for clean speech.12 However, for Theabovediscussionsuggeststhatausefulap- thenoise-test,thenovicetranscriptionisthemost proach to EL documentation using both human robustmethod. languages for language documentation. In LREC Table 9 shows that our proposed rule-based and 2018 (Language Resources and Evaluation Confer- voting-basedfusionmethodscanpotentiallyelim- ence),pages3356–3365. inate the errors that come from the novice tran- AntoniosAnastasopoulosandDavidChiang.2017. </Extractive Summary>  </Table ID = 9>  </Paper ID = 96> 

<Paper ID = 97>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Theserespondentspro- chunk-levelfeatures. Allfeaturesaredescribedin videdpre-existingwritingsamplesandaresponse Table 1. For sentence-level features, white space totheessaypromptmentionedabove. </Extractive Summary>  </Table ID = 1>  </Paper ID = 97> 

<Paper ID = 98>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Comparison on interpolation. </Abstractive Summary>  <Extractive Summary> =  Following prior work (Fu method for comparison in interpolation and gen- etal.,2019;Lietal.,2019),weutilizetheYelpsen- eration experiments. Table 2 shows the decoded timentdatasetaspreprocessedinShenetal.(2017). samples. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Besides the standard LM and the vanilla VAE k 1 2 3 4 withKLweightannealing,VAEswithrecentstate- AE 26.05 40.46 52.77 63.07 of-the-art training techniques, Cyclical-VAE (Fu Anneal-VAE 32.20 32.65 33.12 33.39 etal.,2019),Lagging-VAE(Heetal.,2019),SA- Cyclical-VAE 31.83 32.87 33.73 34.38 VAE (Kim et al., 2018), FBP-VAE and FB-VAE Lagging-VAE 31.78 31.99 32.21 32.32 (Lietal.,2019)arealsoincludedforcomparison. SA-VAE 31.63 31.82 32.15 32.46 The results are displayed in Table 1. In terms of FBP-VAE 29.93 32.59 34.90 36.77 PPL,ourmethodoutperformsallthebaselineson FB-VAE 27.92 29.12 30.03 30.85 the PTB and Yahoo datasets, while does slightly Ours 27.12 28.66 30.21 30.46 worsethanLagging-VAEandperformsbetterthan other baselines on the SNLI. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Noisy reconstruction loss on SNLI. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Accuracy on Yelp of unsupervised and semi- supervisedclassiﬁcationasafunctionofthenumberof labeledexampleduringtraining. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  4.2.2 Generation 5 Conclusion Wesamplefromthepriordistributionanddecode the sentences in a greedy manner. Table 3 dis- Thisworkproposestouseshortruninferencedy- playsthesamplesfromourmodelandFB-VAE.It namics to infer latent variables in text generative 1163models. SRIdynamicsisalwaysinitializedfrom ConferenceonMachineLearning,ICML2018,Stock- thepriordistributionofthelatentvariableandthen holmsma¨ssan, Stockholm, Sweden, July 10-15, 2018, pages1086–1094. </Extractive Summary>  </Table ID = 3>  </Paper ID = 98> 

<Paper ID = 99>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Our manually annotated datasets show that LR 0.651 0.603 0.629 0.638 Transcripts themajorityofpodcastepisodesdocontainextraneous (Gold) SVM 0.675 0.679 0.663 0.670 content,makingupasizeablefractionofallsentences. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Classiﬁcation accuracy of sentence-level and tively,excludingthetestset)tocapturethedistinc- document-level extraneous content detection. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Hereisthehypothesistest: A.1 AnnotationProcessandAnnotated H : θ = θ = ... = θ = θ (2) 0 1 2 N−1 N Examples Examplesofextraneouscontentregionsofepisode descriptions are shown in Table 6. and example H : θ = ... </Extractive Summary>  </Table ID = 6>  </Paper ID = 99> 

<Paper ID = 100>  <Table ID = 1>  <Abstractive Summary> =  Table 1: F1 for argument mining using deep struc- tured prediction, Avg results using shallow models Dataset: We used the UKP dataset (Stab and includedinparenthesis Gurevych, 2017), consisting of 402 documents, withatotalof6,100propositionsand3,800links Wecananalyzetheresultsacrossthreedimensions: (17%ofpairs). </Abstractive Summary>  <Extractive Summary> =  Weusethetrain/dev/testsplitsused byNiculaeetal.(2017),andreportmacroF1for Structured Learning: The advantage of lever- componentsandpositiveF1forrelations. aging more structural dependencies can be seen LearningandRepresentation: Wedid5repeti- in Table 1. The model gets increasingly better tionsandreportedtheaverageperformance. Shallow: There is a consistent trend localmodels,weusedalearningrateof0.05and showingthatdeepstructuredmodelsaremoreex- forstructuredlearningweusedalearningrateof pressivethantheirshallowcounterparts,aswecan 1e-4. Similarlytopreviousworkondeepstructured see by comparing average results in Table 1. To prediction (Han et al., 2019), we obtained better obtain good results using linear classiﬁers, Stab resultsbyperformingstructuredlearningoverlo- and Gurevych (2017) relied on an exhaustive set callytrainedmodels,insteadoftrainingthemfrom of features (Table 2). </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Post stance on 4FORUMS. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 100> 

<Paper ID = 101>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Precision (Prec.) and bootstrap 80% conﬁ- two event mentions by measuring similarity denceinterval(80%CI)scoreofprecisionforacquired betweentheireventembeddings(b ,b )through e1 e2 eventpairsbasedonhumanevaluation. </Abstractive Summary>  <Extractive Summary> =  Then,weaskedahuman element-wiseproductanddifferencesoftwoevent annotatortovalidateallthe1000samplesmanually. mention embeddings followed by the sigmoid activation, and multiply them with context em- Table 2 shows the precision and bootstrapped bedding C. Finally, we concatenate the resulting 80%conﬁdenceintervalofprecisionforeventpairs set of embeddings and then use a three-layer fromeachcategory. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Results for event coreference resolution systems on the KBP 2017 and RED corpora. </Abstractive Summary>  <Extractive Summary> =  allmodelsonKBP2017newsarticlescorpus. The mention-pairmodeltrainedonKBP2015corpus In the third segment of Table 3, we compare usingpre-trainedlanguagemodelandlargerevent the performance of all models on a different text context outperforms both local feature-based as genre by evaluating them on the discussion fo- wellasthediscourse-structureawarepreviousbest rumdocumentsfromtheKBP2017corpus. With model(ChoubeyandHuang,2018),outperforming sharedeventtypes,themodeltrainedonKBP2015 ChoubeyandHuang(2018)by2.26pointsinaver- achievesthebestresultwith1.76pointsimprove- ageF1score. </Extractive Summary>  </Table ID = 3>  </Paper ID = 101> 

<Paper ID = 102>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Dataset Statistics: For each dataset, T indic- tates the number of tables, R the average number of rowspertable,Ctheaveragenumberofcolumnsperta- ble,EthenumberofuniqueentitiesandT thenumber ofuniquetypesusedforannotation,E∗ thenumberof newuniqueentitiesandT∗ thenumberofnewunique typesnotseenintrainingannotations. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Table clustering performance using NMI TabGCN 0.33 0.24 0.84 0.83 0.84 0.79 forTabGCN,Tab2VecandTaBERTonT2Dv2andLi- maye. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Ablation study for entity and type detection has40tablecategoriesandLimaye15. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 102> 

<Paper ID = 103>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  InstructionStyle Thenavigationinstructionsvary 3 Methods acrossdifferentoutdoorVLNdatasets. Asshown in Table 1, the instructions generated by Google 3.1 TaskDeﬁnition Maps API is template-based and mainly consists Inthevision-and-languagenavigationtask,therea- ofstreetnamesanddirections. Incontrast,human- soningnavigatorisaskedtoﬁndthecorrectpathto annotated instructions for the outdoor VLN task reachthetargetlocationfollowingtheinstructions emphasize the visual environment’s attributes as (a set of sentences) X = {s ,s ,...,s }. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: S_SPD and F_SPD denotes the average SPD totheSpeaker,theevaluationperformanceonall valueonsuccessandfailurecasesrespectively. </Abstractive Summary>  <Extractive Summary> =  navigation. Table 4 compares the SPD values on success TC, SPD, and SED are deﬁned by Chen et al. andfailurenavigationcases. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Ablation study of the multimodal text style transfer model on the outdoor VLN task. </Abstractive Summary>  <Extractive Summary> =  WecompareourVLNTransformer ﬁne-tuneitontheTouchDowndataset. withthebaselinemodelanddiscusstheinﬂuence According to the navigation results in Table 3, ofpre-trainingonexternalresourceswith/without the instructions generated by the Speaker model instructionstyletransfer. misguidethenavigationagent,indicatingthatrely- ingsolelyontheSpeakermodelcannotreducethe Outdoor VLN Performance We compare our gap between different instruction styles. </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Navigation results on the outdoor VLN task. </Abstractive Summary>  <Extractive Summary> =  The style- Both baseline models encode the trajectory and modiﬁed instructions improve the agent’s perfor- theinstructioninanLSTM-basedmanneranduse mance on all the navigation metrics, suggesting supervised training with Hogwild! (Recht et al., that our Multimodal Text Style Transfer learning 2011). Table 2 presents the navigation results on approachcanassisttheoutdoorVLNtask. theTouchdownvalidationandtestsets,whereVLN TransformerperformsbetterthanRCONCATand QualityoftheGeneratedInstruction Weeval- GAonmostmetricswiththeexceptionofSPDand uate the quality of instructions generated by the CLS. </Extractive Summary>  </Table ID = 2>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Even thoughtheTouchdowndatasetandtheStreetLearn WecompareVLNTransformerperformancewith datasetarebuiltuponGoogleStreetView,andboth andwithoutsplittingtheinstructionsintosentences ofthemcontainurbanenvironmentsinNewYork during encoding. Results in Table 8 show that City,pre-trainingthemodelwiththeVLNtaskon breaking the instructions into multiple sentences the StreetLearn dataset does not raise a threat of allows the visual views and the guiding signals testdataleaking. Thisisduetoseveralcauses: in sub-instructions to attend to each other during cross-modal encoding fully. </Extractive Summary>  </Table ID = 8>  </Paper ID = 103> 

<Paper ID = 104>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Finally,theECOLmodelcon- threshold for the copy decision during inference. catenatestheabovereﬁnedprobabilitiesasfollows: Surprisingly, this simple inference trick provides P(yv) = v (8) a strong baseline (see Table 3). This shows that t t aftertheCEtraining,manycorrectcopyoperations P(yli) = c ·P(yli|l ) (9) t t,i t i are assigned with low probabilities, compared to P(y ) = P(yv)(cid:12)P(yl1)(cid:12)···(cid:12)P(ylkf) (10) the ﬁxed vocabulary items. Themodeltrainedwith theR rewardcopiesmoreobjectsthantheR re- a p 4.3 AblationStudy ward,especiallytrainingwithalltrainingimages. Table 3 presents ablation results for various ThisisbecausetheR rewardassignsconstantpos- a ECOL-Rcomponents,includingourcopyencour- itiverewardforallcopiedobjects. However,such agementapproach. </Extractive Summary>  </Table ID = 3>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Comparison of the ECOL-R model with other state-of-the-art systems on the nocaps Test Split. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: The CIDEr Score in nocaps Validation Set ECOL-R 83.1 11.0 85.1 12.2 withdifferentInferenceBias. </Abstractive Summary>  <Extractive Summary> =  Weapplyan orcopy). OptimizingeithertheR orR reward increasingamountofinferencebiastotheECOL, a p functions improves the ECOL + CIDEr model by ECOL + CIDEr and ECOL-R models in Table 4. 7.0CIDErand7.8CIDErrespectively(row7and WenotethatonlyECOL-Rmodelisnegativelyim- 5). </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: The contribution of Abstract Label (AL) and score. </Abstractive Summary>  <Extractive Summary> =  8). Table 5 shows the effect of Abstract Labels (AL) EffectivenessofCopyEncouragement: Wedi- and the M Selector (M) in the ECOL + IB and rectlymeasurethecopyquantitybycountingthe ECOL-Rmodels. RemovingALandMfromthe numberofcopiedobjectlabelsandObjectCIDEr. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  et al. (2019) in Table 6. Both of our models out- PeterAnderson,BasuraFernando,MarkJohnson,and performstheUp-DownandNBTmodelbyalarge Stephen Gould. </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The hyper-parameters of the ECOL-R model is TingYao,YingweiPan,YehaoLi,andTaoMei.2017. shown in Table 7. This architecture is basically Incorporatingcopyingmechanisminimagecaption- from (Cornia et al., 2020). </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  Table 8: The performance of ECOL + IB, ECOL-R and ECOL-R + CBS model on held-out COCO Benchmark ValidationSet. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  Table 9: Data Statistics for the nocaps Benchmark. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 9>  </Paper ID = 104> 

<Paper ID = 105>  <Table ID = 4>  <Abstractive Summary> =  Table 4: The performance with respect to using differ- WMT14De→En,WMT16En→RoandWMT16 ent layer of intermediate interaction. </Abstractive Summary>  <Extractive Summary> =  modelstomeasurethespeedup.Thelatenciesare Effect of Intermediate Representation Align- obtained by taking an average of ﬁve runs. More ment We conduct experiments for our SNAT clearly,WereproducetheTransformeronourma- model on WMT14 En→De with various align- chine.Wecopytheruntimeofothermodelsbutthe mentsbetweendecoderlayersandtarget.Asshown speedupratioisbetweentheruntimeoftheirimple- in Table 4, using the second layer Z2 in the de- mentedTransformerandtheirproposedmodel.We coder as intermediate alignment can gain +1.21 thinkit’sreasonabletocomparethespeedupratio improvement,whileusingthethirdlayerZ3 inthe because it is independent of the inﬂuence caused decoderasintermediatealignmentcangain+1.46 bydifferentimplementationsoftwareormachines. improvement.Thisisinlinewithourexpectation Andtoclarify,thelatencydoesnotincludeprepro- that aggregating layer-wise token information in cessingoftagging,becauseit’saveryfastprocess intermediatelayerscanhelpimprovethedecoder’s asexecutingaround7000sentencesinonesecond. </Extractive Summary>  </Table ID = 4>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  abilitytocapturetoken-tokendependencies. We can see from Table 2 that the best SNAT getsa9.3timesdecodingspeedupthantheTrans- Effect of Sentence Length To evaluate differ- former,whileachievingcomparableorevenbetter entmodelsondifferentsentencelengths,wecon- performance.ComparedtootherNATmodels,we ductexperimentsontheWMT14En→Dedevelop- observethattheSNATmodelisalmostthefastest mentsetanddividethesentencepairsintodifferent (only a little bit behind of ENAT and Hint-NAT) lengthbucketsaccordingtothelengthoftherefer- intermsoflatency,andissurprisinglyfasterthan encesentences.AsshowninTable5,thecolumn DCRF-NATwithbetterperformance. of100calculatestheBLEUscoreofsentencesthat thelengthofthereferencesentenceislargerthan 4.4 AblationAnalysis 50 but smaller or equal to 100. </Extractive Summary>  </Table ID = 2>  </Paper ID = 105> 

<Paper ID = 106>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Top 4-grams preﬁxes of questions in thepredictedspanscorrespondingtoasinglesam- NLQuAD.Even’What’questionsarenon-factoidand needlongeranswers(descriptionsoropinions) pleareaggregatedtopredicttheﬁnalspanthatis thespanbetweentheearlieststartpositionandthe latestendposition. </Abstractive Summary>  <Extractive Summary> =  Therefore, we propose to use Intersection over NLQuAD’sanswersareopenandnotpredeﬁned. Union(IoU)measuringposition-sensitiveoverlap Figure 3 and Table 3 present our question types. betweentwospans. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ternrepeatsfortheROUGE-Nscores. ROUGE-1 We analyze the performance of BERT and similar to F1 can reach 40% while IoU=0, but RoBERTawithdifferenthyper-parametersonthe ROUGE-2 and ROUGE-L are less prone to such development set in Table 5. Smaller strides, i.e., over-estimationduetolowerchanceofoverlapof higheroverlapbetweenthesegments,andwarm-up bigramsthanunigramsandshorterLCSsintworan- contributetobetterperformances. </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Comparing human performance with Long- batchsizeof12(batchsizeof1andgradientaccu- former and RoBERTa-large on a subset of evaluation mulationover12batches)andlearningratewarm- set. </Abstractive Summary>  <Extractive Summary> =  Fig- answer a question. Table 7 compares human per- ure 6 shows that the target answers are preferred formancewithLongformerandRoBERTa-largeon in37%and64%ofcasesovertheLongformerand thesamesubset. SimilartoHotpotQA(Yangetal., RoBERTa predictions, respectively. </Extractive Summary>  </Table ID = 7>  </Paper ID = 106> 

<Paper ID = 107>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  PreservingInformation thelossesoveralltokensinasentence. Table 1 shows the results on SEAT and GLEU Totesttheimportanceofcarefullyselectingthe where original denotes the pre-trained contextu- target words considering the types of biases that alisedmodelspriortodebiasing. Weseethatorigi- wewantto remove fromthe embeddings, we im- nalmodelsotherthanELECTRAcontainsigniﬁ- plement a random baseline where we randomly cantlevelsofgenderbiases. Then, they proposed withtheresultsreportedinTable1andshowsthat m m m the following measures to quantify the bias: (1) debiasingalllayersismoreeffectivethanonlythe NetNeutral(NN):NN = 1 (cid:80)M n ;(2)Frac- ﬁrstlayerasdonebyDevetal.(2020). M m=1 m tion Neutral (FN): FN = 1 (cid:80)M 1[neutral = M m=1 4.5 TheImportanceofDebiasingAllLayers max(e ,n ,c )];and(3)Thresholdτ (T:τ): T:τ m m m = 1[n ≥ τ], where we used τ = 0.7 following In Table 1, we investigated the bias for the ﬁnal m Devetal.(2020). Foranideal(bias-free)embed- layer, but it is known that the contextualised em- ding,allthreemeasurewouldbe1. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Averaged scores over all layers in an embed- dingdebiasedattoken-level,measuredonSEATtests. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  This con- ELECTRA last 0.34 0.20 0.21 ﬁrmsthattheproposeddebiasingmethodpreserves ﬁrst 0.28 0.13 0.34 sufﬁcient semantic information contained in the originalembeddingsthatcanbeusedtolearnac- Table 3: Averaged scores over all layers in an embed- dingdebiasedattoken-level,measuredonSEATtests. curatepredictionmodelsforthedownstreamNLP tasks.5 However,theperformanceofRoBERTaand ALBERTdecreasesigniﬁcantlycomparedtotheir In Table 2, we compare our proposed method originalversionsafterdebiasing. Wesuspectthat against the noncontextualised debiasing method thesemodelsaremoresensitivetoﬁne-tuningand proposed by Dev et al. Inadditiontotheabove-mentioned measures,wealsoreporttheentailmentaccuracy 4.4 MeasuringBiaswithInference onthematched(in-domain)andmismatched(cross- Following Dev et al. (2020), we use the multi- domain) denoted respectively by MNLI-m and genre co-reference-based natural language infer- MNLI-mm in Table 2 to evaluate the semantic ence (MNLI) dataset for evaluating gender bias. informationpreservedintheembeddingsafterde- This dataset contains sentence triples where a biasing. </Extractive Summary>  </Table ID = 2>  </Paper ID = 107> 

<Paper ID = 108>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Puttingtheinferencecandidate as for the other RoBERTa-base approaches and (b) into a pattern-generated context avoids being include results forthe best conﬁguration (chosen fooledbythehighsimilarityofthetwosentences. on dev ) in Table 3. We ﬁnd that manually cu- 2 Only the handcrafted patterns can make sense of ratingautomaticallyrankedpatternshelpsperfor- theimportantdetailsinthisconstruction. </Extractive Summary>  </Table ID = 3>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 108> 

<Paper ID = 109>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  COARSE2FINEisthebestperformingsuper- vised model on the standard split of GEOQUERY 5.1 ResultsandDiscussion andATISdatasets. PT-MAMLisafew-shotlearn- Table 2 shows the average accuracies and signif- ing semantic parser that adopts Model-Agnostic icance test results of all parsers compared on all Meta-Learning (Finn et al., 2017). We adapt PT- three datasets. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Infact,the average accuracy. As shown in Table 3, remov- proportionofpredicatespresentininpututterances inganyofthecomponentsalmostalwaysleadsto statisticallysigniﬁcantdropofperformance. The is only 42%, 38% and 44% on JOBS, ATIS, and correspondingp-valuesarealllessthan0.00327. </Extractive Summary>  </Table ID = 3>  </Paper ID = 109> 

<Paper ID = 110>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In 4.1.1 Results theexample,sincethereisnomatchinthedatabase We ﬁrst test our method on a restaurant search satisfyingtheconstraint”cheapfrenchrestaurant”, dataset, CamRest676 (Wen et al., 2017). Table 1 it returns ”0” and the domain ”restaurant”. Then showsallmodels’resultswithgroundtruthbelief the system can condition on the database state to stateorgeneratedbeliefstate. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Results on MultiWOZ. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 110> 

<Paper ID = 111>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Human wayﬁnding performance following instructions from the Speaker-Follower (Fried et al., 2018) and EnvDrop (Tan et al., 2019) models, compared to Crafty (template-based) instructions, Human instructions, andthreeadversarialperturbationsofHumaninstructions(Direction,EntityandPhraseSwap). </Abstractive Summary>  <Extractive Summary> =  To understand whetherour37humanraterslearntoself-correct the perturbed instructions over time and whether that affects the quality of our human wayﬁnding results,weinvestigateraterperformanceasafunc- tionoftimeusingthesequenceofexamplesthey evaluate. Figure5showstheaveragehumanraterperfor- mance for all of the 9 datasets included in Table 1 of Section 3. Due to the binary nature of SR, we use a 50-pointbin to average eachrater’s per- formance, and then average the results across all ratersforeachbin. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  mance. Overall,trajectoryperturbationsgethigher scoresthaninstructionperturbations,showingthey Results Table 2 reports classiﬁcation AUC in- are easier tasks. Phrase Swap proves the hardest cludingcomprehensiveablationsoflossfunctions, task,whileRandomWalkistheeasiest. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  SDTW3-agents (-0.22,-0.19) (0.20, 0.24) (0.18, 0.22) (0.11, 0.15) Compatibility (-0.20,-0.17) (0.13, 0.17) (0.17, 0.20) (0.19, 0.23) Theinstance-levelevaluationsassesswhetherthe Model-GeneratedInstructions(N=2.2k,M=4) metric can identify the best instruction from two Score Ref NE↓ SR↑ SPL↑ Quality↑ candidates,whilethesystem-levelevaluationsas- BLEU-4 (cid:88) (-0.02, 0.03) (-0.03, 0.02) (-0.02, 0.03) (-0.02, 0.03) sesswhetherametriccanidentifythebestmodel CIDEr (cid:88) (-0.02, 0.03) (-0.03, 0.02) (-0.02, 0.03) (-0.02, 0.03) from two candidates (after averaging over many LevelMROEUTGEOER (cid:88)(cid:88) ((--00..0022,, 00..0033)) ((--00..0035,, 00..0020)) ((--00..0024,, 00..0031)) ((--00..0023,, 00..0032)) instructionscoresforeachmodel). Theresultsin ance-SBPEIRCTEScore(cid:88)(cid:88) ((--00..0252,,--00..0108)) ((00..0109,, 00..0254)) ((00..0108,, 00..0253)) ((00..0116,, 00..0260)) Table 3 are reported separately over all 3.9k in- nstSPL1-agent (-0.21,-0.16) (0.17, 0.23) (0.16, 0.22) (0.07, 0.12) structions(9systemscomprisingtherowsofTable ISPL3-agents (-0.26,-0.21) (0.21, 0.27) (0.21, 0.26) (0.09, 0.14) SDTW1-agent (-0.22,-0.16) (0.17, 0.23) (0.16, 0.22) (0.07, 0.13) 1),andovermodel-generatedinstructionsonly(4 SDTW3-agents (-0.26,-0.21) (0.22, 0.27) (0.21, 0.26) (0.10, 0.15) Compatibility (-0.25,-0.20) (0.22, 0.27) (0.21, 0.25) (0.18, 0.23) systemscomprisingthe2.2kinstructionsgenerated bytheSpeaker-FollowerandEnvDropmodelson Table3: Kendall’sτ correlationbetweenautomatedin- structionevaluationmetricsandhumanwayﬁndereval- R2Rval-seenandval-unseen). uations. agentortheaveragescorefromthreeagents. Tocalculatethestandardmetricsweusetheofﬁ- Results Table 3 compares system-level and cialevaluationcodeprovidedwiththeCOCOcap- instance-levelcorrelationsforallmetrics,bothstan- tionsdataset(Chenetal.,2015). ForBERTScore, dard and model-based. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Automated metric scores for all instructions. </Abstractive Summary>  <Extractive Summary> =  We provide more details about automated metric Fromthese,theWalkercreatesasequenceofmo- scores for all instructions in this section. Table 4 tiontuples,eachofwhichcapturesthecontextof gives automated metrics for each model we con- thesourcepanoramaandthegoalpanorama,along sider. Generated instructions from EnvDrop and withtheheadingtomovefromsourcetogoal. </Extractive Summary>  </Table ID = 4>  </Paper ID = 111> 

<Paper ID = 112>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Masked last-word prediction accuracies: ofinputsequences,averagedacrossthe2016testset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Entity masking on 2016 test set: results are agreementwiththequantitativemetrics(Table1), BLEUaveragesofthreeﬁne-tunedMTsystems. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 112> 

<Paper ID = 113>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Train-test similarity using unigrams (uni), bi- ChE 4I 30.0 96.7 96.7 96.7 grams(bi),trigrams(tri). Table 2: evenonthetrainingdataforthesesamples. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Performances on various similarity thresh- R BioBERT 79.8 76.7 77.9 oldsandthecorrespondingpercentageoftestinstances within the intervals. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  5.2 Modelperformanceandsimilarity (90.6→77.5) from 3I to 4I is caused by six in- We observe drops in F-scores of more than 10 stanceswhoseinputtextshaveexactmatchesinthe pointsbetweenAIMed(R)andAIMed(U)across trainset(fullsamplesshowninAppendixTable11). all three models as shown in Table 3. This is in Thisimpliesthatthemodeldoesn’tperformwell line with the similarity measurement in Table 2: evenonthetrainingdataforthesesamples. The minimum (Min) and the maximum (Max)similaritywithineachquartilearealsoreported. thetestsetinputtextissimilartothetrainingset as shown in Table 3 and 4. While this might be dropsfrom85.8→77.5. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ter if it memorizes or generalizes in a real world Wealsosplitthetestsetsintofourequal-sized context,andamodel’sabilitytomemorizeisnot quartiles based on the similarity ranking of test necessarily a disadvantage. However, in the set- instances, shown in Table 5. We observe similar tingofasharedtask,weoftendonothaveaccess phenomena as in the previous set of experiments to sufﬁciently large training data sets and hence for the dataset BC2GM, ChEMU, and BC3ACT. </Extractive Summary>  </Table ID = 5>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  robustassessmentofmemorizationvs.generaliza- However, this approach may detect similarity tion capabilities of models. Further development evenwhenthemeaningsaredifferent,especiallyin ofapproachestostructuredconsiderationofmodel thecaseofclassiﬁcationtasksasshownforSST2 performanceunderdifferentassumptionswillim- in Table 1. Semantic Text Similarity (STS) mea- proveourunderstandingofthesetradeoffs. </Extractive Summary>  </Table ID = 1>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  <corpus source="AIMed"> <document id="AIMed.d0"> <sentence id="AIMed.d0.s0" text="Th1/Th2 type cytokines in hepatitis B patients treated with interferon-alpha." seqId="s0" > <entity id="AIMed.d0.s0.e0" charOffset="60-75" type="protein" text="interferon-alpha" seqId="e0"/> </sentence> <sentence id="AIMed.d0.s1" text="OBJECTIVE: To investigate the relationship between the expression of Th1/Th2 type cytokines and the effect of interferon-alpha therapy." seqId="s1"> <entity id="AIMed.d0.s1.e0" charOffset="110-125" type="protein" text="interferon-alpha" seqId="e1"/> </sentence> </document> <document id="AIMed.d1"> <sentence id="AIMed.d1.s11" text="Involvement of BMP-2 signaling in a cartilage cap in osteochondroma." seqId="s11"> <entity id="AIMed.d1.s11.e0" charOffset="15-19" type="protein" text="BMP-2" seqId="e15"/> </sentence> </document> </corpus> B ClasswisesimilarityforBC3AST Thetestsethas5090negativesamplescomparedto910positivesamples,with2.96pointshighermean similarityinpositivesamples. Testlabel Unigram Bigram Trigram 0 count 5090.00 5090.00 5090.00 mean 26.31 6.70 1.73 std 9.25 5.35 1.72 min 6.28 0.00 0.00 25% 19.70 3.29 0.79 50% 25.16 5.07 1.39 75% 31.53 8.29 2.27 max 75.01 41.75 18.71 1 count 910.00 910.00 910.00 mean 29.27 8.09 2.26 std 9.36 6.00 1.73 min 11.14 1.52 0.00 25% 22.69 4.51 1.17 50% 28.31 6.25 1.88 75% 34.32 9.38 2.84 max 74.01 51.20 18.97 Table6: Class-wisesimilarityforBC3ACTdataset C BERTandsimilaritythresholds Table 7 shows the impact on precision, recall and F-score using different similarity thresholds on the BC2GMtestset,whichhasapproximately6,300annotations. We also compare the recall when the target annotations are similar as shown in Table 8. </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Testlabel Unigram Bigram Trigram 0 count 5090.00 5090.00 5090.00 mean 26.31 6.70 1.73 std 9.25 5.35 1.72 min 6.28 0.00 0.00 25% 19.70 3.29 0.79 50% 25.16 5.07 1.39 75% 31.53 8.29 2.27 max 75.01 41.75 18.71 1 count 910.00 910.00 910.00 mean 29.27 8.09 2.26 std 9.36 6.00 1.73 min 11.14 1.52 0.00 25% 22.69 4.51 1.17 50% 28.31 6.25 1.88 75% 34.32 9.38 2.84 max 74.01 51.20 18.97 Table6: Class-wisesimilarityforBC3ACTdataset C BERTandsimilaritythresholds Table 7 shows the impact on precision, recall and F-score using different similarity thresholds on the BC2GMtestset,whichhasapproximately6,300annotations. We also compare the recall when the target annotations are similar as shown in Table 8. We only compareunigrams,asthenumberoftokensinagenenametendstobesmall(onaveragelessthan3). </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  Table 9: SST2 and BC3ACT similarity thresholds using ngram N = 1,2 and 3. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 9>  </Paper ID = 113> 

<Paper ID = 114>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The labelsas‘Other’. Theythenaugmentedthedataset sourcesusedtoidentifythesesubredditsareavail- by collecting tweets using keyword sampling on able in Table 9 in the Appendix. We then iden- benevolentlysexistphrases(e.g. support group for former members of the miso- Theirworkhighlightstheneedforgreaterattention gynistic subreddit r/TheRedPill. Table 9 in 1337theAppendixliststhe34targetedsubredditsand instance, a Misogynistic entry could be assigned the number of entries and threads for each in the labelsforbothaPejorativeandTreatment. dataset. </Extractive Summary>  </Table ID = 9>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Treatment 103 1.6% We also calculated level two category-wise Derogation 285 4.3% Fleiss’ Kappas for each of the 17 sets of an- Personalattack 35 0.7% notator groups, then took the mean across all groups (Ravenscroft et al., 2016). Table 1 shows Total 696 10.6% the breakdown of Kappas per category. There was greatest agreement for Misogynistic pejora- Table2: BreakdownofMisogynisticcategorycounts tives(k=0.559)downtothelowestagreementfor Misogynisticpersonalattacks(k=0.145). </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Breakdown of Non-misogynistic category Nonmis. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Breakdown of Misogynistic and Nonmisogy- nisticpersonalattacksbyGenderofthetarget gationisalmosttwiceascommonasexplicit(182 vs103). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Breakdown of Misogynistic Derogation sub- were accepted during the adjudication meetings. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Confusion matrix of the weighted BERT coveragreaterrangeofformsofdiscourse,includ- model ingbothnon-misogynisticdiscussionsofwomen andawidervarietyofmisogynisticspeech. </Abstractive Summary>  <Extractive Summary> =  13427.1 Erroranalysis Wemakeuseofthemoregranularsecondarylabels in our taxonomy to conduct an error analysis for the weighted BERT model. Table 7 shows the confusionmatrixforthe1,277entriesinthetestset. Overall,137entries(10.7%)weremisclassiﬁed. </Extractive Summary>  </Table ID = 7>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Otherfalsepositivesmentionwomenindirectly. “Becausetheyaren’tmen,theyareSIMPS”.‘Simp’ Performance of the three models is shown in is a pejorative term used in the manosphere for a Table 6. All models perform poorly on miso- man who cares too much about a woman. </Extractive Summary>  </Table ID = 6>  </Paper ID = 114> 

<Paper ID = 115>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Illustration how sentences in the wrong lan- sentencesonly; 2) itseemsreasonableto assume guagecanhurtthealignmentprocesswithamargincri- terion. </Abstractive Summary>  <Extractive Summary> =  Thisglobalmininghastheadvantagesthat thanapotentialtranslationinthetargetlanguage. wecantrytoaligntwolanguageseventhoughthere Table 1 illustrates this problem. The algorithm areonlyafewarticlesincommon. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Comparison of NMT systems trained on the German/English,German/French,Czech/German Europarlcorpusandonbitextsautomaticallyminedin andCzech/French. </Abstractive Summary>  <Extractive Summary> =  2018. When and whyarepre-trainedwordembeddingsusefulforneu- ralmachinetranslation? InProceedingsofthe2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), 1360A Appendix Table 2 gives the detailed conﬁguration which wasusedtotrainNMTmodelsonthemineddata Table 5 provides the amounts of mined parallel in Section 5. An 5000 subword vocabulary was sentencesforlanguageswhichhavearathersmall learntusingSentencePiece(KudoandRichardson, Wikipedia. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  withthesameminedbitextsL /L . Scoresonthe 1 2 We use a mining approach based on massively test sets were computed with SacreBLEU (Post, multilingual sentence embeddings (Artetxe and 2018), see Table 4. Some additional results are Schwenk,2018b)andamargincriterion(Artetxe reportedinTable6intheannex. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  2018. When and whyarepre-trainedwordembeddingsusefulforneu- ralmachinetranslation? InProceedingsofthe2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), 1360A Appendix Table 2 gives the detailed conﬁguration which wasusedtotrainNMTmodelsonthemineddata Table 5 provides the amounts of mined parallel in Section 5. An 5000 subword vocabulary was sentencesforlanguageswhichhavearathersmall learntusingSentencePiece(KudoandRichardson, Wikipedia. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Itisalsolikely --arch transformer thatseveralofthesealignmentsareoflowquality --share-all-embeddings --encoder-layers 5 since the LASER embeddings were not directly --decoder-layers 5 trainedonmosttheselanguages,butwestillhope --encoder-embed-dim 512 toachievereasonableresultssinceotherlanguages --decoder-embed-dim 512 --encoder-ffn-embed-dim 2048 ofthesamefamilymaybecovered. --decoder-ffn-embed-dim 2048 --encoder-attention-heads 2 --decoder-attention-heads 2 --encoder-normalize-before --decoder-normalize-before ISO Name Language size cadadeen es fr it nl pl pt svruzhtotal Family --dropout 0.4 --attention-dropout 0.2 an Aragonese Romance 222 24 71223331613 91014 911 6 324 arz Egyptian Arabic 120 7 61118121210 8 910 812 7 278 --relu-dropout 0.2 Arabic --weight-decay 0.0001 as Assamese Indo-Aryan 124 8 611 7111210 9 9 8 8 9 3 216 --label-smoothing 0.2 azb South Azer-Turkic 398 6 4 9 8 910 9 7 6 8 6 7 3 172 baijani --criterion label smoothed cross entropy bar Bavarian Germanic 214 7 64116121210 8 910 810 5 261 --optimizer adam bpy BishnupriyaIndo-Aryan 128 2 1 4 4 3 4 2 2 3 2 2 3 1 71 br Breton Celtic 413 2016222322 19 16 6 200 --adam-betas ’(0.9, 0.98)’ ce Chechen Northeast 315 2 1 2 2 2 2 2 2 2 2 2 2 1 56 --clip-norm 0 Caucasian ceb Cebuano Malayo- 17919 14 922292724241517205521 9 594 --lr-scheduler inverse sqrt Polynesian --warmup-update 4000 ckb CentralKur-Iranian 127 2 2 6 8 5 5 4 4 4 4 3 6 4 113 --warmup-init-lr 1e-7 dish cv Chuvash Turkic 198 4 3 5 4 6 6 7 5 4 6 5 8 2 129 --lr 1e-3 --min-lr 1e-9 dv Maldivian Indo-Aryan 52 2 2 5 6 4 4 3 3 3 3 3 5 3 96 --max-tokens 4000 fo Faroese Germanic 114 131214322118151111171213 6 335 fy Western Germanic 493 13 816322118173812181314 5 453 --update-freq 4 Frisian --max-epoch 100 gd Gaelic Celtic 66 1 1 1 1 1 1 1 1 1 1 1 1 1 41 ga Irish Irish 216 2 3 4 3 3 3 2 2 3 2 3 1 70 --save-interval 10 gom Goan Indo-Aryan 69 9 710 8131313 9 911 910 4 240 Konkami ht HaitianCre-Creole 60 2 1 3 4 3 4 3 2 3 2 2 3 1 72 Figure 2: Model settings for NMT training with ole fairseq ilo Iloko Philippine 63 3 2 4 5 4 4 4 3 3 4 3 4 2 96 io Ido constructed 153 5 3 611 7 7 5 5 5 6 5 5 3 143 jv Javanese Malayo- 220 8 5 813121011 8 711 8 8 3 219 Polynesian ka Georgian Kartvelian 480 11 715121617161211141213 5 288 Finally, Table 6 gives the BLEU scores on the ku Kurdish Iranian 165 5 4 8 5 8 7 8 7 6 7 6 6 3 222 la Latin Romance 558 12 917322018171213181314 6 478 TEDcorpuswhentranslatingintoandfromEnglish lb LuxembourgiGshermanic 372 12 726221918151111161211 4 305 lmo Lombard Romance 147 6 3 710 7 711 6 5 7 5 5 3 144 forsomeadditionallanguages. mg Malagasy Malayo- 263 6 5 913 912 8 7 7 7 8 7 4 199 Polynesian mhr Eastern Uralic 61 3 2 4 3 4 4 5 3 3 4 3 4 2 96 Mari Lang xx→en en→xx min MinangkabauMalayo- 255 4 2 6 7 5 5 5 4 4 4 5 5 2 121 Polynesian et 15.9 14.3 mn Mongolian Mongolic 255 4 3 7 5 6 6 7 6 5 5 5 5 3 197 mwl Mirandese Romance 64 6 3 410 8 6 5 3 434 3 4 2 154 eu 10.1 7.6 ndsnlLow Ger-Germanic 65 5 4 610 7 7 615 5 6 5 5 3 151 man/Saxon fa 16.7 8.8 ps Pashto Iranian 89 2 3 2 3 3 3 3 3 3 3 3 1 73 rm Romansh Italic 57 2 210 5 4 4 3 2 3 3 3 3 1 86 ﬁ 10.9 10.9 sah Yakut Turkic/Sib 134 4 3 7 5 6 6 6 5 5 5 5 6 3 134 scn Sicilian Romance 81 5 3 6 9 7 711 5 5 6 5 5 2 143 lt 13.7 10.0 sd Sindhi Iranian 115 3 9 8 8 7 7 6 7 5 8 5 152 su Sundanese Malayo- 120 4 3 5 7 6 5 6 4 4 5 4 4 2 117 hi 17.8 21.9 Polynesian tk Turkmen Turkic 56 2 2 3 3 4 3 4 2 2 4 2 3 1 76 mr 2.6 3.5 tg Tajik Iranian 248 5 41115 9 9 8 8 7 8 610 6 192 ug Uighur Turkic 83 4 3 910 7 8 6 6 5 6 5 9 6 168 ur Urdu Indo-Aryan 150 2 2 3 5 3 3 3 3 3 3 3 3 2 123 wa Walloon Romance 56 3 2 4 5 5 4 4 3 3 4 3 3 2 93 Table6: BLEUscoresontheTEDtestsetasproposed wuu WuChineseChinese 75 8 61117121110 8 911 91043 283 yi Yiddish Germanic 131 3 2 4 3 4 4 5 3 3 4 3 4 1 92 in (Qi et al., 2018). </Extractive Summary>  </Table ID = 6>  </Paper ID = 115> 

<Paper ID = 116>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Comparisons with providing oracle mention embeddings. </Abstractive Summary>  <Extractive Summary> =  relation detection), we experi- thatthesurfaceandatomcoreferenceresultsarenot ment with providing oracle mentions during the substantiallydifferent,weusesurfacecoreference training process. Table 3 shows that the perfor- asourprimaryevaluationmetricintheremainder mance of both tasks improves substantially with ofthispaper. Forbridgingevaluation,weconsider gold mentions. </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  goldmentionsarecombined. Table 2 presents the results. For coreference Toinvestigatethecontributionofeachstep(men- evaluation,giventhattheresultsinTable2indicate tion detection vs. gainscouldbeattainedwithmoreannotateddata. Thestrongcorrelationbetweenanaphordetection For bridging, as shown in Table 2, the per- and relation detection is also self-evident in the formance suffers from low recall in anaphor de- graph. tection. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Comparison of different pretrained embed- testdatasetoverdifferent%oftrainingdataset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Furthermore, the confusion matrix of To perform error analysis, we analysed the ﬁne-grainedbridgingrelationsinFigure4shows model errors on the dev dataset. As detailed in that the model achieves poor performance for Table 1, the corpus contains discontinuous men- REACTION-ASSOCIATEDandWORK-UPrelation tions. However,ourproposedmodelonlyconsid- prediction,bothintermsofprecisionandrecall. </Extractive Summary>  </Table ID = 1>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  lem in bridging. As shown in Table 5 Ex 2, For coreference resolution, errors can be at- the reaction mixture in line 3 has a RELATION- tributedtothreeprimaryphenomena: ASSOCIATEDlinkwithThereactionmixtureinline 2 and sodium borohydride (10 mg, 0.27 mmol). 1. ability to model context. In Table 5 Ex 3, due to Acknowledgements the expression add into, the ﬁrst-mentioned the reaction mixture does not include the chemicals Funding for the ChEMU project is provided by mentioned prior, unlike the ﬁrst mention of the an Australian Research Council Linkage Project, phraseinEx2. project number LP160101469, and Elsevier. described. Table 5 Ex 4 illustrates chemical compoundsthatarelistedwithoutaprocess. References 2. References 2. Abstract expressions: In Table 5 Ex 6, pre- Saber A Akhondi, Hinnerk Rey, Markus Schwo¨rer, cipitateshouldhaveaWORK-UPrelationwith Michael Maier, John Toomey, Heike Nau, Gabriele Ilchmann, Mark Sheehan, Matthias Irmer, Claudia acetonitrile(150mL),andthetitlecompound Bobach,MariusDoornenbal,MichelleGregory,and ... withtheﬁltercake;thesearemisseddueto Jan A Kors. </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Results with different pretrained embeddings. </Abstractive Summary>  <Extractive Summary> =  Table6providesafullcomparisonoftrainingwithgold-standardoraclementionsperanaphorarelation onthetestdataset. Table 7 provides a full comparison of training with different pretrained embeddings per anaphora relationonthetestdataset. Relation Method P R F P R F A A A R R R coreference 84.9 50.0 62.9 73.7 41.9 53.4 Coref.(Surface) -w/oraclementions 86.0 78.1 81.7 84.8 74.5 79.2 joint train 89.4 45.8 60.5 81.7 40.6 54.2 -w/oraclementions 90.5 70.8 79.5 87.0 65.9 74.9 coreference 84.9 50.0 62.9 75.6 42.6 54.4 Coref.(Atom) -w/oraclementions 86.0 78.1 81.7 85.1 74.7 79.5 joint train 89.4 45.8 60.5 82.3 40.8 54.5 -w/oraclementions 90.5 70.8 79.5 88.7 66.3 75.9 bridging 88.4 80.9 84.5 76.0 65.4 70.3 Bridging -w/oraclementions 91.1 92.8 91.9 83.8 82.8 83.3 joint train 89.5 81.8 85.5 77.0 66.1 71.1 -w/oraclementions 91.3 92.4 91.8 82.8 84.3 83.5 bridging 77.5 63.8 69.7 76.2 63.8 69.1 TR -w/oraclementions 90.2 90.8 90.3 90.2 90.8 90.3 joint train 76.9 69.0 72.7 75.9 69.0 72.3 -w/oraclementions 91.5 86.2 88.6 90.6 86.2 88.1 bridging 82.7 83.3 83.0 66.0 57.5 61.4 RA -w/oraclementions 88.0 88.3 88.1 83.4 71.1 76.7 joint train 89.0 85.0 86.9 70.8 60.5 65.1 -w/oraclementions 85.4 93.9 89.4 78.0 76.6 77.3 bridging 92.0 82.5 87.0 81.1 68.5 74.3 WU -w/oraclementions 92.4 94.4 93.4 83.7 86.7 85.2 joint train 91.6 82.7 86.9 79.4 67.9 73.1 -w/oraclementions 93.7 92.6 93.1 85.2 86.9 86.0 bridging 100.0 88.9 94.1 72.1 79.4 75.4 CT -w/oraclementions 93.3 100.0 96.5 79.5 100.0 88.3 joint train 95.8 85.2 90.2 89.4 78.4 83.4 -w/oraclementions 90.6 100.0 94.9 71.9 100.0 83.3 joint train 89.5 70.6 78.9 77.5 61.6 68.6 Overall -w/oraclementions 91.1 85.7 88.3 83.4 81.0 82.1 Table6: Testresultswithgold-standardmentionsduringtraining. </Extractive Summary>  </Table ID = 7>  </Paper ID = 116> 

<Paper ID = 117>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Improvement (or degradation) in labeled F-score, weighted by relative frequency, for the 10 best UD relationsinthe5languageswithgreatestLASimprovementsoverthebaseline(softcomposition). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 117> 

<Paper ID = 118>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Prediction accuracy (averaged across languages) by decoding strategy for Transformer and HMM. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Average log probability of inﬂections gener- employefﬁcientsearchstrategiesduetosomeprop- ated with various decoding strategies and the empty ertiesofp . </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Average time (s) for inﬂection generation by decodingstrategy. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  </Paper ID = 118> 

<Paper ID = 119>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Result on curriculum learning. </Abstractive Summary>  <Extractive Summary> =  We use GloVe1 as the initial parameters of word embeddings. The di- 5 ResultandDiscussion mensionsofthewordembeddingsandthehidden The result on Table 1 shows curriculum learning layersare300and200. Thetrainingepochis20. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Tovalidatethestatistical models, but it is unclear what the appropriate- signiﬁcanceoftheresult,weaggregatethenumber ness represents. Table 2 shows Pearson’s corre- ofvoteson“better”and“slightlybetter”andcon- lationcoefﬁcientsbetweentheappropriatenessand ductchi-squaretest. Thestatisticalsigniﬁcanceis source/targetlength. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In Proceedingsofthe58thAnnualMeetingoftheAsso- ciation for Computational Linguistics, pages 6934– 6944. 1400A ParametersofSummarizationModels C ExamplesofdatawithHighandLow Appropriateness The dimensions of hidden layers of both Table 5 and 6 shows the examples of source and Seq2seqAtt and Transformer are 256. The di- target pairs with low appropriateness. </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  Table 4: The segments at which each model gets best validationmetric(ROUGE-1-F).Meanandstandardde- viationvaluesof5experimentsareshown.Thenumber ofthesegmentsis10. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 119> 

<Paper ID = 120>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  This conﬁrms the quality of our retrieval With the lexical collocations of the extended strategy,andhence,ourresource. LEXFUNCdatasetathand,weﬁrstcompilefromthe In terms of corpus statistics, Table 3 indicates EnglishGigaword3 acollocationscorpus,which the number of sentences for each LF distributed contains the occurrences of these lexical colloca- across training (70% of the sentences), develop- tions. Inprinciple,theidentiﬁcationofagivencol- ment(15%)andtest(15%)sets. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 120> 

<Paper ID = 121>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  arXiv preprint rating between 2.0 and 2.5. Table 2 showcases a arXiv:1409.0473. fewsamplesoforiginallabels. </Extractive Summary>  </Table ID = 2>  </Paper ID = 121> 

<Paper ID = 122>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Performance degradation caused by the ap- proachwithouttheproposedpre-trainingtask. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Examples of attention weights. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 122> 

<Paper ID = 123>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Repetition on small and medium GPT-2 models. </Abstractive Summary>  <Extractive Summary> =  x(j) =argmaxp (·|x(j)) t θ <t x∈V 5 Results end end forj=1tomdo We showed that Implicit Unlikelihood Training fort=k+1tok+T do is a competitive approach that outperforms other if(x ,...,x ,...,x )∈ t−i t t+h methods in sequence repetition when ﬁne-tuning x(j) forany(h−i)=n,i≤n≤h <t−i smallandmediumGPT-2models(seeTable1)on then Ct (x(j))={x } most variants of top-k and top-p sampling, while repeat-n t else maintaining the lowest perplexity and the highest Ct (x(j))=∅ repeat-n count of unique tokens generated. This approach end alsoachievedbetterresultsthantrainingwithent- end end max loss and other related approaches, using a L(θ,Dm)= different range of sampling methods (see Table 1 (cid:80)m 1 k(cid:80)+T Lt (cid:16)p (·|x(j)),Ct (cid:17) 2), with the only exception being the rep metric, m T UL θ <t repeat-n j=1 t=k+1 whereentmaxperformedsimilartoi-UT. Samples of generated outputs are provided in Tables8,9inAppendix. In arXiv preprint arXiv:1506.02438. be effective, taking twice less time (see Table 1). Theparameters(cid:15)for(cid:15)−pplandαforα−entmax JohnSchulman, FilipWolski, PrafullaDhariwal, Alec trainingweretakenfrom(Martinsetal.,2020)(ex- Radford, and Oleg Klimov. </Extractive Summary>  </Table ID = 1>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Inthisexperiment,weobservedthatImplicit awholesequenceandsubtractingabaselinevalue Unlikelihood Training performed better or equal to reduce the variance of the gradient estimation to Unlikelihood Training with different sampling performedbest. methods measured by the DIMEN metric, having a signiﬁcantly better value of uniq-seq (see Table 7). We also evaluated sequence repetition with beam-search sampling for MLE, UT, and i-UT methods for both small and medium GPT-2 mod- els, using validation data to form sampling pre- ﬁxes. </Extractive Summary>  </Table ID = 7>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Repetition with Beam Search. </Abstractive Summary>  <Extractive Summary> =  We also evaluated sequence repetition with beam-search sampling for MLE, UT, and i-UT methods for both small and medium GPT-2 mod- els, using validation data to form sampling pre- ﬁxes. Whensamplingwithbeamsearch,wefound that Implicit Unlikelihood Training produced bet- ter results than Unlikelihood Training (see Table 3). For greedy sampling with small GPT-2 model, weevaluatedsequencerepetition,wrep,uniq,and perplexity. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Diversity on small GPT-2 model for TLDR and MLE approaches. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  </Paper ID = 123> 

<Paper ID = 124>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Examples of offensive sentences from the Civil els that can help suggest rephrasings of toxic Commentstestsetandthemorecivilrephrasinggenerated comments in a more civil manner. </Abstractive Summary>  <Extractive Summary> =  tionedonboththeinputsentenceandthedestina- Interestingly, our model is able to add toxicity tionattribute. Wepresentmoreresultsshowingthat incivilcommentsasshownbytheexamplesinthe wecaneffectivelysuggestﬂuentcivilrephrasesof Appendix Table 10. Even if such an application toxic comments in the Appendix Table 8. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Statistics for the Yelp dataset and the processed versionoftheCivilCommentsdataset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Automatic evaluation scores of different mod- languageformalrules, thatisonlypartiallyrepli- elstrainedandevaluatedontheprocessedCivilComments dataset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Wepresentmoreresultsshowingthat incivilcommentsasshownbytheexamplesinthe wecaneffectivelysuggestﬂuentcivilrephrasesof Appendix Table 10. Even if such an application toxic comments in the Appendix Table 8. How- shows limited interest for online platforms, it is everweobservemorefailuresthaninthesentiment worthwarningaboutitspotentialmisuse. </Extractive Summary>  </Table ID = 8>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  </Paper ID = 124> 

<Paper ID = 125>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Vectorct rep- SNOW. To extract weather labels from weather resentsthecontextvectoratt,whichiscreatedby comments,wedeﬁnecluewordsforeachweather concatenating four context vectors [cg;ca;cm;cl] t t t t label, as shown in Table 1. Our strategy is to constructed with the attention mechanism (Bah- explicitly match the clue words and words in danau et al., 2015) for input data (g,a,m) and weathercomments. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: F scores for time-dependent expressions. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Results of weather-label prediction by classi- Next,wecomparedthemodelstoinvestigatehow ﬁer, whichonlyperformsweather-labelprediction, on eachcomponentcontributestotheirperformance. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  takesintoaccountabsolutepositionsonthemaps tobemoresuitablethanaCNNencoderbecause 5.2 Results ofitssimplicity. The results of the automatic evaluation are listed in Table 2. To see how our proposed model can Effect of meta-data With regard to the meta- correctlydescribethecontentofweatherforecasts, datasuchasdeliverytimeanddate,forwhichwe we calculated precision, recall, and F scores of expectedthemodeltogeneratetime-dependentex- 1 weatherlabels,whichareextractedfromtheirgen- pressions(e.g.,“today”),weconductedanablation eratedtexts,byreferringtoweatherlabelsextracted 10Inspiteofthis,weusedanMLPforthemodelstoadd fromhuman-generatedtextsasreferences. </Extractive Summary>  </Table ID = 2>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Wefoundthatmodel(4),whichtakes 5.4 HumanEvaluation into account the meta-data, can more accurately providetime-dependentexpressionsthanw/oMeta. Table 6 lists the results of the human evaluation, Thisﬁndingsuggeststhatintroducingthemeta-data where # represents the number of cases, which intoagenerationmodelimprovethecorrectnessof includes each weather label in the evaluation set. meta-dataingeneratedcomments. </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Weather labels and their corresponding clue words we used to extract labels from weather comments. </Abstractive Summary>  <Extractive Summary> =  Inthisexample,wecan 4507–4515.IEEEComputerSociety. observethatmodel(2)wasjudgedaslessinforma- tiveonaveragebytheﬁveratersincomparisonwith A ExamplesofAllClueWords model(4)sincethecommentsgeneratedbymodel Toextractweatherlabelsfromweathercomments, (2)provideincorrectinformation(e.g.,rainshower we deﬁned clue words for each weather label, as insteadofsnowinTable8),orlackimportantinfor- shown in Table 7. Note that we selected the clue mation(e.g., raininTable9). </Extractive Summary>  </Table ID = 7>  </Paper ID = 125> 

<Paper ID = 126>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  thatwecannotfullyguaranteethesamechoiceof First, we evaluate embeddings that were not wordsineachsentencepairintheoriginaldataset, speciﬁcally trained on SICK. Table 2 shows the aswetranslatesentencebysentence. Inthewhole correlationresultsontherelatednesstaskofSICK process, we adapted 1833 of the 6076 automatic andSICK-NL,wherethecosinesimilaritybetween translations,eitherbecauseoftranslationerrorsor twoindependentlycomputedsentenceembeddings alignmentconstraints. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  taintherelatednessandentailmentscoresfromthe originaldataset. SICK SICK-NL Table 1 shows some statistics of SICK and its Skipgram 69.49 Skipgram 56.94 Dutchtranslation. Themostnotabledifferenceis BERT 50.78 BERTje 49.06 cls cls thattheamountofuniquewordsinDutchis23% BERT 61.36 BERTje 55.55 avg avg higherthanthatinEnglish, eventhoughthetotal RoBERTa 46.62 RobBERT 43.93 cls cls numberofwordsinSICK-NLisabout93%ofthat RoBERTa 62.71 RobBERT 52.33 avg avg inSICK.Wearguethatthisisduetomorpholog- ical complexities of Dutch, where verbs can be Table2: Pearsonr correlationcoefﬁcientfortherelat- separableorcompound,leadingthemtobesplitup edness task of the English SICK dataset (left) and its intomultipleparts(forexample,“storing”becomes Dutchtranslation(right). </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (2020), with their corresponding En- Following Naik et al. (2018), we inspect these glish counterparts, as well as multilingual BERT prominent cases of misclassiﬁcation in Dutch by (mBERT), as sequence classiﬁers on the Entail- lookingatthenumberofcasesofhighoverlap(at menttaskofSICK(-NL).Hereweobserveasimi- mostfourwordsnotincommon),andatthenum- larpatternintheresultsinTable3: whilethereare beroflengthmismatches(thedifferencebetween individual difference on the same task, the main sentence length exceeds 4), and set off these dis- surprise is that the Dutch dataset is harder, even tributions against that of the test set, in Table 4. whenexactlythesamemodel(mBERT)isused. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Confusion matrices for English vs Dutch language models, ﬁnetuned. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Wethenverifyhowthemodel’spredic- tions do on those inference pairs that were in the testset(116). Additionally,wecheckwhetherthe The results in Table 6 indicate that the inter- modelsareabletointerchangesentencesandtheir change between present continuous and simple rewrittenequivalent(i.e. classifyasEntailment). </Extractive Summary>  </Table ID = 6>  </Paper ID = 126> 

<Paper ID = 127>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Accuracy (in %) of logistic regression classi- ourdataincludedasubstantialnumberofcontrasts ﬁerspredictinglanguageidentityofagivenwordfrom of this type, the model’s higher error rates could itsacousticembedding,averagedoverﬁverandomini- beattributedtothesedifﬁcultphonecontrasts. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 127> 

<Paper ID = 128>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Average test F1 score over all relations. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Syntactic Search by Example with different trainingsizes C TriggerListExpansion ForthemajorityofpatternsusedintheSyntactic SearchbyExampleexperimentsweusedasingle triggerword(seeAppendixD).Toexperimentwith usingtriggerlists,wemodiﬁedthepatternsinAp- pendixDinthefollowingway: Wechangedthetriggersinallchild\childrenpat- ternstoincludeanyofthefollowingpossibilities: baby, child, children, daughter, daughters, son, sons,step-daughter,step-son,step-child,step- children,stepchildren,stepdaughter,stepson For founded-by relations we change the “founder"triggertobeanyofthesetriggers: founder,co-founder,cofounder,creator andchanged“founded"tobeanytriggerfromthe followinglist: 1501D ExamplesusedforSyntacticSearchbyExample child <>e1:[e=PER]John’st:[w={triggers}]daughter,<>e2:[e=PER]Tim,likesswimming. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 128> 

<Paper ID = 129>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  held a significant ricstotheRESMATCHscoresofsystemsdisplayed amount of opium . in Table 1(see Appendix A.3), and observe, e.g., ------------------------BLEU score--------------------------- thatR’19,whichrankslastintheoverallranking, 37.7 >> 22.6 improvesuponthebestoverallsystemby3.4points ---------------------Reconstructions------------------------ in NER recall and 1.9 points in F1. The analysis (c0 / add-02 (c0 / add-01 also corroborates that W’20 excels among com- :ARG0 (c2 / person :ARG0 (c2 / person :name (c4 / name :name (c5 / name petitorswithbestscoresforcoreference,SRLand :op1 "Costa") :op1 "Costa")) :ARG1 (c1 / insurgent) :ARG1 (c1 / hold-01 negation,i.e.,themoreglobalaspectsofsentence :ARG2 (c3 / hold-01 :ARG0 (c4 / insurgent) meaning. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Weselect Table2: ourstandardsetupisdisplayedincolumns GPLAandTTSAsincetheyconstitutetechnically labeled GSII and the results of the setup where quitedistinctapproachescomparedtoGSII. wereplacethegoldinputgraphwithanautomatic ♦ The results are shown in Table 2 (columns la- parse is indicated by GSII . When considering belledGPLA,TTSAandGSII).Allvariantstend RESMATCHscores,weseeonlyoneswitchedrank to agree in the majority of their rankings11 (e.g., between Mb’20 and G’20 (3–4). </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Fine-grained results using MF0 parameter- izedwithmetricsproposedbyDamonteetal.(2017). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 129> 

<Paper ID = 130>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  1526Results. In Table 3, we observe that on the Ne- Ondˇrej Bojar, Christian Federmann, Mark Fishel, En task augmenting the parallel dataset with ei- Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, and Christof Monz. 2019. </Extractive Summary>  </Table ID = 3>  </Paper ID = 130> 

<Paper ID = 131>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  whenadaptingtonewtopics. wesuggestthat: 1)in- 4.5 ResultsandAnalysis steadoftraininganunstableadversarialcomponent WepresentourexperimentsonthePHEMEdataset orremovingbiasdirectlyfromsemanticcontents, in Table 2 and Table 3. Several observations can the mixture of experts provides us with another bemadefromtheexperimentresults: way to increase generalization ability. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  whenadaptingtonewtopics. wesuggestthat: 1)in- 4.5 ResultsandAnalysis steadoftraininganunstableadversarialcomponent WepresentourexperimentsonthePHEMEdataset orremovingbiasdirectlyfromsemanticcontents, in Table 2 and Table 3. Several observations can the mixture of experts provides us with another bemadefromtheexperimentresults: way to increase generalization ability. </Extractive Summary>  </Table ID = 3>  </Paper ID = 131> 

<Paper ID = 132>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  EvaluatetheMToutputofthecurrentST andJapanese-to-Korean(Ja-Ko)translations. based on a 5-point scale criterion shown in Table 1. If the quality of the MT output is 3Weoperationallydeﬁned“tominimallyedit”as“tomod- ifyanSTwithasmalleditthatisdifﬁculttobefurtherdecom- satisfactory(i.e.,“Perfect”or“Good”),goto posedintomorethanoneindependentedit,withoutinducing Step4;otherwise,gotoStep2. </Extractive Summary>  </Table ID = 1>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  These strategies can be used as concise pre- correlation is between EditCount(ST,ST(cid:48)) and editingprinciplesforhumaneditorsandcanguide EditCount(MT(ST),MT(ST(cid:48))). researchers in devising effective tools for pre- As shown in Table 6, most coefﬁcients are in editing. Wealsoemphasisethatthesegeneralinfor- the range of 0.15–0.25, suggesting a very weak mationalstrategiesarenotspeciﬁctotheJapanese correlation. </Extractive Summary>  </Table ID = 6>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  6.1 CorrelationoftheAmountofEdits Figure3presentsthedistributionofthedegree betweentheSTandMT of changes in the MT output when an ST is pre- To grasp the general tendency, using all the col- edited,measuredbyTER(MT(ST),MT(ST(cid:48))). Most lected pre-editing instances (see Table 3), we of the structural edits (S01–S04) resulted in size- ﬁrst calculated the correlation coefﬁcients (Pear- able changes in the MT. This is reasonable since son’s r and Spearman’s ρ) between the amount structuralmodiﬁcationsintheSTtendedtocause of edits (the TER and the number of edits) majorchangesintheMTaswell,leadingtohigh in the ST and in the MT. </Extractive Summary>  </Table ID = 3>  </Paper ID = 132> 

<Paper ID = 133>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Score functions proposed in selected prior sentences,andhaveappliedpopularwordembed- workonKGEs. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Therefore,thesmallertheν values,more R orthogonal the relation embeddings will be. We measureν valuesforthe11relationtypesinthe R WN18RR dataset as shown in Table 3. From Ta- ble 3 we see that ν values are indeed small for R differentrelationtypesindicatingthattheorthog- onality requirement is satisﬁed as expected. Prior work studying lower- randomlysampled10000headortailentities. We rankdecompositionofKGEshaveshownthat,al- computethestandarddeviationsσ andσ respec- though linear embeddings of graphs can require c c(cid:48) tivelyforthedistributionsofZ andZ andtheir prohibitively large dimensionality to model cer- c c(cid:48) geometric means as shown in Table 3. We ob- tain types of relations (Nickel et al., 2014) (e.g. </Extractive Summary>  </Table ID = 3>  </Paper ID = 133> 

<Paper ID = 134>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ingtasks,i.e. thetrainlabelspaceisdisjointwith Furthermore, Table 2 presents the results from test label space and the trained model evaluated AttentiveRelationalNetswithdifferentembeddings onunseenclasses. Therefore,weutilizeotherdo- methodsanddemonstratesthatourproposedmodel main data as the training set whereas the models with ELMo and BERT outperforms the previous are evaluated by using the current domain. modelwhiletheglobalfeaturesaresuppressed. 5 Conclusion 4.2 DifferentContextualEmbeedings Table 2 shows that ELMo and BERT have com- Wepresentedadeepanalysiswithawidevariety parable performance, with BERT slightly better offew-shotlearningmethodsandpretrained(con- onmosttasks: ELMo,however,scoreshigheron textual)embeddingsforslottagging. Furthermore, FindScreeningE.consistentlywithAttentiveRela- weproposedanovelarchitecturethatleveragesat- tionalNetsandalldifferentKs. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thus, modelsconsistently. Additionally,AttentiveRela- we created 7 different sets contain a train which tionalNets signiﬁcantly improve the results with consistsof6differentdomainsaswellasatestset BERT from the previous experiments in Table 1. whichincludesonlyonetestdomain. </Extractive Summary>  </Table ID = 1>  </Paper ID = 134> 

<Paper ID = 135>  <Table ID = 4>  <Abstractive Summary> =  Table 4: The results of multi-label emotion classiﬁca- tiononSemEval-2018testset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Finally,weperformedthisoperationon 5.1 PredictionofMultipleEmotions all inputs in the SemEval2018 English validation Weadditionallyvalidatedtheeffectivenessofour setandthensortedallwordsforeachemotionclass methodforlearningthemultipleco-existingemo- inascendingorder. tionsonEnglish,ArabicandSpanishsets.Table6 Table 7 presents the top-10 words per emo- presentstheresults,including BERT . SpanEmo tion class. SpanEmo tion class. As shown in Table 7, the words dis- BASE demonstratedastrongabilitytohandlemulti-label covered by our framework are indicative of the emotionclassiﬁcationmuchbetterthanBERT . corresponding emotion. </Extractive Summary>  </Table ID = 7>  </Paper ID = 135> 

<Paper ID = 136>  </Paper ID = 136> 

<Paper ID = 137>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Machine reading comprehension datasets published until 2017. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 137> 

<Paper ID = 138>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Language families in the TED8-Related cor- encoder, E, and N multiple decoders, D = pus. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: The abbreviations of language assignment onthecheckpointwiththebestvalidationlossand methodsandmetrics. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  3 and 8 vs. 9 in Table 3). To ingprocedureisthesameastheML50. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Translation speed and accuracy trade-off on TED8-Related and TED8-Diverse corpora. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Language families in the TED8-Related cor- AUSTROASIATIC km,id,xh,he pus. </Abstractive Summary>  <Extractive Summary> =  Table 7: Language embedding-based language assign- B Languagefamilyassignmentresults mentresultontheML50corpus. In Table 5, we show the language family-based assignmentresultonTED8-Diverse. Sincethiscor- pus is collected without considering relatedness, somegroupsjusthaveonelanguage. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Language families in the TED8-Related cor- pus. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Language embedding-based grouping results onML50. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  </Paper ID = 138> 

<Paper ID = 139>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We report results of this comparison for each of The evaluated metrics’ averages for all compres- the black-box algorithms listed in Section 4.2.1). sion buckets are presented in Table 3, evaluating NotethattheF-scoreisbasedontheactualresults theGGLdatasetusingthreedifferentcompression of each of the black-box models, and that both models. Bestresultsareinbold. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Readability and Informativeness average F1- andInformativenesssupportsourchoiceofR(see scoresandvariance. </Abstractive Summary>  <Extractive Summary> =  4.3.2 Documentlevelcompression TheresultsconﬁrmthatbyutilizingB-BOC,the Having a document or a paragraph comprised of topsentenceswhichyieldthebestoverallcompres- severalsentencesthatareneededtobecompressed, sionresultswillbechosen,nomatterwhichblack- thetargetistoﬁndthesentencesthatwouldgain boxcompressionmodelisapplied,foreverygiven thehighestperformancescoresubjecttotheoverall compression ratio. Table 4 describes the average compressionratioconstraint. F1-scoresandvariancesforthemanualevaluations 1629(a)Filippova’scompressionmodel (b)Zhou’scompressionmodel (c)Klerke’scompressionmodel Figure 1: Average F1-score (y-axis) applied on the GGL dataset for different compression rate buckets (x-axis) whiletrainingB-BOCwithFilippova(a),Zhou’s(b)andKlerke(c)compressionmodels. </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Feature Importance of B-BOC. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  </Paper ID = 139> 

<Paper ID = 140>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Average human accuracy for native English thecontextandthedeﬁnitionandfeedthewhole annotators,ondifferentsubsetsofthedataset: general sequence to BERT. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Performance for the baseline models for the three tasks (i.e., T1: deﬁnition-based, T2: hypernymy- based, and T3: both sources of information) split by domain: General (WNT/WKT), Cocktails (CTL), Medical Subjects(MSH),andComputerScience(CPS).Baseline isanaivebaselinethatalwaysreturns“True”. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: F1 score for the in-domain few-shot analy- BERT-large, which performed better overall. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  </Paper ID = 140> 

<Paper ID = 141>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  fact that the “summaries” in RottenTomatoes are 7 EvaluationResults critical reviews, written in a very different style thantheoriginalreviews. AutomaticEvaluation Table1containstheau- Table 2 contains reference-less evaluation, an- tomatic evaluation metrics with respect to refer- alyzing the number of distinct n-grams (an indi- ence summaries. The proposed multi-input self- catorofrepetitiveness)onthesummaryleveland supervisedmodelwithcontrolcodesperformcon- corpora level. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Table 4: Results of the human evaluation focused on faithfulnessofgeneratedreviews. 8 Conclusion The results in Table 4 show that 87.0% of the Thepromiseofunsupervisedmulti-documentab- generatedsummariesofoursystemareconsidered stractivesummarizationisbeenhamperedbythe factuallycorrect(comparewith95.7%forthegold complexityofthosemodelsandtheproblemofhal- 1653lucinations. Ourproposedmodelhastheadvantage copycat-review generation. </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Average time to Answer and the theoritical hourly wage of turkers (in USD) for the crowdsourcing experimentsofhumanevaluation. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  </Paper ID = 141> 

<Paper ID = 142>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Statistics of NewsMTSC. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ofsentimentandemotion(10,multiple). 5 Experiments 5.5 Overallperformance Table 2 reports the performances of the models 5.1 Experimentaldata using different LMs and evaluated on both test InadditiontoNewsMTSC,weusethethreeestab- sets. Weﬁndthatthebestperformanceisachieved lishedTSCsets: Twitter,Laptop,andRestaurant. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  thusistarget-independent. Table 4 details the results of exemplary EKS, 5.6 Ablationstudy showingthatthebestcombination(SENT,MPQA, We perform an ablation study to test the impact and NRC) yields an improvement of 2.6pp com- of four key factors: target mask, EKS, coreferen- paredtonotusinganEKS(zeros). Thesinglebest tialmentions,andﬁne-tuningtheLM’sparameters. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (performanceinﬂuenceissimilaroneithertestset, Theuseofcoreferenceshasamixedinﬂuenceon performance(Table5). Whileusingcoreferences 10For previous models, Table 3 lists results reported by hasnoorevennegativeeffectinourmodelforlarge theirauthors. Inourexperiments, weﬁnd0.4-1.8pplower performancecomparedtothereportedresults. </Extractive Summary>  </Table ID = 3>  </Paper ID = 142> 

<Paper ID = 143>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  C.1 Runtime OurimplementationiswritteninPyTorchandruns on both GPU and CPU. Table 5 shows the run- time for one epoch of both our Combined TM andNeural-ProdLDAfor25and50topicsonthe GeForceGTX1050. Neural-ProdLDAisslightly fasterthanourZeroShotTM.Thisisduetothead- ditional representation that cannot be encoded as a sparse matrix. </Extractive Summary>  </Table ID = 5>  </Paper ID = 143> 

<Paper ID = 144>  <Table ID = 1>  <Abstractive Summary> =  Table 1: UAS (DSpr) of d2 and d embeddings, plus 2 1 resultsonlabeleddependencytreeprediction. </Abstractive Summary>  <Extractive Summary> =  Givenour impacts performance. Table 1 reports UAS (and modelpredictionandareferenceparseforagiven DSpr)onthedevelopmentportionoftheEnglish- input, accuracy is calculated using two standard EWTcorpus,fortreeembeddingsinboth(Rm,d ) 1 measures: Unlabelled Attachment Score (UAS), and(Rm,d2)metricspaces(referredtoasd and 2 1 that is the percentage of tokens that are assigned d2 forbrevity,respectively). 2 thecorrecthead;andLabelledAttachmentScore The ﬁrst row shows results when trained with (LAS)thatisthepercentageoftokensthatareas- L only. withthed metric,unlessotherwisestated. theembeddingobtainedusingL underestimates 1 CE Table 1 also shows a comparison between the theground-truthtreedistances,andaddingtheaux- differenthead–dependentlosses,withandwithout iliary MAE loss helps regulate this distortion of theauxiliarylossL thatfurtherconstrainsφ treedistances. MAE tobegloballyisometric. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  However, the p (w |w ) term in this of Qi et al. (2018) for all treebanks in Table 2. T j i loss gives higher weights to word pairs that are For a fair comparison, we re-run all experiments closer in terms of ground-truth tree distance and and report our results for the biafﬁne parser. </Extractive Summary>  </Table ID = 2>  </Paper ID = 144> 

<Paper ID = 145>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Overall, 24 AL iterations are made, so the rest 25% as the development set similarly to in the ﬁnal iteration, half of the training dataset the experiments with AL. From Table 1, we can (in tokens) is labeled. We do not use validation ﬁnd that for both OntoNotes and CoNLL-2003, sets provided in the corpora but keep 25% of the themodelperformancepatternisalmostthesame. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 145> 

<Paper ID = 146>  </Paper ID = 146> 

<Paper ID = 147>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Weextendthisdatabyadding pairs,formingachainwhereoneclaimisthesuc- allpairsbetweennon-consecutiveversionsthatare cessorofanotherandisconsideredtobeofhigher inferrable transitively. Considering the previous quality (examples found in Table 1). In addition, example,thismeansweadd(v ,v ),(v ,v ),and claimpairsmayhavearevisiontypelabelassigned 1 3 1 4 (v2,v4). </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Pearson’s r correlation in our annotation studybetweenincreasesinthe15qualitydimensionsof tune the model for two epochs using the Adam Wachsmuthetal.(2017a)andthemainrevisiontypes: optimizerwithlearningrate1e-5. </Abstractive Summary>  <Extractive Summary> =  typeforallthesepairs. Consistency of Relative Quality In this study, Table 3 shows Pearson’s r rank correlation for weaimedtocapturethegeneralperceptionofclaim eachqualitydimensionforthethreemainrevision qualityonameta-level,byderivingadata-driven types. Weobserveastrongcorrelationbetweenthe qualityassessmentbasedontherevisionhistories. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Example of a pairwise score matrix for rank- Credibility 0.06 -0.16 0.10 ingofthreeclaimrevisions,v –v ,giventhefollowing EmotionalAppeal 0.00 0.00 0.00 1 3 pairwisescores: (v ,v ) = (0.018,0.982),(v ,v ) = Clarity -0.16 0.35 -0.18 1 2 2 3 Appropriateness 0.01 0.02 -0.04 (0.428,0.572),and(v ,v )=(0.002,0.998). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Claim quality ranking results: Pearson’s r and Spearman’s ρ correlation as well as top-1 accuracy for alltestedapproachesintherandom-splitandthecross-categorysettingonClaimRev .Inallcases,SVMRank+ EXT SBERTissigniﬁcantlybetterthanallothersatp<0.001accordingtoatwo-sidedStudent’st-test. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  that BERT and SBERT consistently outperform SBOWinallsettingsonbothcorpusversions,with 5.3 ClaimQualityRanking SBERT’saccuracyofupto77.7beingbest.7 Table6liststheresultsofourrankingexperiments, A comparison of the performance of the meth- whichshowpatternssimilartotheresultsachieved ods depending on the corpus used for training in intheclassiﬁcationtask. Table 5 shows the effect of augmenting the origi- We can observe similar patterns in both of the nalKialodata. Inmostcases,theresultsobtained selectedrankingapproaches: SBERTconsistently by models trained on ClaimRev are compara- EXT outperformsallotherconsideredapproachesacross ble(slightlyhigher/lower)thanresultsobtainedby allsettings(upto0.73and0.72inPearson’sr and modelstrainedonClaimRev . </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Suchsystemscanbeusedtoin- manceoneachrevisiontypeanddistance. dicateifthesuggestedrevisionsincreasethequality As the upper part of Table 7 shows, SBERT is ofanargumentorrecommendthetypeofrevision highlycapableofassessingrevisionsrelatedtothe needed. Weleaveitforfutureworktoinvestigate correction and addition of links and supporting whetherthelearnedconceptsofqualityaretrans- information. </Extractive Summary>  </Table ID = 7>  </Paper ID = 147> 

<Paper ID = 148>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Zero-shot performance on MultiWoz 2.0 low-data setting (< 32 dialogues). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 148> 

<Paper ID = 149>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Examples of ADEs extracted by PubMedBERT (overlined in blue) and SpanBERT (underlined in red). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Best hyperparameters for all Transformer- Teven Le Scao, Sylvain Gugger, Mariama Drame, basedarchitecturesonSMM4HandCADEC. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 149> 

<Paper ID = 150>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  SlotAccuracyistheaccuracyofpredictedslots. FPs Repairs tophiladenvertophiladelphia” Statisticsaboutthesyntheticdisﬂuenciesarein #ofexamples 18 34 Table 2. We incrementally add between 1% to ATIS 0.06 0.06 100% of the disﬂuent examples into the original +real 0.72 0.12 trainanddevsetsandfullyretrainthemodel. </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Example: “what’stheearliestﬂightmm disﬂuencies may add more noise at training time showmeﬂightsfromdenvertophiladelphia” thansyntheticdisﬂuencies. 3 Repeat repair: Split u after a random char- In Table 5, we see that the performance boost acter,ensuringtheﬁnaltokenoftheﬁrstpart fromsyntheticexamplescomesmostlyfrombetter p1 is at least 2 characters long. We repeat handling of repairs. </Extractive Summary>  </Table ID = 5>  </Paper ID = 150> 

<Paper ID = 151>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ThehistogramofenergyvaluesEˆ pxqforsam- θ plesinthetestsetofQNLIandSST-2areshown B DetailsAbouttheNoiseDistribution inFigure5. Weshowsomeexamplesofgeneratednoisesam- InFigure6,weprovideanenlargedversionof ples and the masking in Table 4. Note that the Figure2. </Extractive Summary>  </Table ID = 4>  </Paper ID = 151> 

<Paper ID = 152>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Number of workers and Krippendorff’s α estimate of ideology, we use these labels to cre- agreement within the annotator groups over the full ate a basis of comparison. </Abstractive Summary>  <Extractive Summary> =  Thetwogroups than75%ofthescreeningquestionscorrectly,giv- withthehighestpercentageofquestionswithmis- ing us a ﬁnal pool of 158 annotators. Table 1 il- matchedanswersareleft-leaningandright-leaning lustratesthedistributionoftheremainingworkers annotators, and 3 of the top 4 comparison pairs across labels within the three categories. Labels with the most mismatched answers are between across categories do not appear to be correlated ideologygroups(SupplementaryMaterialTable6). </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Selected entities included in the construction of the dataset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Krippendorff’s α agreement results for sur- 1. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Comparison pairs with highest percentage of questionswherethemajoritygavedifferentanswers. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Entities with highest percentage of ques- tions where the left-leaning and right-leaning annota- torsgavedifferentanswers. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  </Paper ID = 152> 

<Paper ID = 153>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 153> 

<Paper ID = 154>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Statistics on arXiv, PubMed and marization datasets, namely ArXiv, PubMed and CNN/DailyMail validation datasets in terms of CNN/DailyMail: documentsandsummarylengths. </Abstractive Summary>  <Extractive Summary> =  samples. 2.2 OutputLayer Table 1 presents some statistics on these three Inthiswork,wevalidateourapproachonthetask datasets. Asonecannote,forthescientiﬁcarticles, ofextractivesummarizationdescribedinSection the average number of tokens in the documents 3. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Comparison of ROUGE scores on of L = 12 propagation layers with a transformer CNN/DailyMail wrt extractive models. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  performsthebaselinemodelsonalmostallofthe Lastly, we evaluate the impact of several ele- reportedmetrics. Ourapproachmanagestosumma- ments of our proposed model in Table 4. We rizelongdocumentswhilepreservinginformative- ﬁrst study the inﬂuence of the underlying lan- ness(evaluatedbyROUGE-1)andﬂuency(evalu- guagemodelbyconsideringbothRoBERTa(Liu atedbyROUGE-L)ofthesummaries. </Extractive Summary>  </Table ID = 4>  </Paper ID = 154> 

<Paper ID = 155>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Therangeofvaluestestedare k predictlabelFM forthegeneratedtext. given in Table 1, with default values emphasized k inboldface. Notethatonechoosestouseeitherp- 3 Data&Methods valueork-valuesamplingsincetheyhavethesame goal-controllingthenumberofwordstakeninto In this section, we present details about (1) the considerationwhilesamplingtextfromanLM. Wenextexploretheparameterdifferencesangle 4.4 Attributingﬁne-tunedvariantsofa furthertogetasenseofwhatwouldhappenifthe pre-trainedLM adversarychose aparametervalueotherthan the Finally, we explore the scenario where the ad- ones explored in Table 5. The different values versary is using different ﬁne-tuned LMs with for k, p, and temperature are as listed in Table 1. the same parent pre-trained LM. </Extractive Summary>  </Table ID = 1>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  aconcernasdiscussedlaterinthissection. Wenextexploretheparameterdifferencesangle 4.4 Attributingﬁne-tunedvariantsofa furthertogetasenseofwhatwouldhappenifthe pre-trainedLM adversarychose aparametervalueotherthan the Finally, we explore the scenario where the ad- ones explored in Table 5. The different values versary is using different ﬁne-tuned LMs with for k, p, and temperature are as listed in Table 1. TheresultsinTable6showthatitischallenging Curiously,CNNwhichwastheleastsuccessfulin totellapartsynthetictextsgeneratedbydifferent earlier experiments performed almost identically values of k and p. Given their strong similarities toXLNet-FT.GPT2performedjustslightlybetter we expect to see results as in Table 5 when the thanarandomattributor(1/4,i.e.,25%). Overall, adversarypicksotherparametervalues. </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The attributor We remind the reader that one uses either top− is aware of this ﬁne-tuning and is attempting to k or top − p sampling to control the number of tell apart these ﬁne-tuned LMs. Table 7 presents wordsunderconsiderationduringtextgeneration. the accuracy results when synthetic text is gener- We use top − p sampling as the default strategy. </Extractive Summary>  </Table ID = 7>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Accuracy percentages for identifying texts models for differentiating between synthetic and generatedbyGPT2LMﬁne-tunedonr/changemyview organic text. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thisholdseven Net, BART, GPT, GPT2. Details about those are whentheadversarydecidestouseaﬁne-tunedLM given in Table 8. Comparing sizes, XLNet is or varies text generation parameters like p-value, trainedonlargestdatasetwithover142GBofdoc- k-value, or temperature. </Extractive Summary>  </Table ID = 8>  </Paper ID = 155> 

<Paper ID = 156>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Error reductions over random baselines on Standard (original) splits, if available, Random splits (ob- tainedusingcross-validation),Heuristicsplitsresultingfromasentencelength-basedthreshold,Adversarialsplits basedon(ﬁve)approximatemaximizationsofWassersteindifferencesbetweensplits,andonNewSamples. </Abstractive Summary>  <Extractive Summary> =  Whilebootstrap ROUGE-2andROUGE-Lovertheidentitybase- resamplingleadstoslightlylowererrorreduction line (see §2) for the different data splits. The re- thancross-validationwedecidedtoreportthelat- sults are consistent with Table 2. Figure 2 gives ter in the main part of this paper, because it is a more details on an interesting drift phenomenon, morewide-spreadwaytorandomlysplitdatasets. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Error reduction as compared with an identity RMSEis0.813. </Abstractive Summary>  <Extractive Summary> =  Notethatthiswaywecreate A.1 Headlines only1dataset,becauseit’snotarandomprocess. Table 3 reports the error reduction in ROUGE-1, Results Table4liststheresults. Whilebootstrap ROUGE-2andROUGE-Lovertheidentitybase- resamplingleadstoslightlylowererrorreduction line (see §2) for the different data splits. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Error reductions over random baselines on Standard (original) splits, if available, Bootstrap splits, Random Length splits resulting from a sentence length-based separation, Rare Words splits based on word frequency. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Hence, the models are end expectedtoperformsimilarlyonthesametestset. Algorithm1:Computingadversarialsplits As Table 5 indicates, shifting the training data by ﬁve years to the past results in a big perfor- mancedrop. Samplingtrainingdatarandomlyor takingthemostrecentperiodproducesmodelswithsimilarROUGEscores,bothmuchbetterthanthe identitybaseline. </Extractive Summary>  </Table ID = 5>  </Paper ID = 156> 

<Paper ID = 157>  <Table ID = 1>  <Abstractive Summary> =  Table 1: The misclassiﬁcation detection performance (ROC AUC) (±SD) for the baseline with the ELECTRA modelandperformanceimprovementsoverthebaselineforvariousUEmethods. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: The inference time of the ELECTRA model higherthan0.1totheMCdropoutdowngradesthe onthedevelopmentdatasetwithBALDUE. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 157> 

<Paper ID = 158>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thus,w shouldbelargerthanw ,for i j ismallerthanj. Forthepurposesofthepresentar- The results obtained for testset1 of the English- ticle,w issettointegervaluesof5,2and1forthe to-Greek translation pair are depicted in Table 3, i ﬁrst,secondandthirdstagerespectively(othersets when running the single transformer, amun and ofweightvaluesthatfollowthisreasoningproduce s2smodelsrespectively,aswellastheirensembles. similarresultstothosereportedhere). According to the Wilcoxon test, these asterisk. differences are statistically signiﬁcant, at a 0.05 Based on Table 3, for testset1 the best BLEU level,onlyforthes2s(BLEUscore)andthetrans- scores are achieved by the Marian-NMT ensem- formermodel(bothBLEUandNISTscores). ble in comparison to single-NMT models. </Extractive Summary>  </Table ID = 3>  </Paper ID = 158> 

<Paper ID = 159>  <Table ID = 1>  <Abstractive Summary> =  Table 1: An illustrative example of a generated showourmethodperformsstronglyondomain rephrase using our Interchangeable Rephrase, while andintentclassiﬁcationtasksforavoiceassis- stillmaintainingtheoriginaldomainandintent. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  thethreeutterancesources. However, it merely changes a small percent of words in utterances by replacement operations, The results on DC task are shown in Table 2. In whicharenotalwaysgrammaticallysound. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  1858DOMAIN/SKILL NUM. OFINTENTS TRAINDATASIZE TESTDATASIZE WEATHER 22 732 313 SYSTEMAPP 29 665 286 SMARTTHINGS 61 1190 511 TVCONTROL 19 392 168 TVSETTINGS 40 834 357 Table6: IntentClassiﬁcationDataStatistics Intent Classiﬁcation: Table 6 shows the train- ingsplitcountsforeachofthe5differentdomains. Similartodomainclassiﬁcation,thepartitionused fortrainingisalsothepartitionthatisaugmented (andsubsequentlyalsousedfortraininginintent classiﬁcation,seeFigure3). </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  This is also illustrated in the results of domain classiﬁcation where these VAE based modelsachieveaslightimprovementsincethereis ahigheramountoftrainingdataavailable. Please refer to Table 7 for the ﬁne-grained performance results of the augmentation techniques on the 5 domains. A.5 UserStudyDetails ParticipantsrespondedtothreeitemsonaLikert- scaleofonetoﬁve: 1. </Extractive Summary>  </Table ID = 7>  </Paper ID = 159> 

<Paper ID = 160>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  WeshowinSection5 iﬁcations,papersthatdonotﬁtthenormalmould) thatthisisasubstantialproblemforcurrentsumma- thetwoauthorsdiscussedthecategorisations. Sur- rizationevaluationsandsuggestalternativeanalysis vey results are given in Table 1. Further details methods. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Results of our annotation experiment. </Abstractive Summary>  <Extractive Summary> =  Likert Gehrmann et al. (2018), which is a strong summarization system that does not rely on Table 2 shows the average Likert scores and the externalpretrainingforitsgenerationstep averagerankforallsystems,tasksandannotation methods. Weusemixed-effectordinalregression • Seneca(Sharmaetal.,2019),asystemthat to identify signiﬁcant score differences (see Sec- combines explicit modelling of coreference tion 5 for details). </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Krippendorffs α with ordinal level of mea- surementandSplit-Half-Reliabilityforbothannotation methodsonthetwotasks. </Abstractive Summary>  <Extractive Summary> =  result in consistent system scores. In Table 3 we We recruited native English speakers from the thusreportsplit-halfreliability(SHR)inaddition crowdsourcingplatformProliﬁc2 andcarefullyad- toKrippendorffsα(Krippendorff,1980). Tocom- justed the reward to be no lower than £7.50 per pute SHR, we randomly divide judgements into hour based on pilot studies. </Extractive Summary>  </Table ID = 3>  </Paper ID = 160> 

<Paper ID = 161>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We with proper reasoning. Table 1 provides a partial expecttoobservehigherlevelsoflinguisticcoordi- andtruncatedexampleofasubmissionfromCMV. nationpresentedbytheDeltagiversthanbyother participants. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  3.2 FormalizingtheResearchQuestions and verify that C(GOPs,U) > C(GOPs,U), showingthatcoordinationofOPs(indiscussions Notation Before using the deﬁnitions above to theyinitiated)towardsallotherusersishigherthan formalizetheresearchquestionspresentedinSec- thecoordinationofregularuserstowardallother tion 1.4, we introduce a few more notations, pre- users. sented in Table 2 for convenience. An important WefurtherexplorethebehaviourofOPsputting aspecttonoteisthatforeachRedditusera,wepro- forwardthefollowingnullhypothesis: ducedummyusersforeachsubmissionshetakes partin. </Extractive Summary>  </Table ID = 2>  </Paper ID = 161> 

<Paper ID = 162>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Additionally, to better estimate ria in Sec.4.2. Spot-checking reveals that this is, how- the importance of morphology, we run our cross- ever, plausible: for example, in UDP, the average la- beled attachment score (LAS) on the Thai TH_PUD tree- validationpipelineasecondtimewithoutthemor- bank was only 1.38 (Zeman et al., 2018, Table 15), with 23 systems achieving a LAS of only 0.77 or lower (out 7WeusethedefaultparametersinScikit-learn0.23,with of100; cf.http://universaldependencies.org/ theexceptionofsettingclassweightstobe“balanced”accord- conll18/results-las.html),whichisreﬂectedbyan ingtotheirfrequenciesintheinputdata. errorrateof≥99.23%inourdata. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  “Avg. F ” shows the average Table 4a shows an example for how we classify 1 F -scoreafterstratiﬁed5-foldcross-validation(cf. errors (cf. </Extractive Summary>  </Table ID = 4>  </Paper ID = 162> 

<Paper ID = 163>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Average development accuracy on morpho- logical inﬂection with different LS and β , which de- 2 notehyperparameteroflabelsmoothingandAdamop- timizerrespectively. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Average test performance on historical text Transformer(Dropout=0.1) 95.56 0.090 normalizationofTransformeragainstmodelsfromthe literature. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Average test performance on Grapheme-to- atleast128,whichismuchlargerthanbatchsize Phoneme and dev performance on Transliteration of commonlyusedinrecurrentmodels.6 Notethatthe Transformeragainstmodelsfromtheliterature. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 163> 

<Paper ID = 164>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Performance of MLE and different RL al- wordsarenottakenintoaccount,5 weobservean gorithms on the IWSLT 2014 test set trained on the improvementforthe2016and2017testsets(+0.5 IWSLT2014trainset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We also provide MLT dataset. Results are shown in Table 4. We theperformanceforthepopularMIXERalgorithm. </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  etal.,2017). Thisdemonstratesthepotentialoftheunsupervised Results are in Table 6. We observe a tendency reward function for the cases when we have to ofSAC BLEUtodowellonthetranslationofrare choose between possible translations for an am- source words, but not so well on the translation biguousword(i.e.,betterexplorationofthesearch of words in the middle frequency range (this ob- space). Our unsupervised reward tends to in- inspectionoftheseSAC unsuperimprovements crease the performance on more frequent words conﬁrmed their increased accuracy (see Table 5). (‘Other’ in Table 6) by promoting their less com- Forexample,theambiguousFrenchsourceword mon translations in the distribution, hence better ‘hill’ (‘colline’) is translated as ‘pente’(‘slope’) translationsforambiguouswordsfromourprevi- by both MLE and SAC BLEU, while only SAC ousexperiment. Theseambiguouswordsarequite unsuperproducesthecorrectsentence: ‘adoles- frequent, they potentially have multiple possible centsautelacolline‘hill’avecsonve´lo’. </Extractive Summary>  </Table ID = 6>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  BLEUreward,ontheotherhand,ismore servation is conﬁrmed by the analysis of the fre- reliable when we have to adjust distributions to quency of output words, see Appendix A.5, see produce one single possible translation. Manual Table 10). Our unsupervised reward tends to in- inspectionoftheseSAC unsuperimprovements crease the performance on more frequent words conﬁrmed their increased accuracy (see Table 5). </Extractive Summary>  </Table ID = 1>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Manual Table 10). Our unsupervised reward tends to in- inspectionoftheseSAC unsuperimprovements crease the performance on more frequent words conﬁrmed their increased accuracy (see Table 5). (‘Other’ in Table 6) by promoting their less com- Forexample,theambiguousFrenchsourceword mon translations in the distribution, hence better ‘hill’ (‘colline’) is translated as ‘pente’(‘slope’) translationsforambiguouswordsfromourprevi- by both MLE and SAC BLEU, while only SAC ousexperiment. </Extractive Summary>  </Table ID = 5>  </Paper ID = 164> 

<Paper ID = 165>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Events and periods used for each group. </Abstractive Summary>  <Extractive Summary> =  250words,andsampledfromspeciﬁcperiodsdur- (2019) used holistic grading to assess whether a ing which each group was actively discussed on text is populist or not, to later determine the de- Reddit. See Appendix A Table 3 for details. (4) greeof‘populism’ofindividualpoliticalleaders, We removed comments that contained keywords thuscreatingtheonlyexistingdatasetofpopulist frommultiplesocialgroupstomaketheannotation rhetoric,theGlobalPopulistDatabase. </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Hence, comments with higher values on the scale where therearecommentsforwhichthepredictionseems MTLimprovedtheSTLbaselinepredictionsforthe to be about a target different than the one at an- regressiontask. Commentswithhighemotionva- notation time (see the third example in Table 2). lencewerebetterpredictedbymodelsthatincluded MoreexamplescanbefoundinAppendixA.5Ta- emotionidentiﬁcation. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Mean UsVsThem Regression scale for each group and bias. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  </Paper ID = 165> 

<Paper ID = 166>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The translation was carried out by an ES, KO. The results are shown in Table 3. The in-housemultilingualsystemsimilarto(Przybysz overallpercentageofcorrectnessreached79-80%, etal.,2017;Williamsetal.,2018;Weteskoetal., whichisveryhigh,consideringthelevelofautoma- 2019). </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  of the 35 positive relations. Table 4 shows one We also tried other automated approaches for exampleofeacherror. ﬁnding no_relation sentences. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Micro F results on the Wikipedia EN-mid languages simultaneously, e.g. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 166> 

<Paper ID = 167>  <Table ID = 1>  <Abstractive Summary> =  Table 1: This table presents the best hyper-parameter baselineisessential,wecanofferasanexplanation settings of the STT model for all data sets reported in thatthelackofclarityinreproducibilityshouldnot ourexperiments. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: The number of parameters in each model for Mult(paper) 90.7 88.6 87.4 87.0 86.7 86.0 72.4 70.7 Mult(rep) 88.7 86.9 87.0 87.2 86.6 86.3 70.6 69.4 thedifferenttasks. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Ablation study and effect of different ratios sincelargeratioscorrespondtoanalmostcomplete onCMU-MOSI.Ratiovaluesvaryindifferentlevelsas dependencyontheaudioandvideofeatures(which deﬁnedin(2). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  This supports our hypothesis that one the stepped ratio from 0.1 to -0.5 and report the should utilize the text embedding to capture the best numbers. Table 6 presents results from STT major semantics and utilize the audio and video usingunimodaltextoraudioinputwhenevaluated embeddingstocaptureadditionalcrucialstylistic against the CMU-MOSI data set. From Table 6, informationinjectedintotextinamoregentleman- whileitishardtodetermineifindividuallyaudio ner. Table 6 presents results from STT major semantics and utilize the audio and video usingunimodaltextoraudioinputwhenevaluated embeddingstocaptureadditionalcrucialstylistic against the CMU-MOSI data set. From Table 6, informationinjectedintotextinamoregentleman- whileitishardtodetermineifindividuallyaudio ner. orvideomakesforabetterinputonthesentiment Finally, we see that using the stepped ratio is classiﬁcationtask,itisevidentthatthecombined better than setting the same stepped ratio at each bimodal(audio+video)inputdoesbetterthaneach layer. </Extractive Summary>  </Table ID = 6>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Results from ﬁne tuning BERT, adapter, and competitive results against ﬁne-tuning the whole our method SAdLaN on CMU-MOSEI. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  </Paper ID = 167> 

<Paper ID = 168>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Average accuracy of 5 different seeds on unseen target languages for Amazon when initializing from monolingual classiﬁer in l . </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  </Paper ID = 168> 

<Paper ID = 169>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Performance of language models on the CovidQA and BioASQ 7b1 dataset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 169> 

<Paper ID = 170>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  theeffectivenessofcapturingimportantsyntactic informationforsentimentanalysis. 4.4 ResultsandDiscussion Table 2 shows the main experimental results of Cosine Similarity the baselines and our models on four benchmark 1.0 O 1 -0.48 -0.16 0.8 datasets. We can observe that under the same 0.6 conditionofusingGloVeforwordrepresentation, 0.4 B -0.48 1 0.94 ourbasemodelTarget-BiLSTMoutperformsPE- 0.2 0.0 BiLSTM with large improvements ranging from I -0.16 0.94 1 0.2 3.31% to 6.18% on F1-score. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  4.6 AblationStudy To evaluate the inﬂuence of each component of 0.850 ARGCN, we conducted an ablation study on 0.845 ARGCN. As shown in Table 3, we observe per- 0.840 1 formancedropsonthefourdatasetswhenreplac- f 0.835 ingARGCNlayerswithR-GCNlayersfollowing 0.830 Equations(2)and(3),whichveriﬁestheeffective- ness of employing the distance attention mecha- 2 4 6 8 10 12 # GNN layers nism in ARGCN. In addition, we also ﬁnd that Figure5: EffectofthenumberofGCNlayers. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Evaluation of syntactic information (%). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 170> 

<Paper ID = 171>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Lastly,transformermodelssuch (NS) label. Table 2 shows the data statistics in asBERTandRoBERTahavebeenusedinthewin- termsofargumentativerelations(A/D/N)andsar- ningentriesfortherecentsharedtaskonsarcasm casm(S/NS). Wesplitthedatasetintotraining detection(Ghoshetal.,2020). </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Results for argumentative relation detection tection task. </Abstractive Summary>  <Extractive Summary> =  scription (e.g., Logistic Regression, Dual LSTM, BERT)isinthesupplementalmaterial. For BERT, we notice better results when per- forming multitask learning, while the best per- 5 ResultsandDiscussion forming model is obtained from BERTMTuncert whereweexperimentedwiththedynamicweight- Table 3 presents the classiﬁcation results on the ingoftask-speciﬁclossesduringthetrainingpro- testset. WereportF1scoresforeachclass(A,D cess (Kendall et al., 2018). particularfeature. Two of the authors independently investigated a random sample of 100 instances (qual set) tistically signiﬁcant, as shown in Table 3. More- chosen from the union of the test instances over,BERTMTuncert modelimprovestheF1micro that are correctly predicted only by the mul- by a large margin when compared to the LR and titask models (LR , LSTM , ArgF+SarcF MTuncert the LSTM models. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Top discrete features from LR and ArgF ple, BERT correctly identiﬁes 6 A, 50 LR models, respectively. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Evaluations of sarcasm detection on the test batchsize(e.g.,8,16,32),dropoutvalue(e.g.,0.3, setofIAC . </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 171> 

<Paper ID = 172>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Learning the relatedness between types (full trainingset). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 172> 

<Paper ID = 173>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  beddingmatrix(GloVeembeddings,inourcase). A bidirectionalLSTMlayer(HochreiterandSchmid- Wesplitthedatasetfortrainingasindicatedin huber, 1997) is used to process the sequence of Table 2. Due to the class imbalance we use the embeddingsandcalculateanembeddingforeach area under the precision-recall curve (Davis and word in the context of the utterance. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Ten features with the largest coefﬁcients of Toxicity 0.140 0.125 the best feature-based model, which uses politeness Sentiment 0.150 0.055 Politeness 0.232 0.241 andcollaborationfeaturesets.Featurevariationsarethe +gradients 0.275 0.243 mean(x¯)andgradient(∇). </Abstractive Summary>  <Extractive Summary> =  WediscussthePR- tures,withaPR-AUCof0.281(P < 0.05,usinga AUCscoresofeachmodelcategorybelow,noting randomizedpermutationtest). thatthescoresfromtheF1metricgenerallyfollow The ten features with the largest coefﬁcients thesametrends,butthatPR-AUCismorerobust from the combined model are shown in Table 4 in the face of imbalanced data. Furthermore, we indescendingorderormagnitude,alongwiththeir investigatewhetheritispossibletopredicttheout- directions. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (Zhang et al., 2016a). Adding the gradient fea- turesweproposedimprovesperformanceforboth, indicatingthatnotonlythepresenceofamarkeris 6 Results important,butalsohowitsusagethroughoutacon- Results are shown in Table 3. We note that gen- versationchanges. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Words associated with the positive and nega- tiveclass,usingabag-of-wordsmodeltopredictesca- lation. </Abstractive Summary>  <Extractive Summary> =  In Proceedings of agree -4.124 NAACL-HLT,pages136–141. discussion -4.106 A Wordslist issue 3.864 say 3.771 Words associated with the positive and negative pov -3.754 class are shown in Table 5. Coefﬁcients are de- want 3.610 termined by training a bag-of-words model with article -3.398 logisticregression. </Extractive Summary>  </Table ID = 5>  </Paper ID = 173> 

<Paper ID = 174>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Contributions from various resources for the BestWorst.html I-FORGERlexicon 15https://www.clickworker.de 16 Again,weusedthescriptsfromKiritchenkoandMo- hammad(2016,2017). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Averaged Spearman’s ρ for different models In order to gather evidence for the value of (10-fold cross-validation on I-FORGER); statistically signiﬁcant differences (using the two-sided Wilcoxon I-FORGER incombinationwiththewordscoring signed-ranktestonSpearman’sρ)aremarkedwith‘*’ approachwithinarealisticusecase,weranexperi- forp<0.005withrespecttoBOOSTEDFFNN mentswithemails,whichpossessahigherstylistic variability than news concerning their formality spread(PavlickandTetreault,2016). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Spearman’s ρ for BOOSTED FFNN on I- FORGER with results for different input streams (10- foldcross-validation) 4.3 AssessmentofFormalityScaleExtension A comparison with VULGER suggests that scor- ing an extended range of linguistic styles is a moredifﬁculttask,sinceevaluatingthe BOOSTED FFNNmodelonVULGERachievedahigherSpear- man’sρof0.827(10-foldcross-validation)thanon I-FORGER (seeTable 3). </Abstractive Summary>  <Extractive Summary> =  2Hidden NN 0.771 3Hidden BOOSTED FFNN 0.773 5 ExtrinsicEvaluationof I-FORGER Table 2: Averaged Spearman’s ρ for different models In order to gather evidence for the value of (10-fold cross-validation on I-FORGER); statistically signiﬁcant differences (using the two-sided Wilcoxon I-FORGER incombinationwiththewordscoring signed-ranktestonSpearman’sρ)aremarkedwith‘*’ approachwithinarealisticusecase,weranexperi- forp<0.005withrespecttoBOOSTEDFFNN mentswithemails,whichpossessahigherstylistic variability than news concerning their formality spread(PavlickandTetreault,2016). Otherwork 4.2 AssessmentofInputStreams related to the formality of emails is typically car- Table 3 pinpoints the predictability of formality riedoutinthecontextofcommunicationbehavior for a particular input stream of I-FORGER in a studiesinenterprises,withafocusondetermining 10-fold cross-validation setting. Learning scores socialfactors(socialdistance,relativepower,and ofCOLLOQUIALWORDSandELEVATEDWORDS the weight of imposition) that affect the sender’s seems harder than scoring SWEARWORDS and choiceofformality(Petersonetal.,2011)oronthe SIMSENTWORDS. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 174> 

<Paper ID = 175>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  context-level causal relationships, such as gen- eralcausalitydetectiontasksPDTB(Prasadetal., 2Thestatisticsarecalculatedaftermanualcleaninginthe secondstep. 1http://www.nlpr.ia.ac.cn/cip/ 3http://q.stock.sohu.com/index.shtml ˜liukang/dataset/finreason1.html 4http://choice.eastmoney.com/ 2043In total, as in Table 5, we align 8,794 documents cause the alignment in the ﬁrst step may not be with corresponding 12,861 structural events of 3 perfectly accurate, annotators are also responsi- typesinﬁnancialdomain,namelyPledgeofShares ble for removing those wrongly aligned cases in (Pledge),OverweightandUnderweightofShares theannotationtomaintaindataquality. Finally,as (O/U),LawsuitandArbitration(Lawsuit). </Extractive Summary>  </Table ID = 5>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We reasonsforeachevent. Speciﬁcally,9workersare can see in Table 2, the human performance on divided into 3 teams to annotate each event type the test set is in line with intuition. Compared separately. BiLSTM-CRF (BiLSTM): We can take the rea- Besides, we also evaluate the 3 challenges on sonsasonepartoftheeventdescriptionandregard the whole test set. As from Table 2, the average thetaskasanEEtask. SimilartoYangetal.(2018), F1gapbetweenthebestmodelandhumanforthe weemployaBiLSTM-CRF(MaandHovy,2016) 3 challenges are 12pp, 16pp, 28pp, respectively, topredictthestartandendpositionsofeachreason. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Performance of baselines and human beings thescoresasfollows. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 175> 

<Paper ID = 176>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  identiﬁesthecorrectamongmultiplelicensorsin Followingpreviouswork,wequantifyanLM’s around 97% of cases. In Table 1, we report ex- amples and frequencies of the different licensing 2Hyperparameters: batchsize=64, BPTTlength=35, contextsinthetrainingcorpusbasedonthisﬁlter- dropout=0.1,adaptiveSGDlearningrate=20,layers=2, hiddenandembeddingsize=650,epochs=40. ingscheme. </Extractive Summary>  </Table ID = 1>  </Paper ID = 176> 

<Paper ID = 177>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  andtheirtargetanswersanddialoguecontexts,and 4.4.1 AutomatedMetricsResults token-basedmetricsareinherentlyunabletomea- sure the similarity between such sequences with Results of all models and baselines are shown lowdegreesoflexicaloverlap. Asarecourse,we in Table 1. In the top row of this Table, P is alsoassessourmodelperformanceonanumberof the perplexity, B1-4 are BLEU 1 through BLEU humanevaluationmetricsdescribedinaprevious 4, M is METEOR, RL is ROUGE-L, BS is the section: AnswerConsistencyandFluidity. </Extractive Summary>  </Table ID = 1>  </Paper ID = 177> 

<Paper ID = 178>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  MutualFriends,(Heetal.,2017). Thedevelopmentofquantitativemetricstoevalu- ate the quality of dialogues generated by conver- Games Asillustratedbythesnippetsreportedin sational agents is a difﬁcult challenge (Liu et al., Table 1, the three tasks also differ in the ﬂexibil- 2016), and it is under investigation for chit-chat ityofthedialogues: GuessWhatandGuessWhich dialogue systems. For instance, Guo et al. </Extractive Summary>  </Table ID = 1>  </Paper ID = 178> 

<Paper ID = 179>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Nobataetal.(2016)alsouse 2005). Thiscanbeseenintheexampleslistedin word2vecandcomment2vecasoneoftheirfeatures Table 1, where the questions are asked to shame todetect‘abusivelanguage’which,intheirwork, the interlocutor for adopting a particular point of encompasseshatespeech,profanityandderogatory viewandareofteninsultsaskedasquestions. For language. </Extractive Summary>  </Table ID = 1>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  C Results The complete list of results for the All-Data sce- narioareshowninTable5(deeplearningmodels) andTable7(traditionalmachinelearningmodels). Next, the complete list of results for the High- Agreement-Data scenario are shown in Table 6 (deep learning models) and Table 8 (traditional machinelearningmodels). 2094Figure4: InstructionsforthecrowdsourcingtaskasseenbyMechanicalTurkWorkers. </Extractive Summary>  </Table ID = 6>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  C Results The complete list of results for the All-Data sce- narioareshowninTable5(deeplearningmodels) andTable7(traditionalmachinelearningmodels). Next, the complete list of results for the High- Agreement-Data scenario are shown in Table 6 (deep learning models) and Table 8 (traditional machinelearningmodels). 2094Figure4: InstructionsforthecrowdsourcingtaskasseenbyMechanicalTurkWorkers. </Extractive Summary>  </Table ID = 8>  </Paper ID = 179> 

<Paper ID = 180>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The MCTS 0.90 100 80 algorithm being not designed for multi-objective 0.88 problems,weneededtocombinesemanticsimilar- 0.86 ityBERTS,syntaxcorrectnessGPT2 andsurface 0.0 0.1 0.2 0.3 0.4 0.5 Levhenstein Distance diversity Lev into a single criterion. We opted S forthefollowingpolynomial: Figure 1: The cloud of candidates generated from the sample of Table 2. The optima of any positive combi- α·BERT +β·Lev ·BERT −γ·GPT2 (1) S S S nationofBERTscore,normalizedLevenshteindistance, andGPT2perplexitybelongtotheParetofront(orange where the product LevS · BERTS is intended to dots). </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Datasets statistics. </Abstractive Summary>  <Extractive Summary> =  We ex- Thisresultcanbeexplainedbythesmallnumber tendedtheexperimentstotheotheralignedcorpora: oftrainingexamplesavailableonthiscorpus(See MSRPARAPHRASE,OPUSPARCUSandPAWS. Table 4). On the weakly-supervised side, MCPG Tobemoreprecise,wetraineda4-layersLSTM and PTSmodelsoutperformthe CGMHbaselineon Seq2Seqwithabidirectionalencoderanddecoder allcorporaexceptonthe MSCOCO datasetwhere using attention. </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Experiments summary. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Data-augmentation experiment summary. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  </Paper ID = 180> 

<Paper ID = 181>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Comparisons of probability thresholding in our ablation studies. </Abstractive Summary>  <Extractive Summary> =  followingSabetetal.(2020). Wecanseethatincorporatingthisobjective(β=1) As shown in Table 3, probability thresholding cansigniﬁcantlyimprovethemodelperformance methodscanconsistentlyoutperformoptimaltrans- onRo-En,whileitalsodeterioratestheJa-Enand port by a large margin on the ﬁve language pairs. Zh-En performance by a non-negligible margin. </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Inthisparagraph, we want to ﬁnd out how our models perform on lan- Bilingual Model Performance. From Table 2, guagepairsthatithasneverseenduringtraining. we can see that our softmax model can achieve To this end, for each language pair, we train our consistentimprovementsoverthebaselinemodels, modelwithdataofalltheotherlanguagepairsand demonstrating the effectiveness of our proposed test its performance on the target language pair. method. Surprisingly, directly extracting align- Results in Table 2 demonstrate that training our ments from mBERT (the w/o ﬁne-tuning setting) modelswithparalleldataonotherlanguagepairs can already achieve better performance than the can still improve the model performance on the popularstatisticalwordalignerGIZA++on4out target language pair. This is a very important re- of5settings,especiallyintheZh-Ensettingwhere sult,asitindicatesthatourmodelcanbeusedasa thesizeofparalleldataissmall. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Wetakerep- resultingalignments. Whileeffectiveinstatistical resentationsfromdifferentlayersandreporttheper- word aligners, as shown in Table 7, the growing formanceofthebestthreelayers. Wecanseethat heuristics only improve our alignment extraction whileXLM-15(MLM+TLM)canachievethebest methodonthevanillamBERTmodelintheRo-En performanceonDe-EnandFr-En,thebestlayeris settingwhiledegradingthemodelperformanceon notconsistentacrosslanguagepairs. </Extractive Summary>  </Table ID = 7>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Word alignment has studiesoneachofourtrainingobjectives. Wecan been a useful tool in cross-lingual annotation see from Table 5 that the self-training objective projection (Yarowsky et al., 2001; Nicolai and can best improve the model performance. Also, Yarowsky, 2019). </Extractive Summary>  </Table ID = 5>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  <Table ID = 1>  <Abstractive Summary> =  Table 10: Our model is also effective in monolingual 1e-2 15.3 4.3 22.7 37.9 13.8 alignmentsettings. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 181> 

<Paper ID = 182>  <Table ID = 2>  <Abstractive Summary> =  Table 2: For an analogy equivalent to two paraphrases W and W , the rank of W in the list of the closest ∗ ∗ paraphrases to W with respect to the L2 norm of the paraphrase error vector. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ferencebetweentwoPCIcolumnsisaparaphrase Wealsoreplacelog(0/0)with0. error vector, and their Euclidean distance is the Table 1 shows the mean and median values of normoftheparaphraseerror. theL2normsoftheparaphraseerrorvectorsacross Wenowcompute,foreachanalogy,thedistance several categories of the BATS dataset. </Extractive Summary>  </Table ID = 1>  </Paper ID = 182> 

<Paper ID = 183>  <Table ID = 1>  <Abstractive Summary> =  Table 1: F1 scores for attribute prediction and accura- fulandfailedgoldgamesareusedbythegenerator ciesforzero-shotevaluationonCompGuessWhat?!. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  TDIUC is evaluated using the and location attributes (L); 2) zero-shot game- arithmetic mean accuracy per question type (A- playwithnear-domainaccuracy(ND)andout-of- MPT), as well as the harmonic mean (H-MPT) domainaccuracy(OD).Table1showsthecompar- thatbettercapturestheskewedquestion-typedis- isonwithpreviousstate-of-the-artmodelsonthis tribution. In Table 2, we report a comparison benchmarksuchasdeVriesetal.(2017)(DV-*) between variants trained on guessing games data and Shekhar et al. (2019) (GDSE-*). </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thebaselinemodelused byalltheotheris78.5%(deVriesetal.,2017). Guesser evaluation We report in Table 3 the accuracy of the guesser in predicting the tar- get object when gold dialogues are given in input. We compare this model with several baselines reported in (de Vries et al., 2017) (ﬁrst block), more sophisticated methods such asParallelAttention(Zhuangetal.,2018) andGDSE-*(Shekharetal.,2019)(secondblock) aswellasotherTransformer-basedmodelssuchas VILBERT(Luetal.,2020)(thirdblock). </Extractive Summary>  </Table ID = 3>  </Paper ID = 183> 

<Paper ID = 184>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Class distributions of fully-labeled words in Impact .630 87.6 73.7 94.6 theconnotationlexicon. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Lexicon annotation results. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 1>  <Abstractive Summary> =  Table 13: Macro-averaged F1 results for connotation womenin prediction on the test set. </Abstractive Summary>  <Extractive Summary> =  distributions(e.g.,10.5%ofwordsarepoliteand To generate our lexicon, we map dimensions only1%areimpolite)(seeTable2). Foremotions, from existing lexica to connotation aspects (see wecalculatetheclassdistributionusingthenum- Table 1). We use dimensions from the Harvard ber of fully-labeled words with atleast one asso- General Inquirer (Stone and Hunt, 1963) for So- ciated emotion (1,373 wordsor 18%). </Extractive Summary>  </Table ID = 1>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Stance detection macro-averaged F1 on the test set. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Neitherversionincludesexplicitinformation onpart-of-speech,andsoweinferpart-of-speech usingthewordsprovidedtodistinguishdifferent senses. Weprovidethecompletedistantlabelingrules for each of the connotation aspects in Table 9 (seehttp://www.wjh.harvard.edu/˜inquirer/ homecat.htmforcompleteinformationonabbrevi- ations). Withineachconnotationaspect,wedeter- minetheconnotationpolarityusingtheadditional categories: Positiv,Negativ,Strong,Weak,Hostile, Submit,ActiveandPower. </Extractive Summary>  </Table ID = 9>  </Paper ID = 184> 

<Paper ID = 185>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Next,wefurtherprobetheperformance ofFASTthroughaseriesofablativeexperiments. Astheultimategoalofstockpredictionisproﬁt,we comparetheproﬁtabilityofFASTagainstbaseline 5.2 FASTComponentAblation methods in Table 2. FAST generates signiﬁcantly (p < 0.001)highercumulativeandrisk-adjustedre- Table1showshowFAST’sstockrankingabilityand turnsthanallmethods. </Extractive Summary>  </Table ID = 2>  </Paper ID = 185> 

<Paper ID = 186>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Sources of potential bias in data collection when operating in rural and illiterate settings in developing countries,andkeycountermeasuresthatcanhelpmitigatingthem. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 186> 

<Paper ID = 187>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Details of PEG predicate types, along with Autoprotocolorsimilarlaboratoryinterfacesonce examplefrequenttriggerspansandrelativefrequency it is fully instantiated, thanks to edge labels and inX-WLP. Table 11: Operation argument role labeling (core taskwithgoldinputdenotedasPIPELINE(gold). Table 13: Details of non-core roles and restrictions on source and target node types. </Abstractive Summary>  <Extractive Summary> =  in(Waddenetal.,2019). Third,cross-sentencerelationsarechallenging for both models, as shown in Table 11. This 6.1 Results explainsthelowperformanceofco-ref,which The results of the two models on the different iscomprisedof92.4%cross-sentencerelations. For example, we use a more general Spin Spin Convert N/A measure operation type rather than the speciﬁc Seal Seal,Cover typesofmeasurementoperationsinAutoprotocol Create Oligosynthesize,Provision (spectrophotometry, measure-volume, General N/A Destroy N/A etc.). Absorbance,Fluorescence, Table 12 maps between X-WLP operation Luminescence,IlluminaSeq, SangerSeq,MeasureConcentration, types and their equivalents in Autoprotocol, if Measure MeasureMass,MeasureVolume, one exists. The X-WLP operation types do CountCells,Spectrophotometry, not perfectly overlap with Autoprotocol as the FlowCytometry,FlowAnalyze, ImagePlate former is written for humans, while the latter Mix Agitate is designed for the more constrained domain of Remove Unseal,Uncover robotexecution. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  As can be seen in mustbeﬁlledforagivenoperation. ARG*isshortfor Table 4, X-WLP annotates long examples, often tARG0,ARG1,ARG2u. spanning dozens of sentences, and its size is comparable(e.g.,intermsofannotatedwords)to researchercanspecifywhatgentlymeansinterms the ProPara corpus (Dalvi et al., 2018) and other relatedproceduraldatasets. X-WLPisourmaindatasetincluding279 Non-Core ‚Allroles 55.7 48.8 4826 fully annotated protocols. Statistics of X-WLP ‚Allroles(goldmentions) - 78.1 4826 are presented in Table 4. Additionally, we have ‚site 58.7 55.4 962 ‚setting 77.4 74.7 974 344protocolsfromtheoriginalWLPdataset. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: X-WLP inter-annotator agreement metrics. </Abstractive Summary>  <Extractive Summary> =  For example, the second interaction ournotation)anddirectedlabelededgeswhichcan in Figure 2 indicates a missing argument for the formundirectedcyclesthroughreentrancies(nodes chill operation (the argument to be chilled). with multiple incoming edges).4 In Table 5 we Finally, tracking temporal dependency (“succ” report a graph Smatch score (Cai and Knight, edges)isalsomanagedentirelybythesimulatorby 2013) widely used to quantify AMR’s graph trackingtheorderinwhichtheannotatorissuesthe structureagreement,aswellasﬁnergrainedgraph differentoperations. agreement metrics, adapted from Damonte et al. </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Comparison of average arguments per Re-entrancies 73.12 operation and percentage of semantically under- speciﬁedoperations(missingcorearguments)inWLP Table 5: X-WLP inter-annotator agreement metrics. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  sub-component prediction, which are chained to Analysis of the relations in X-WLP, presented formapipelinePEGpredictionmodel. Second,in in Table 6, reveals that a signiﬁcant proportion §5.2wepresentamodelwhichdirectlypredictsthe of arguments in PEGs are re-entrancies (32.4%) or cross-sentence (50.3%).5 Figure 3 shows a entirePEGusingaspan-graphpredictionapproach. representativeexample,withthevialparticipating 5.1 PipelineModel(PIPELINE) inmultiplere-entranciesandlong-rangerelations, AfullPEGrepresentationasdeﬁnedin§3canbe 5Forthesecalculationsweconsideronlyargumentrelations obtainedbychainingthefollowingmodelswhich thatcaninprincipleoccurasre-entrancies: “ARG*”and predictitssub-components. </Extractive Summary>  </Table ID = 6>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We use the uncased versionofSciBERT7 forallourmodelsduetothe First, PIPELINE outperforms MULTI-TASK on importanceofin-domainpre-training. Themodels the operation classiﬁcation task in Table 9, as it underthe PIPELINEsystemareimplementedusing usesallprotocolsfromWLPasadditionaltraining HuggingfaceTransformers(Wolfetal.,2020),and datatoimprovementiontagging. we use AdamW with the learning rate 2 ˆ 10´5 Second,MULTI-TASKperformsbetterthanthe for SciBERT ﬁnetuing. </Extractive Summary>  </Table ID = 9>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  points to note. In Table 8, the performance of PIPELINE on the X-WLP subset is lower than its 7https://github.com/allenai/scibert performance on the WLP test set, likely because 2197there are fewer protocols in the training set. For procedure-levelrepresentations.8 therelation-decomposedperformanceinTable10, Anotherlineofresearchfocusesonprocedural wecanseethatsomeoftherelationslike“ARG2” text understanding for more general domains: canbecorrectlypredictedbyMULTI-TASKusing simple scientiﬁc processes (Dalvi et al., 2018), only a few gold labels while some more widely opendomainproceduraltexts(Tandonetal.,2020), usedrelationsarehardertolearn,suchas“ARG0” andcookingrecipes(Kiddonetal.,2015;Bosselut and “site”; indeed, “ARG2” is only used in the etal.,2018). </Extractive Summary>  </Table ID = 8>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Dependent on a particularroler,certainrestrictionsmayapplyto theﬁne-grainedtypeofsandt,asdescribedbelow. A.3.1 CoreRoles Core roles, displayed in Table 3, represent operationspeciﬁcroles,forexample“ARG1”for thesealoperationisasealentityrepresentingthe sealofthe“ARG0”argument. Forcoreroles,the followingrestrictionshold: • Source nodes s are restricted to any of the object types s P treagent,device,seal,locationu representing physical objects. </Extractive Summary>  </Table ID = 3>  </Paper ID = 187> 

<Paper ID = 188>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Vocabulary sizes. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Lexical Frequency Proﬁle (French, Spanish, ﬁrst2000wordsand(iv)theremainingwords. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Synonym frequency metrics for our MT Carthy,2005). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  For Spanish{“mirar”,“esperar”,“buscar”,“parecer”, easeofreadabilityandcomparisonwemultipliedTTR “dar”, “vistazo”, “aspecto”, “ojeada”, “mirada”}, scoresby1,000andYule’sIscoresby10,000. thenumberofappearancesintheTRANSdataare as follows: {(“mirar”: 4002), (“esperar”: 3302), The scores in Table 5 show that, overall, and (“buscar”: 2814),(“parecer”: 1144),(“dar”: 977), accordingtoallthreemetrics,theoriginaltraining (“vistazo”: 182), (“aspecto”: 46), (“ojeada”: 0), datahasahigherlexicaldiversitythanthemachine (“mirada”: 0)}. Fromthismappingoftranslation translationese. </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  Table 7: An illustration of the Shannon entropy and English. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  </Paper ID = 188> 

<Paper ID = 189>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Relative Zero shot Cross-Lingual perfor- mance slightly decreases by 0.09 points in the manceofmBERTwithRANDOM-INIT (§2.1)onpairs same-language while it increases by 1.00 in the of consecutive layers compared to mBERT without cross-language case). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Trans- formers: State-of-the-art natural language process- ing. InProceedingsofthe2020ConferenceonEm- pirical Methods in Natural Language Processing: 2221A Appendices Domain Analysis Datasets We list here the datasets for completing our domain analysis ex- A.1 Reproducibility periment in Section 4.1 reported in Table 2. To A.1.1 Optimization haveafullcontrolonthesourcedomains,weuse Wefine-tuneourmodelsusingthestandardAdam forfine-tuningtheEnglishPartuttreebankforPOS optimizer (Kingma and Ba, 2015). </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Fine-tuning best hyper-parameters for each 0 2 4 6 8 10 Hidden Layer Index taskasselectedonthevalidationsetofthesourcelan- guagewithbounds. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Spearman-Rank Correlation between the Cross- observe the same distribution for each task with lingualGap(X-LangGap)andtheCross-lingualSimilarity betweenthesourceandthetargetlanguagesofthefine-tuned layer6beingthemostcorrelatedtocross-lingual modelsandthepretrainedmodelaveragedoverallthehidden transfer. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Parsing (LAS score) Relative Zero shot Cross-Lingual performance of mBERT with RANDOM-INIT (section2.1)onpairsofconsecutivelayerscomparedtomBERTwithoutanyrandom-initialization(REF). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  </Paper ID = 189> 

<Paper ID = 190>  <Table ID = 2>  <Abstractive Summary> =  Table 2: F1 results (in %) from Test Set 2. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Proportion of female pronouns assigned for ofgenderbias,allowingustobetterprobemodel each competency category for the WinoBias T2 sen- tences. </Abstractive Summary>  <Extractive Summary> =  The online approach has been shown agnosticterm“person”. to be effective at mitigating stereotype at the ex- penseofskew,demonstratingtheoppositeeffectto Table 3 reports the proportion of examples in what it was designed for. We took the Data Aug- eachcompetencyclassthatwereassignedafemale mentationmethodproposedbyZhaoetal.(2019) pronounacrossBERT,BERT-U,andBERT-A.The for debiasing ELMo and extended it to BERT, proportionsoffemalepronounassignmentsshow demonstratingthatitreducesbothformsofgender that BERT-A allocates a more balanced ratio of biascomparedtounaugmentedﬁne-tunedmodels. </Extractive Summary>  </Table ID = 3>  </Paper ID = 190> 

<Paper ID = 191>  </Paper ID = 191> 

<Paper ID = 192>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Variations for each knowledge setting. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: %Knowledge extracted for each subset wrt. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Accuracy for extraction variations on: SIQA best alignment, due to the high quality achieved CS-3forATOMICpaths&ConceptNettriples; PIQA whenconstrainingknowledgefortheSIQAques- CS-2 for ATOMIC pairs & ConceptNet subgraphs & tiontolinktoknowledgefortheanswer. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  wegainfromourKGsviaourtransformer-based 6.2 Analysis: CommonsenseProbes KS model with respect to a BERT baseline. Sec- ond,weexaminedistributionalchangesinourmod- Table 4 compares the performance between the elsbeforeandaftercommonsenseintegrationand best performing ATOMIC-SIQA KS model and verify our results with human evaluation. With respectiveSIQABERTbaseline. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Distribution changes for selected class from baselinetoKSmodel. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Valid, inthiscase,meansthatthecorrectconcepts(that We conduct an integration analysis on our best identify a relevant knowledge gap) were used to ATOMIC-SIQAsetting(QC-HQ).Weexamine40 create a link. These results are found in Table 6. multiple choicequestions andanalyze KS model WealsoshowthatforourbestATOMICextraction predictionchangeswithrespecttothebaseline. </Extractive Summary>  </Table ID = 6>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ceptNetfor43%. Thehighercorrectextractionsfor 2266Type BERT RoBERTa 7.2 Results RelationProbes Table 8 compares the performance of BERT and xWantvsxEffect 60.90 56.68 RoBERTaforzero-shotresults. Majoritylabelre- xWantvsxReact 94.15 94.96 xWantvsxIntent 57.02 57.10 sultsarefoundintheappendix. </Extractive Summary>  </Table ID = 8>  </Paper ID = 192> 

<Paper ID = 193>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  This results in consistingofatokenaandalabelAtakenfroma ﬁxedset.2 Toavoidclutter,wewriteaA inplaceof Table2. (a,A);thisalsoemphasizestherelationtomoretra- Thereisasimpleone-to-onecorrespondencebe- ditionalformulationsofdependencyparsing,which tween a computation according to Table 2 and a areobtainedbyomittingthesuperscripts. computation according to Table 1 satisfying the left-before-right policy. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  Table 10: Grammar for unnormalized arc-eager de- pendencyparsing. </Abstractive Summary>  <Extractive Summary> =  (a,A);thisalsoemphasizestherelationtomoretra- Thereisasimpleone-to-onecorrespondencebe- ditionalformulationsofdependencyparsing,which tween a computation according to Table 2 and a areobtainedbyomittingthesuperscripts. computation according to Table 1 satisfying the left-before-right policy. The difference is merely A ﬁrst illustration of this is traditional shift- an application of complete just before a token reduce dependency parsing, deﬁned by the tran- ceases to be a topmost stack element, either be- sitions in Table 1, here without labels, or alterna- cause it is reduced into the token to its left, or tively,onemayconsidertheretobeonlyonesuch becauseanothertokenispushedontop. computation according to Table 1 satisfying the left-before-right policy. The difference is merely A ﬁrst illustration of this is traditional shift- an application of complete just before a token reduce dependency parsing, deﬁned by the tran- ceases to be a topmost stack element, either be- sitions in Table 1, here without labels, or alterna- cause it is reduced into the token to its left, or tively,onemayconsidertheretobeonlyonesuch becauseanothertokenispushedontop. Ifatoken 2Thereisacloseconnectiontobilexicalcontext-freegram- mars(EisnerandSatta,1999),onthebasisofwhichonemay 3Shift-reducedependencyparsinghasbeenknownatleast alternativelychoosetorefertosuchalabelasa‘delexicalized sinceFraser(1989)andNasr(1995).Itisalsoreferredtoas stacksymbol’,inakindoflexicalizedpushdownautomaton. E.g. rule (5) in was in the remaining input, or that was the top Table 10 counts the ﬁrst right child, but there is of stack with label N. In (6), the underscore can no further rule with right-hand side ( ,R ) R to 1 be substituted by R or Rt. policiesrespectively. Themostfrequenterrorsare The parser was ﬁrst trained on conﬁgurations listed in Table 11. Somewhat surprisingly, in the correspondingtoprojectivizedgoldtreesfromthe great majority of cases, the approximate set was German(GSD)corpusofUniversalDependencies containedinthetrueset;thesecasessumto89.0% v2.2. 8 GB of RAM. The larger context-free grammar of Table 10, relative to the one for shift-reduce 7 Conclusions parsing, leads to a higher constant factor in the time complexity. Nonetheless, the calculation is Ourexactcalculationoftheoptimalstepsolvesan feasibleevenforlongsentences. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Reformulated arc-eager parsing, with X ∈ Table3: Arc-eagerparsing(Nivre,2008,p.525). </Abstractive Summary>  <Extractive Summary> =  Here 12) R →(R,R) 28) L →(L,L) rules(7)–(9)areneededtogiveanR-labeledstack 13) R →(R,R1) 29) L →(L,L1) elementatleastonerightchild,whichby(14)–(15) 14) R →(Rp,R) 30) L →(Lp,L) allowsthetokentoparticipateinafullderivation. 15) R →(Rp,R1) 31) L →(Lp,L1) Inordertocomputethescoreforarc-eagerpars- 16) Rp →(Rp,Rp) 32) Lp →(Lp,Lp) ing without our correction (starting in Table 4 (N,N) →n (N, ) →L (N, ) 33) 41) b withreduce correct),oneshouldomittherules (N ,N )→n N →(N,N) 34) p p p 42) fromTable10thatcorrespondtoL-labeledtokens ( ,N) →( ,N)N N →(N,N ) 35) 43) p becoming right children, i.e. (6), (9), (22), (25). </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The ﬁrst is that this earlier com- ofthestack,afterapplicationsofreduce leftthat mitment made by right arc, in terms of the ear- giveitrightchildren,thenitwillstillhavelabelC, lier creation of the dependency edge, offers addi- whichpreventsitfromtakingfurtherleftchildren. tional information about the tree under construc- Table 3 is almost verbatim the formulation of tion, to better predict the next steps, using some arc-eagerparsingbyNivre(2008),exceptthatwe type of classiﬁer. The second argument in favor renamedsymbols,andweignoredependencyrela- ofarc-eagerparsingisthattheearliercreationof tions;theformulationsbye.g.Nivre(2003,2004) the dependency edge ensures that the partial tree and Nivre et al. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Arc-eager parsing is stuck in a conﬁguration (αaX1 aY2 , β, T) (cid:96)RE (αaX1 , β, T ∪{(a1,a2)}) withoutapplicabletransitions. </Abstractive Summary>  <Extractive Summary> =  Theconventionalwisdomof Arc-eagerparsingineitheroftheabovetwofor- deterministicparsingisthatoneshouldpostpone mulations cannot work in practice. The problem commitmenttooccurrencesofgrammarrules(or is illustrated in Table 5. In the last conﬁguration, here, dependency edges) for as long as possible, noneofthestepsisapplicable. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Here we assume the for- length k and remaining input length m. Now (cid:96) is b mulation of arc-eager parsing as in Table 6. The used if the candidate transition is shift, and a non- numberσi,asdeﬁnedinSection4,foraconﬁgura- bottommost symbol to the left of that becomes rp or tionwithstackα = a ···a andremaininginput (cid:96)p. </Extractive Summary>  </Table ID = 6>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Normalized arc-eager parsing, with X,Y ∈ stillbesolvedinlineartime,byastraightforward {R,L}. </Abstractive Summary>  <Extractive Summary> =  However,theseedgesarestillidentiﬁed child or a right child. Table 8 presents this nor- by investigating which tokens in the stack have malized arc-eagerparsing. Theshiftisnowsim- labelR: theirparentisthetokenimmediatelyleftto ply the transfer of a token from remaining input itinthestack. Where before we had left arc, we arefewstudiesthatcompareparsingaccuracybe- nowhavereduce right,andreduceismoreap- tween the two ways of resolving this, by prefer- propriately renamed to reduce left. There is a ring either shift before reduce, or reduce be- simpleone-to-onecorrespondencebetweenacom- foreshift,andsomeliteraturesuggeststhechoice putation according to Table 8 and a computation is arbitrary,5 although the results from one study accordingtoTable6satisfyingthereduce-before- 5Cf.“harmlessSHIFT-REDUCEconﬂicts”(Nivre,2006,p. 98). </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  Table 9: Grammar for normalized arc- correspondingderivation. </Abstractive Summary>  <Extractive Summary> =  thegoldtree. Because|T ∩T |isthesameforall We start with the normalized form (Table 8), g i, and because the value of |T(cid:48)(cid:48) ∩ T | ≤ 1 with which requires the grammar in Table 9, with the i g (α,β,∅) (cid:96)i (α ,β ,T(cid:48)(cid:48))iseasilydeterminedbya indicatedtranslationfromtheconﬁgurationtoan i i i singlelookup,theremainingproblemistocompute inputstring. Theintuitionbehindthisgrammaris similar to the one by Nederhof (2019), but more 6Thedivisionoflaborbetweenstackandremaininginput cases need to be distinguished due to the labels. </Extractive Summary>  </Table ID = 9>  </Paper ID = 193> 

<Paper ID = 194>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Subword tokenization statistics by language and model. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Distribution of the location of the highest a l weightedsubword. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 194> 

<Paper ID = 195>  <Table ID = 1>  <Abstractive Summary> =  Table 10: Hyperparameters of the best performing SVMrank models for all ﬁve datasets. Table 11: Hyperparameters of the best performing lambdaMART models for all ﬁve datasets. Table 12: Hyperparameters of the best performing neural models for all ﬁve datasets. </Abstractive Summary>  <Extractive Summary> =  from the JSTOR Understanding Series (JS- TOR Labs, 2019). The JSTOR Labs team As seen in Table 1, there is substantial variation createdadatabaseofallquotationswithinJS- acrossthefourdatasets,bothintermsoftotalnum- TOR,then,usingtextreusedetectionmethods, bersofdocumentsandpassagesineachdatasetand alignedthosequotationstopassagesinanum- inmediannumbersofpassagesperdocumentand berofgreatworks,includingtheKingJames tokensperpassage. Forexample,LATcontainsa Bible,Shakespeare,andAmericanandBritish relativelylargenumberofdocumentsandpassages, Literature datasets. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Summary statistics for the three derived doc- passages are quoted at least once, the passages ument datasets – Chronicling America (CA), JSTOR: in the 75th percentile are quoted three times, and EarlyJournalContent(EJC)andJSTOR:All(JA). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Results from examination of linguistic attributes of quotations. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: 5-fold cross validation results on the collec- other, we ﬁnd that models struggled most on the tionofLatintexts(LAT)withalignmentsfromtheJS- Latindataset,LAT-EJC,andachievetheirhighest TOREarlyJournalCollection(EJC).WereportNDCG scoresontheShakespearedataset,SHAK-JA. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  Table 7: The top 3 most quoted books of the King level. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  </Paper ID = 195> 

<Paper ID = 196>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  graphs. Note for this particular task, researchers However, there is no single dataset that contains observe that CIDEr-D is susceptible to common bothvisualcomparisonandsingle-imageannota- patterns in the data (See Table 1 for proof), and tions. Hence,weleveragetwodatasetsfromsimi- ROUGE-L is anecdotally correlated with higher- lardomainstofacilitatetraining. AsshowninTable2, L2C outperforms 3.2 AutomaticEvaluation CNN+LSTM,whichisconsistentwithautomatic metrics. As shown in Table 1, ﬁrst, L2C[B2W] (training with visual comparison task only) outperforms 3.4 AblationStudies baselinemethodsonBLEU-4andROUGE-L.Pre- Effect of Individual Components We perform viousapproachesandarchitecturesfailedtobring ablationstudiestoshowtheeffectivenessofseman- superiorresultsbydirectlymodelingthevisualrela- ticpooling,totalvarianceloss,andgraphreason- tionshiponResNetfeatures. Second,jointlearning ing, as shown in Table 3. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Human evaluation results. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  As shown in Table 1, ﬁrst, L2C[B2W] (training with visual comparison task only) outperforms 3.4 AblationStudies baselinemethodsonBLEU-4andROUGE-L.Pre- Effect of Individual Components We perform viousapproachesandarchitecturesfailedtobring ablationstudiestoshowtheeffectivenessofseman- superiorresultsbydirectlymodelingthevisualrela- ticpooling,totalvarianceloss,andgraphreason- tionshiponResNetfeatures. Second,jointlearning ing, as shown in Table 3. First, without seman- withasingle-imagecaptionL2C[B2W+CUB]can tic pooling, the model degrades to average pool- helpimprovetheabilityofsemanticunderstanding, ing, and results show that semantic pooling can thus,theoverallperformanceofthemodel. </Extractive Summary>  </Table ID = 3>  </Paper ID = 196> 

<Paper ID = 197>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  • EXCLUDE:thecustomeraskstoexcludeprod- uctshavingaspeciﬁcvaluefortheattribute, e.g.,“excludethepurpleones”. As reported in Table 1, we collected 13,492 utterances, and 9,810 (i.e., ∼72%) were consid- • RANGE:thecustomeraskstoselectproducts eredvalidreﬁnementsinthesubsequentannotation having attribute values in a closed interval, phase,whiletherestarenotreﬁnementsorcontain e.g.,“Pricebetween200and300”. ASRerrors. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Differences between utterances from differ- 3.2 DescriptiveAnalysis entregions. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  test the model, we split the data into train, Experimental Results. Table 5 reports the validationandtestportions,asreportedin modelperformance(withandwithoutcontext)at table4. ThesplittinghasbeendoneatWorkerlevel, entire span level and at token level. </Extractive Summary>  </Table ID = 5>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Finally, on all attributes. In terms of generalization capa- apossibleexplanationforthelowerperformance bility, the best performance is observed on price onthematerialreﬁnementsisthesmallertraining anddiscount,withlearningcurvesthatstartfrom size, as shown in Table 2. Less variability w.r.t. </Extractive Summary>  </Table ID = 2>  </Paper ID = 197> 

<Paper ID = 198>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Results of clustering performance for different document representation strategies as compared against contemporarymodels. </Abstractive Summary>  <Extractive Summary> =  tering performance of the model. It can be seen (rowsthatarenotmarkedwith“Time”inTable1) in Table 1 that introducing entity awareness and andthenstreamdocumentsinrandomorder(rows trainingontheeventsimilaritytask(TF-IDF+E- marked with (“out- of-order”) in Table 1), the S-BERT + Time) results in a clustering score of numberofclustersincreaseoverwhenaccounting 94.76%3, achieving a new state-of-the-art on the fortime. Whenablatingtime,wealsoobservethat dataset4. tering performance of the model. It can be seen (rowsthatarenotmarkedwith“Time”inTable1) in Table 1 that introducing entity awareness and andthenstreamdocumentsinrandomorder(rows trainingontheeventsimilaritytask(TF-IDF+E- marked with (“out- of-order”) in Table 1), the S-BERT + Time) results in a clustering score of numberofclustersincreaseoverwhenaccounting 94.76%3, achieving a new state-of-the-art on the fortime. Whenablatingtime,wealsoobservethat dataset4. The increaseinB-Cubedmetrics. scores reported in Table 1 correspond to entity- WhileB-CubedF scoreisthestandardmetric awaremodelstrainedonthislabelsubset. Wealso 1 reportedintheliterature,itisanarticle-levelmet- experimentedwithseparateembeddingsforeach ricwhichgivesmoreimportancetolargeclusters. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Results of clustering performance across different evaluation metrics. </Abstractive Summary>  <Extractive Summary> =  Theresultsarestatisticallysigniﬁcantand supplying documents in random order produces pvaluesfromapairedstudent’st-testarereported fewer clusters and better b-cubed F scores. We 1 in Table 2. This is almost 3 points better than observe examples of clusters that are incorrectly thecorrespondingmodelwithoutentityawareness, mergedintheabsenceoftemporalinformation(in which highlights the importance of this external theout-of-ordersetting). Inordertoensurethatourmodel’sbetterperfor- Inthispaper,wepresentanovelnewsstreamclus- manceismetric-agnostic,wealsoempiricallyeval- teringalgorithmthatusesacombinationofsparse uatedourclusteringmodelagainstpriorworkusing and dense vector representations. We show that severalclusteringmetrics,theresultsofwhichare while dense embeddings by themselves do not presented in Table 2. For this, we compare with achievethebestclusteringresults,enhancements Mirandaetal.(2018)sincetheirresultsarereadily like entity awareness and event similarity ﬁne- replicable. </Extractive Summary>  </Table ID = 2>  </Paper ID = 198> 

<Paper ID = 199>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  By using the average of scoreof3areneutralreviews; andtheremaining arepositivereviews. Thestatisticsofourdatasetis weightedwordaccount,theprocessisnowdiffer- shown in Table 12. We can observe that our data entiable and we use the sampled word counts to ishighlyimbalanced,withthepositivereviewsfar formthedocumentrepresentationandfeeditasan morethannegativeandneutralreviews. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Brand ranking results generated by various classlabelsintothemodelfortraining,essen- modelsbasedonthetestset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Example topics generated by BTM and TBIP on Amazon reviews. </Abstractive Summary>  <Extractive Summary> =  In the topics generated by reviews. TBIP and BTM, we can vary the topic polarity 3https://mpqa.cs.pitt.edu/lexicons/ scores to generate positive, negative and neutral 2346subtopics as shown in Table 4. We would like to clearly expressing negative sentiment under the achievehightopiccoherence,butatthesametime negativesubtopic. Aswillbeshownin polarityimplyingapoorquality. Assuch,itisdif- Table 4, topics extracted by TBIP contain words ﬁcult for TBIP to separate words under different signiﬁcantly overlapped with each other among polarity-bearingtopics. Onthecontrary,withthe sentimentsubtopics. polarityoftheword‘cheap’. Forexample,‘cheap’ appears in both positive and negative subtopics Topic Topic Model for ‘Brush’ in Table 4. But we can ﬁnd other co- Coherence Uniqueness occurred words such as ‘pretty’ and ‘soft’ under JST 0.1423 0.7699 thepositivesubtopic,and‘plastic’and‘ﬂimsy’un- JST* 0.1317 0.7217 derthenegativesubtopic,whichhelptoinferthe SCHOLAR 0.1287 0.9640 contextualpolarityof‘cheap’. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Topic coherence/uniqueness measures of re- subtopics extracted by TBIP, all of them convey sultsgeneratedbyvariousmodels. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 199> 

<Paper ID = 200>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  yses, we opt for 10-fold cross-validation on the training split of the data in experiments. Statis- 4.1 Word-levellanguageidentiﬁcation ticsofthetrainingsplitsofthedatasetsareshown Wetreatlanguageidentiﬁcationasasequencelabel- in Table 1. The datasets are relatively small, but ingtaskwherethelabelofeachwordisalanguage a high ratio of words is normalized, including a ID.Weevaluatethreesequencelabelinglibraries: highpercentageofsplitsandmerges. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  On top of this, we 5.1 Languageidentiﬁcation alsoaddafeaturethatindicateswhichlanguagea Results for the language identiﬁcation task are wordbelongsto. Theremightbesomemismatches reported in Table 3. Unsurprisingly, the perfor- in the importance of features because different mances are in line with the chronological order data sources and languages are used. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Normalization performance of the baselines Table5:Effectofdifferentlanguagepredictionsonnor- and the proposed models (10-fold accuracy). </Abstractive Summary>  <Extractive Summary> =  withinalanguagepair(twomodels). Results for the different models are compared Effect oflanguage predictions To evaluate the in Table 4. For the Id-En dataset, the differences effectofthelanguagepredictions,werunboththe betweenallproposedmodelsaresmallandnotsig- Fragments and the Language-aware mod- niﬁcant. </Extractive Summary>  </Table ID = 4>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Normalization and POS tagging accuracies izeCSdata. </Abstractive Summary>  <Extractive Summary> =  The results more language pairs. We hope to evaluate this in Table 7 show that, parallel to 10-fold cross- setupifresources(i.e.,normalizationtestdatafora validationresults(Table4),Multilingualand CSlanguagepair,andmonolingualnormalization Language-aware scores are similar and their trainingdataforbothlanguages)becomeavailable. difference is insigniﬁcant for both datasets. </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Normalization accuracies per language (with ADJ-PROPN 103 -2 +0 +5 goldlanguagelabels) Table10:10mostcommonPOStaggingerrorsforLAI baseline,countedforallnormalizationstrategiesforthe Id-En Tr-De MaChAmptagger. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  baseline(LAI).Thetagontheleftisgold,rightispre- B Precisionandrecall dicted. Table 9 show the precision and recall of all mod- els on both datasets. LAI has 0.0 on all metrics, becauseitneverﬁndsacorrectnormalization. </Extractive Summary>  </Table ID = 9>  </Paper ID = 200> 

<Paper ID = 201>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Statistics of the training, validation, and test should be able to better understand the tables by setsfortheTabFactinChenetal.(2019). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Percent accuracy of different models for table-based fact veriﬁcation on the TabFact test set. </Abstractive Summary>  <Extractive Summary> =  typesofedges—greaterandlessthan—according thevaluesofthecells. ANumGNNlayeristhen 4.2 MainResults utilizedtopropagateinformationamongthenodes Table 2 presents our main results, comparing inthegraphtointegratethenumericalcomparison TAPAS-derived models with baseline models on information into each table cell’s representation. table-based fact veriﬁcation. Table-BERTusesasim- pletemplatetotransformatableintoa“somewhat Model Parameters Following Table-BERT, all natural”sentencetoexploitalanguagemodelpre- ofourmodelsarebasedontheopen-sourceimple- trained on natural sentences. However, Table 2 mentation of BERT with 12-layers, 768-hidden, showsthatTAPAS-Row-Col,whichdirectlycon- 12-heads2. Both statements and tables are tok- catenatesalltablecellsandusesrowindexandcol- enizedintosequencesofsubwordswiththeStan- umnindexpositionembeddingtocapturethetable structure, signiﬁcantly outperforms Table-BERT 2https://github.com/huggingface/ pytorch-pretrained-BERT onthecomplextestsetwiththesamepre-training, 2370Test Model Pre-train Columns All Simple Complex BERTclassiﬁerw/oTable BooksCorpus+WikipediaText N/A 50.5 51.0 50.1 Table-BERT BooksCorpus+WikipediaText subset 65.1 79.1 58.2 LPA N/A N/A 65.3 78.7 58.5 LogicalFactChecker BooksCorpus+WikipediaText all 71.7 85.4 65.1 GNN-TabFact BooksCorpus+WikipediaText subset 72.2 86.4 65.4 TAPAS-Row-Col BooksCorpus+WikipediaText all 60.5 63.8 57.9 TAPAS-Row-Col BooksCorpus+WikipediaText subset 68.3 79.5 62.9 TAPAS-Row-Col WikipediaTables all 73.4* 86.6 67.0* TAPAS-Row-Col WikipediaTables subset 73.9* 87.7* 67.2* TAPAS-Row-Col-Rank WikipediaTables all 74.5* 87.0 68.5* TAPAS-Row-Col-Rank WikipediaTables subset 74.8* 88.1* 68.5* TAPAS-Row-Col-Rank WikipediaTables+SQA all 76.0* 89.0* 69.8* TAPAS-Row-Col-Rank WikipediaTables+SQA subset 74.6* 88.9* 67.7* Table 2: Percent accuracy of different models for table-based fact veriﬁcation on the TabFact test set. WerunTAPAS-Row-Colmodelonbothfull modelnumericalcomparisonrelationsamongcells table and subset of the columns. Table 2 shows inthesamecolumn. GNN-TabFactlearnsarepre- that,whenTAPAS-Row-Colisﬁne-tunedfromthe sentationforeachtablecellbyiterativelypassing originalBERTmodel,shrinkingthetableonlyto theirnumericalrelationssuchasgreaterorlessin the related columns signiﬁcantly improves its ac- a graph neural network. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Results of ﬁne-tuning Tapas-Row-Col-Rank test set in a similar way by ﬁnding words end- modelonstatementswithdifferentcomplexity. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 201> 

<Paper ID = 202>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Afterthepre-processingstep,theagreement effectivecommunication. IntheNLGﬁeld,Clarity between subjects increased, see Table 1 for the implies that text is easily understood (Belz and post-processingAlphavaluesforeachoftheBayes Kow,2009;vanderLeeetal.,2017)andthatthe Nets. Alphavaluesbetween.21to.40indicatefair readerisfamiliarwithbasicinformationintroduced agreementandvaluesbetween.41to.60indicate in the text (Lampouras and Androutsopoulos, moderate agreement (Hallgren, 2012). </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  HighestabsoluteSpearmancorrelation explanations and their ratings. Table 4 gives betweenautomaticevaluationmetricsandhuman some extracts from the dataset along with the ratingsforInformativeness,wheretheboldfont automaticmetricsandthehumanevaluationscores representsthehighestcorrelationcoefﬁcientobtained byanautomaticevaluationmetric of Informativeness and Clarity. Based on these human scores, the extracts are divided into: Metric Diagram1 Diagram2 Diagram3 AllDiagrams good explanations (high scores for both), bad BLEU-1 0.25 0.09 0.34 0.24* explanations (low scores for both) and mixed BLEU-2 0.24 0.15 0.41* 0.22 BLEU-3 0.01 0.10 0.31 0.14 explanations(mixedscores). Itwouldthus high,andInformativenessisverylow. Explanation benecessarytolookmoredeeplyintothelinguistic 8 in Table 4 is the only example of this in phenomena that may indicate the quality of our corpus. It is thus difﬁcult to make any explanations. </Extractive Summary>  </Table ID = 4>  </Paper ID = 202> 

<Paper ID = 203>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Post-attack accuracy scores (below chance forcommonn-gramfeaturesandlinearmodels. </Abstractive Summary>  <Extractive Summary> =  experimentalconﬁguration. Apiecewaschosenif itsatisﬁedthesecriteria: i)containschangesforall 5.2 Baselines threeattacks,ii)consistsofatleast15words(ex- The results for all attacks are shown in Table 3. cludingemojisandtags),andiii)doesnotcontain Notethattheseareperformancesforf;therefore, obvious profanity.14 All 60 document pieces of when no attacks are applied (none), the perfor- thethreemodelswereshufﬂed,andthe20original mance for both substitute corpora stays the same versions were appended at the end (so that ‘cor- (asthoseonlyinﬂuencetheattacks). </Extractive Summary>  </Table ID = 3>  </Paper ID = 203> 

<Paper ID = 204>  </Paper ID = 204> 

<Paper ID = 205>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  to4),andPOScountsfromthetweettobeassessed andfeedsthemtoaRandomForest(RF)classiﬁer. We observe from Table 1 that PHASE signiﬁ- C-LSTM (Sawhneyetal.,2018a): Adeepneural cantly(p < 0.005)outperformsallbaselines. We network having a CNN followed by an LSTM to note that contextual models outperform the non- extractshortandlongrangefeaturesinatweet. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: * shows signiﬁcant improvements compared 0.81 ∗ ∗ to C + HST + LSTM (p < 0.005). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 205> 

<Paper ID = 206>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  theboldlexicalunitorframenames),asshownin (2017) for FN 1.7. Table 2 shows the number of Table1. examplesineachsplit. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  UsingtheLUdeﬁnitions 4.3 Results alone on FN 1.7 also achieves quite good results. Table 3 compares our model with previous meth- Butcombiningbothdeﬁnitionstogetheryieldsbet- odsontheFN1.5dataset. Hermannetal.(2014), terresultsthaneitheronealone. </Extractive Summary>  </Table ID = 3>  </Paper ID = 206> 

<Paper ID = 207>  </Paper ID = 207> 

<Paper ID = 208>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Example of SNOMED-to-ICD-10 mappings. </Abstractive Summary>  <Extractive Summary> =  Wetrainandeval- anddistributionalsimilarityoftextualoccurrences uate our encoder on different categorizations of in large-scale free-text, as well as combinations biomedical names. For instance, Table 1 shows thereof(Kartsaklisetal.,2018;Phanetal.,2019). how concepts from the SNOMED-CT ontology captureliteralsynonymy,whiletheseconceptscan Whileknowledgegraphembeddingsofbiomed- also be grouped into the ICD-10 coding system icalconceptscanencodeavarietyofsemanticrela- whichreﬂectsmoregeneralsemanticrelatedness. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Synonym retrieval and concept mapping scores for the SNOMED-CT encoders. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Synonym retrieval and concept mapping scores for the MedMentions encoders. </Abstractive Summary>  <Extractive Summary> =  ness. MedMentions Table 5 shows the performance The correlations in Table 8 conﬁrm the robust- ofthedifferentencodersfortheMedMentionsdata. ness of our conceptually grounded biomedical Table 7 gives an example of how, similar to the name representations. </Extractive Summary>  </Table ID = 5>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  performed on a batch size of 64, using a learn- ing rate of 0.001 and a dropout rate of 0.5. Input ICD-10 & SNOMED-CT Table 3 and 4 show strings are ﬁrst tokenized using the Pattern tok- theconceptmappingandsynonymretrievalperfor- enizer(SmedtandDaelemans,2012)andthenlow- manceofthedifferentencodersfortheICD-10and ercased. We use a triplet margin of 0.1 for the SNOMED-CTdata. </Extractive Summary>  </Table ID = 3>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  semanticswhileretainingtherelevantlexicalinfor- mation. Table 6 gives an example of the impact 5.5 Semanticrelatednessbenchmarks ofourconceptual groundingconstraintsforICD- Wealsoevaluateournameencodersontwobiomed- 10 test data: the model is able to encode domain- icalbenchmarksofsemanticsimilarity, whichal- speciﬁcsemanticsbeyondword-levelanalogiesfor low to compare cosine similarity between name the semantically related names of the test men- embeddingswithhumanjudgmentsofrelatedness. tion pain provoked by breathing. </Extractive Summary>  </Table ID = 6>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ness. MedMentions Table 5 shows the performance The correlations in Table 8 conﬁrm the robust- ofthedifferentencodersfortheMedMentionsdata. ness of our conceptually grounded biomedical Table 7 gives an example of how, similar to the name representations. </Extractive Summary>  </Table ID = 8>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  MedMentions Table 5 shows the performance The correlations in Table 8 conﬁrm the robust- ofthedifferentencodersfortheMedMentionsdata. ness of our conceptually grounded biomedical Table 7 gives an example of how, similar to the name representations. While the correlations for disorder data, our CCA+DAN encoder is able to theBNEmodelsbarelyimproveoverthoseofthe encode speciﬁc semantics that the BNE model is fastTextembeddings,ourCCA+DANencoderim- lacking: theconceptualgroundingconstraintshave provessubstantiallyoverall3benchmarks,regard- 2447MayoSRS UMNSRS UMNSRS Acknowledgments (rel) (rel) (sim) fastText 0.443 0.473 0.479 Wewouldliketothanktheanonymousreviewers CCA+DAN,ICD-10 0.666 0.556 0.561 CCA+DAN,SNOMED-CT 0.648 0.537 0.540 for their feedback. </Extractive Summary>  </Table ID = 7>  </Paper ID = 208> 

<Paper ID = 209>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Performance of various models on languages (cid:88)(cid:88) ro:Romanian,fr:French,pl:Polish,es:Spanishon1000and Sk = NPMI(wi,wj) (11) 7000documents. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: ‘Genuine’ Topic model clusters learnt from covarianceΣ toitssamplecovariance,removing 0 thedocumentsvsclusterswith≥0.8GMMoverlap. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 209> 

<Paper ID = 210>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Example of strategic paraphrasing: red indicates the important words, which were used as negative constraintsintheparaphrasing;blueindicateschangedwordsintheparagraph. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: MC error prediction across datasets, embed- dings,andperturbationtypes. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  </Paper ID = 210> 

<Paper ID = 211>  <Table ID = 4>  <Abstractive Summary> =  Table 4: An example showing a reliable news article using Bag-of-Words features and show the word from the “Daily Mail’ site which has a “Low” factual features with the highest weights for each class reportingrateonMBFC.Despitecomingfromasource in Table 3.8 The features in the table show clear with low reliability score, the shown article is reliable patterns: thetop-featuresforthereliable(positive) andverysimilartothecontentonsiteswithhighrelia- class are either stop words (e.g., ‘at’, ‘the’, etc.) bilityscores(suchas“BBC”and“TheWeekUK”)on or words presumably carrying neutral semantics thesamedate. </Abstractive Summary>  <Extractive Summary> =  Internet search engines have pro- inagivensite. Whiletheseweakordistantlabels prietary news ranking and veriﬁcation processes, are not always accurate (one example is shown whichmeansthatevenwhenusingtheoriginalti- in Table 4) , they provide an easy way to create tleandsourceofagivenarticle,thesearchresults large-scaledatasets. InTable2,wehighlightthree 8WealsocalculatedthePMIbetweenthelabelandword recent large-scale unreliable news datasets along featuresassuggestedbyGururanganetal.(2018)andfound withtheirdatacollectionprocedure. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  To achieve this, we train a Logistic Re- inWashingtonDConWednesday... gression(LR)modelonthetitlesofFakeNewsNet Table 4: An example showing a reliable news article using Bag-of-Words features and show the word from the “Daily Mail’ site which has a “Low” factual features with the highest weights for each class reportingrateonMBFC.Despitecomingfromasource in Table 3.8 The features in the table show clear with low reliability score, the shown article is reliable patterns: thetop-featuresforthereliable(positive) andverysimilartothecontentonsiteswithhighrelia- class are either stop words (e.g., ‘at’, ‘the’, etc.) bilityscores(suchas“BBC”and“TheWeekUK”)on or words presumably carrying neutral semantics thesamedate. (e.g. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Accuracy on validation sets with different split strategies. </Abstractive Summary>  <Extractive Summary> =  (2015);andtheCORALdistanceiscalculatedby D = 1 (cid:107)C −C (cid:107)2,wheredisthefeature PerformanceVarianceandSiteSimilarityAnal- CORAL 4d2 s t F dimension, C and C are the co-variance of two s t ysis: Anotherinterestingobservationfromthere- setsand(cid:107)·(cid:107)2 isthesquaredmatrixFrobeniusnorm. F sults in Table 6 is that while the performance on Tosimplifyouranalysis,weﬁlteroutallthesites everyrandomsplitisfairlystable,theperformance containinglessthan100examples(assumingthe ismuchmoreunstablewithrespecttosplittingby articlesfromthesesitesaretoofewtosigniﬁcantly source. Forexample,theRoBERTa(Title+Article) inﬂuence the model). There- outlet waiting for classiﬁcation may only have a fore, at least for the current models and datasets, verylimitednumberofarticles. Accordingly,while splitting by time does not signiﬁcantly inﬂuence in Table 6, we see a general improvement of the the current results. This ﬁnding may result from aggregation,itisalsoimportanttochecktheaggre- thatthefactthatthemodelisnotmemorizingthe gationeffectwhenthenumberofarticlesinagiven exacteventsinthetrainingset(thisisnotlimited siteissmall. </Extractive Summary>  </Table ID = 6>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Average similarity score between sites in the sentations;theMMDdistanceiscalculatedusing evaluationandtrainingsets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  </Paper ID = 211> 

<Paper ID = 212>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We use the learning rate of2e-5foralltasksexceptRTEandMRPCwhich Inimageclassiﬁcationexperiments,theimprove- use 4e-5. Table 5 compares the performance of ment gap between the annealing KD results and annealing KD and other baselines on dev set for the other baselines in CIFAR-100 experiments is small-BERTexperiments. Formoredetailsregard- largerthanCIFAR-10ones. </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Common Hyper-parameters for Distil- In this section, we include more detail of our ex- RoBERTaandBERT-SmallmodelsonGLUEtasks perimentalsettingsofsection4.2inthepaper. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Model speciﬁc Hyper-parameters for Distil- RoBERTaonGLUEtasks size, EP1= number of epochs in phase 1 (for the baselines, this is the number of training epochs), Hyper-parameter CoLA RTE MRPC STS-B SST-2 QNLI QQP MNLI WNLI EP2=numberofepochsinphase2,LR=learning LearningRate 2e-5 2e-5 2e-5 2e-5 2e-5 2e-5 2e-5 2e-5 2e-5 Phase1epochs 14 14 14 14 14 14 14 14 14 rate,MO=momentum,WD=weightdecay,τ max Phase2epochs 4 4 4 4 4 4 4 4 4 =maximumtemperature) τmax 7 7 7 7 7 7 7 7 7 Table 6: Hyper-parameters of CIFAR-10 and CIFAR- 100experiments Model Type Trainingmethod BS EP1 EP2 LR MO WD τmax Teacher(110) fromscratch 128 160 N/A 0.1 0.9 10−4 N/A TA(20) KD 128 160 N/A 0.1 0.9 10−4 N/A Student(8) fromscratch 128 160 N/A 0.1 0.9 10−4 N/A ResNet Student(8) KD 128 160 N/A 0.1 0.9 10−4 1 Student(8) TAKD 128 160 N/A 0.1 0.9 10−4 1 Student(8) AnnealingKD(ours) 128 160 160 0.1 0.9 10−4 10 Teacher(10) fromscratch 128 160 N/A 0.1 0.9 10−4 N/A TA(4) KD 128 160 N/A 0.1 0.9 10−4 N/A Student(2) fromscratch 128 160 N/A 0.1 0.9 10−4 N/A CNN Student(2) KD 128 160 N/A 0.1 0.9 10−4 1 Student(2) TAKD 128 160 N/A 0.1 0.9 10−4 1 Student(2) AnnealingKD(ours) 128 160 160 0.1 0.9 10−4 10 B BERTExperiments Intheseexperiments,RoBERTa-large(24layers) and DistilRoBERTa (6 layers) are used as the teacher and student models respectively. </Abstractive Summary>  <Extractive Summary> =  Wetrained thestudentmodelfor14epochsinphase1,and6 epochs in phase 2. Table 8 illustrates the details ofthehyper-parametersoftheexperiments. Also, Table11illustratesthehyper-parametervaluesof BERT-small experiments in detail. </Extractive Summary>  </Table ID = 8>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Hyper-parameters of CIFAR-10 and CIFAR- 100experiments Model Type Trainingmethod BS EP1 EP2 LR MO WD τmax Teacher(110) fromscratch 128 160 N/A 0.1 0.9 10−4 N/A TA(20) KD 128 160 N/A 0.1 0.9 10−4 N/A Student(8) fromscratch 128 160 N/A 0.1 0.9 10−4 N/A ResNet Student(8) KD 128 160 N/A 0.1 0.9 10−4 1 Student(8) TAKD 128 160 N/A 0.1 0.9 10−4 1 Student(8) AnnealingKD(ours) 128 160 160 0.1 0.9 10−4 10 Teacher(10) fromscratch 128 160 N/A 0.1 0.9 10−4 N/A TA(4) KD 128 160 N/A 0.1 0.9 10−4 N/A Student(2) fromscratch 128 160 N/A 0.1 0.9 10−4 N/A CNN Student(2) KD 128 160 N/A 0.1 0.9 10−4 1 Student(2) TAKD 128 160 N/A 0.1 0.9 10−4 1 Student(2) AnnealingKD(ours) 128 160 160 0.1 0.9 10−4 10 B BERTExperiments Intheseexperiments,RoBERTa-large(24layers) and DistilRoBERTa (6 layers) are used as the teacher and student models respectively. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  </Paper ID = 212> 

<Paper ID = 213>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Weshowanexamplecontrastingthetwo (2019). metrics in Table 2. Further, our method achieves betterorcomparableresultsacrossalldatasetscom- Language model ﬁne-tuning. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Ablation Results on CNN-DM and Reddit- method inspired by PageRank. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Size of train, validation and test splits of the right setting. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  It again highlights the need to datasetsused considerwhatoneexpectsfromthesummarisation task. B AnalysisofExamples Table 5 shows some example summaries from the CNN-Dailymail validation set in comparison to extractive candidate summaries obtained for the correponding documents using the baseline Lead-3 approach, our Interpolated PMI based approach and the PacSum approach (Zheng and Lapata, 2019) that uses sentence similarity to obtainstate-of-the-artRougeresultsonthedataset. These are shown to highlight the difference between using PMI and similarity for sentence selection. </Extractive Summary>  </Table ID = 5>  </Paper ID = 213> 

<Paper ID = 214>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  contextinformationacrossthemini-batch. 4.3 ExperimentalResultsandAnalysis Effect of Decoding Batch-size In the previous section,wediscussedthetranslationperformance Translation Performance Table 1 summarizes givenadocument,whichmeansthatthesentences themodelperformanceonseveraltestsets. SeeTa- in the entire document are in a mini-batch. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Number of sentences and documents in from(Morishitaetal.,2020). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 214> 

<Paper ID = 215>  </Paper ID = 215> 

<Paper ID = 216>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  MoreNEdetailsare Resultsforthequality-latencycurvescreatedbythe foundinAppendixB. useofconstraineddecodingandmask-k(Section3) Interleaving Rate Table 1 also shows us the areshowninFigure2. Unconstrainedsettingsare overallresultsfordifferentinterleavingrates. </Extractive Summary>  </Table ID = 1>  </Paper ID = 216> 

<Paper ID = 217>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The classic entity ranking problem is, Prediction given a text query and a ﬁnite set of entities, to rank them according to their relevance to the TheoriginalMovieLensTagPredictiontaskisto query. Recallthat DOCENT modelsarenaturally producemovie-tagscoresforasetofmoviesand designed to make such relevance predictions via a canonical vocabulary of tags (see examples in P(Entity|Sentence)—withoutanyﬁne-tuning, Table 1), based on a collection of crowdsourced if necessary. We therefore leverage the Reddit (movie, tag, user) votes, as well as (user, movie) MovieSuggestionsDataset(detailedinSection4.3) star ratings. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Any leveragingmoviereviews,hereafterreferredtoas similar deeply disturbing yet realistic movies you can rec- theclosed-vocabularytagpredictiontask7. Thisis ommend?”, see Table 2 for more examples). An- a supervised setup where models are ﬁne-tuneed otherchallengeisanexplicitrecommendationin- 6Conversely, a very large E may require an optimized tent present in many of the queries (i.e., “Movies implementationofsoftmaxtomaintainscalability. DOCENTapproachesclose-vocabularyAUCaftertrainingwith only10-50%ofthevocabulary(showingallbaselinesthatwereavailabletousinthissetting). truth (for completeness, see also the qualitative et al., 2016; Chang et al., 2020; Kobayashi et al., results in Table 2). DOCENT models outperform 2016; He et al., 2013; Gupta et al., 2017), with theLucenebaselineonallmetrics,with DOCENT- Lingetal.(2020)alreadydiscussedinSection2.1. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Forcom- nario,evaluationisdoneonaholdoutsetofmovies pleteness, we also compare to BOS-BERT∗20, (withasmallerdevelopmentsetusedforhyperpa- BOS-GLOVE and BOS-SENTENCEBERT,neural rametertuning;seeTable3fordetails). baselinesdeﬁnedinSec.5.3,whosequery-movie Results for ranking (MAP) and binary classiﬁ- relevance score is given by the maximum cosine cation (AUC) metrics are shown in Table 4. Col- similarityamongthemovie’sreviewsentences21. </Extractive Summary>  </Table ID = 4>  </Paper ID = 217> 

<Paper ID = 218>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Eight label taxonomy deﬁned by De Waard pose, method, ﬁnding/contribution, other} while andMaat(2012). </Abstractive Summary>  <Extractive Summary> =  Formally,aparagraphcanbe calpapersproposedbyDeWaardandMaat(2012). representedasanorderedcollectionofsequences Thetaxonomycontainseighttypesincludinggoal, fact,result,hypothesis,method,problem,implica- 1https://github.com/jacklxc/ ScientificDiscourseTagging tion and none as Table 1 shows. Most recently, 2551Coxetal.(2017)usedthesameschema(DeWaard andMaat,2012)byexploringavarietyofmethods forbalancingclassesbeforeapplyingclassiﬁcation algorithms. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  5 ExperimentalResults 4.2 BaselineModels 5.1 SupervisedLearningResults PubMed-RCT Dataset. We compare our dis- Table 2 reports the test F1 score of our scientiﬁc coursetaggeragainsttwostrongbaselinesonthe discoursetaggeranditsvariationsagainstbaseline PubMed 20k RCT dataset: (1) a hierarchical se- models on PubMed 20k RCT dataset and SciDT quential labeling network (HSLN) proposed by dataset. Ourbestscientiﬁcdiscoursetaggeroutper- Jin and Szolovits (2018) and (2) the state-of-the- formsthestate-of-the-artmodel(Srivastavaetal., 25552019)onPubMed20kRCTdatasetbymorethan result 0.000.000.010.120.020.000.000.85 2 % absolute F1 score. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ThenasAchakulvisutetal.(2019) suggested, we pre-train the scientiﬁc discourse 19 label set. As a result, as Table 3 shows, our tagger on PubMed 20k RCT (Dernoncourt and zero-shot prediction results are even higher than Lee,2017)andﬁne-tuneitontheclaim-extraction thebaselinefromHuangetal.(2020)whichwasdi- dataset. WereplacethelastCRFlayerwithanew rectlytrainedontheCODA-19dataset. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  achieves0.94F1scoregiventhegroundtruthBIO sequences. Table 5 compares our feature-based 8 Conclusions CRFmodelperformancewithBurnsetal.(2017) We also compare our feature-based CRF model Wedevelopastate-of-the-artmodelforscientiﬁc performancestrainedwithorwithoutscientiﬁcdis- discoursetagginganddemonstrateitsstrongperfor- coursetagsfromSciDTdataset. Ourfeature-based manceonPubMed-RCTdataset(Dernoncourtand CRFmodelwithoutscientiﬁcdiscoursetagsasin- Lee,2017)andSciDTdataset(Burnsetal.,2016; putsdoesnotoutperformBurnsetal.(2017). </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Optimal hyper-parameters of scientiﬁc dis- wordsineachclauseorsentence. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  </Paper ID = 218> 

<Paper ID = 219>  </Paper ID = 219> 

<Paper ID = 220>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Weusetwometricstoevaluatetheabstrac- 6.1% novel trigrams and copies entire sentences tivenessofthemodel: 51.7%ofthetime. ThisshowsthatStructSumon average generates 14.7% more novel n-grams in Copy Length: Table 2 shows a comparison comparisontothepointer-generatorbaseline. of the average length (Copy Len) of contigu- ous copied sequences from the source document 4.3 Coverage (greaterthanlength3). </Extractive Summary>  </Table ID = 2>  </Paper ID = 220> 

<Paper ID = 221>  <Table ID = 1>  <Abstractive Summary> =  Table 1: SDP scores for each model and approach. </Abstractive Summary>  <Extractive Summary> =  Graph-UDify: We trained it with UDify’s pre- trained language model3 instead of multilingual 4.2 ResultsandDiscussion BERT. Since UDify is pre-trained on many lan- guagesinUD,weexpectthatitcapturemorecross- Table 1 shows the SDP scores for each model in lingualityonsurfacethanBERT. each approach. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Unlabeled and labeled scores trained on En- we obtained better results with the Project-then- glishPSDwithprojectedPUD Czech. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  whicharealsoclosetotheupper-boundofrelation label prediction. Thus, our best model achieved Cross-linguality on Surface: Table 5 shows highperformance,whichisclosetotheoreticalup- SDPscorestrainedonEnglishPSDwithprojected perbounds. Thisindicatesthatbi-lexicalrelations PUD Czech. Our MTL setting is to Figure5showsexpectedvalidationperformances. shareonlyBERTlayers,buthigherlayersincluding Table 4 and Table 5 show performances on the scalar-mixlayersaredistinct. WeusedUDify“as dev-set. </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Our MTL setting is to Figure5showsexpectedvalidationperformances. shareonlyBERTlayers,buthigherlayersincluding Table 4 and Table 5 show performances on the scalar-mixlayersaredistinct. WeusedUDify“as dev-set. </Extractive Summary>  </Table ID = 4>  </Paper ID = 221> 

<Paper ID = 222>  <Table ID = 1>  <Abstractive Summary> =  Table 1: OIE dataset metrics. </Abstractive Summary>  <Extractive Summary> =  TheextractedtupleinournewLSOIEdataset is(physicians,provide,drugs,inAsiancountries). tations (see Table 1). We benchmark LSOIE with that has been crowdsourced speciﬁcally for OIE, several models, providing baseline results for fu- annotating 1,282 sentences. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Example sentences with example extractions. </Abstractive Summary>  <Extractive Summary> =  Exampleextractionsareshown train,andtestourmodels. Wetrainrnnoieand in Table 2. We provide the distribution of argu- srl bert oie2016 on OIE2016 and ls oie ment, predicate, and null tag labels in Figure 2. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Figure3showsprecisionandrecallcurvesonthe 6 Conclusion LSOIE-wikitestset,accompaniedbythels oie model’s estimated conﬁdence. Table 3 shows F Inthispaper,weintroducedtheLSOIEdatasetasa 1 andAUCscoresforthebenchmarkmodelsonthe resourceforsupervisedOIE.Wehavealgorithmi- LSOIE-wikiandLSOIE-scitestsets. callyre-purposedtheQA-SRL BANK2.0intoanew The OIE modeling task is difﬁcult. </Extractive Summary>  </Table ID = 3>  </Paper ID = 222> 

<Paper ID = 223>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  istheaverageembeddingofallwordsintheinput Table3comparestheoptionsgeneratedbydif- prompt. ferent methods while Table 4 compares the text The second type of methods discovers the K generatedusingdifferentoptiongeneratorsandtext topics from the input prompt. We cluster non- generators. We also sample K non-stop makesthegeneratedcontinuationbecomeacopy words from the prompt and call it Sample-local. of the input prompt in Table 4. We will quantita- Similartoequation1,werepresenteachtopicus- tively evaluate the generated continuations using ing M words and compute the weighted average different option generators in the appendix. </Extractive Summary>  </Table ID = 4>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  testKmeans-localinourhumanevaluation. 3.3 ConditionalTextGeneratorEvaluation 3.2.4 Results Todemonstrateourtextgenerator’seffectiveness, In Table 1, we show that local methods generate we use our option generator to prepare the topic theoptionsmorerelevanttotheinputpromptthan embeddings and randomly select n topics as our theglobalmethodsduetosigniﬁcantlyhigherSim conditionstosimulatetheuser’schoice,wheren andSimShort. Ourmethodperformsbettercom- is a random number from 1 to K. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The improvement on Sim Short is larger 3.3.1 AutomaticEvaluationMetrics thanthatonSimbecauseourmethodcouldsuggest We match the union of M ×K top words in the therelatedtopicsthatarenotexplicitlymentioned chosentopicswiththewordsinthegeneratedcon- intheshortprompt(e.g.,U.S.inFigure1). tinuationsandcountthenumberoftokensthatare The human evaluation results are presented in matched exactly (token), the number of matched Table 2. Our method wins in terms of generat- wordtypes(word),andthenumberoftopicsthat ingrelevanttopicsthatpromotethenarrative. </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Comparison of conditional text generators. </Abstractive Summary>  <Extractive Summary> =  ﬁcity(Seeetal.,2019b)ofthetext,orthelengthof thegeneratedsentence(Tuetal.,2019). However, 3.3.4 Results theoptionscannotprovideﬁne-grainedcontrolon Table 5 indicates that our model outperforms topicaldirectionsofthegeneratedcontents. PPLM in all metrics except in Dist-1 and Dist-2. </Extractive Summary>  </Table ID = 5>  </Paper ID = 223> 

<Paper ID = 224>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Statistics of the datasets used for our experi- todefeatthedevil.Don'tbehappytoseethebeautifulfaces.] ment. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  representsverypoorperformance,and5represents verygoodperformance. Here,contentmeanshow Results Wereportourmodel’sperformancecom- well the summary can convey the original input pared with the baselines in terms of F1 scores of document’s meaning, and readability represents R-1, R-2, and R-L in Table 2. According to Ta- the grammatical correction and the overall sum- ble2,ourabstractivesummarizationmodeloutper- marysentencecoherence. </Extractive Summary>  </Table ID = 2>  </Paper ID = 224> 

<Paper ID = 225>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  to make meaningful judgments about the overall We analyze the programs on two levels with tone of presentation. Table 1 gives an overview the help of paid research assistants paid $15 per ofnumberofUncivil,CivilandRandomspeaker hour. All of them are undergraduate students in turnsaroundwhichlongersnippetswereselected non-technical majors at the University of Penn- forﬁne-grainedannotationofincivility. </Extractive Summary>  </Table ID = 1>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Sub-error trigger Words. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 225> 

<Paper ID = 226>  </Paper ID = 226> 

<Paper ID = 227>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Per topic labeled users in test set along with Table2: Testsetinformation: %skippedtweets,inter- thenumberofusersforwhichwereabletoscrapetheir annotatoragreement,%pro,and%anti. </Abstractive Summary>  <Extractive Summary> =  accounts were deleted, suspended, or made pro- SincefastTextwasdesignedforsentence-levelclas- tected. Table 3 lists the number of labeled users siﬁcation,weoptedtoperformtweet-levelclassiﬁ- and the subset of them for whom we were able cation. Duringtraining,weassignedthelabelofa to scrape their timelines. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  was not able to assign any test user to a cluster, mostlybecausethenumberoftweetspertestuser • SVM yieldedtheworseresultsoverall,de- TEXT weretoofew. Hence,weomittedtheunsupervised spitetheinclusionofallthefeaturesinthetweets, method from Table 4. Table 5 shows the results suchasretweetedaccounts,hashtags,words,etc. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Hence,weomittedtheunsupervised spitetheinclusionofallthefeaturesinthetweets, method from Table 4. Table 5 shows the results suchasretweetedaccounts,hashtags,words,etc. onSetB,whereweexpandedthetest,training,or Itseemsthattheinclusionofmorefeatures(com- either or both training and test user tweets using paredtoSVM )confusedtheclassiﬁerleading RT timelinetweets. </Extractive Summary>  </Table ID = 5>  </Paper ID = 227> 

<Paper ID = 228>  <Table ID = 1>  <Abstractive Summary> =  Table 1: SRL results on the CoNLL-2005 WSJ test tionsforpredicatesandtheirarguments,andalso set averaged over 5 independent runs. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: SRL results on the CoNLL-2012 test set averaged over 5 independent runs. </Abstractive Summary>  <Extractive Summary> =  WereferthereadertoAppendixA.2for WepresentourmainresultsforSRLinTable1 additionaltrainingdetails. and Table 2, NER in Table 3, and RE in Table 4. Alltheseresultsreportaverageperformanceover 4 ResultsandAnalysis ﬁve runs with different random seeds. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: NER results on the OntoNotes-5.0 test set off-the-shelf NLP toolkits.6 In previous work, it averaged over 5 independent runs. </Abstractive Summary>  <Extractive Summary> =  WereferthereadertoAppendixA.2for WepresentourmainresultsforSRLinTable1 additionaltrainingdetails. and Table 2, NER in Table 3, and RE in Table 4. Alltheseresultsreportaverageperformanceover 4 ResultsandAnalysis ﬁve runs with different random seeds. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  WereferthereadertoAppendixA.2for WepresentourmainresultsforSRLinTable1 additionaltrainingdetails. and Table 2, NER in Table 3, and RE in Table 4. Alltheseresultsreportaverageperformanceover 4 ResultsandAnalysis ﬁve runs with different random seeds. </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: SRL results with different parses on the JointFusion 90.59 91.35 90.97 CoNLL-2012testset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  Table 7: NER results with different parses on the cantpositivecorrelationbetweenparsequalityand OntoNotes-5.0 test set averaged over 5 independent relativemodelperformancewhentrainingandtest- runs. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  Table 8: SRL results from using different pre-trained andUAS,suggestingthatasUASofStanzaparseincreases, models on the CoNLL-2005 WSJ test set averaged themodelmakeslesserrors. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 9>  </Paper ID = 228> 

<Paper ID = 229>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Itcontains3,731movies;foreachmovie oneofﬁverandomlysamplednegativesummaries: wearegivenalargesetofreviewswrittenbypro- fessionalcriticsandusersandagold-standardcon- (cid:88)5 L = max(0,1−d(cid:48)z+d(cid:48)n ) (19) sensus summary written by an editor (see an ex- fuse i i=1 ampleinFigure1). Wereportthedatasetstatistics in Table 1. Following previous work (Wang and The ﬁnal objective is then the sum of both loss Ling, 2016), we used a generic label for movie functions: 2http://www.ccs.neu.edu/home/luwang/ Labstract = Lgenerate+Lfuse (20) publications.html 2666Train Dev Test (i)BERTCENT+PTGEN,thesamemodelas(h)but #movies 2,458 536 737 enhanced with a copy mechanism (Vinyals et al., #reviews/movie 100.0 98.0 100.3 2015). </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  2014)atarateof0.5. Themodelwastrainedusing Our results are presented in Table 2. Among theAdamoptimizer(KingmaandBa,2015)with one-passsystems,theextractivemodelBERTCENT defaultparametersandl constraint(Hintonetal., 2 performsthebest;despitebeingunsupervisedand 2012)of2. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  One-pass terestingly,BERTCENTperformsbetterthanBERT- methodsinclude(a) LEXRANK (ErkanandRadev, CENT+PTGENintermsofMETEORandROUGE- SU4, while the latter performs better in terms of 2004),aPageRank-likesummarizationalgorithm whichgeneratesasummarybyselectingthenmost ROUGE-1/2/L.OurCA-basedmodel CONDASUM outperforms all other models across all metrics, salientunits,untilthelengthofthetargetsummary showing that exploiting information about all re- isreached;(b)OPINOSIS(Ganesanetal.,2010),a viewshelpsinimprovingperformance. graph-basedabstractivesummarizerthatgenerates We present in Table 3 various ablation studies, concisesummariesofhighlyredundantopinions; which assess the contribution of different model (c)SUMMARUNNER(Nallapatietal.,2017),asu- components. Resultsconﬁrmthatourmulti-source pervisedneuralextractivemodelwhereeachreview fusionmethodandthefusionlossimproveperfor- isclassiﬁedastowhetheritshouldbepartofthe summary or not; and (d) BERTCENT, a centroid- mance. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Proportion of summaries which mention a horrorﬁlmwithanenormouscastofeagerstars. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 229> 

<Paper ID = 230>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Top: accuracy for inﬂection and G2P and train 50 models with different random seeds for character-levelperplexityforlanguagemodeling. </Abstractive Summary>  <Extractive Summary> =  26765.2 Architectures Inﬂ Inﬂ G2P G2P LM LM LSTM T LSTM T LSTM T Toisolatetheeffectsoftaskandmodelarchitecture, 0.95 0.94 0.64 0.62 3.37 2.62 50 50 50 50 10 10 wetraindifferentarchitecturesforeachtask. All test set performances are shown in Table 1. We Table 1: Top: accuracy for inﬂection and G2P and train 50 models with different random seeds for character-levelperplexityforlanguagemodeling. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: The most similar character pairs in descending order (top) and the most dissimilar character pairs in ascendingorder(bottom)forhumansynesthetes. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 230> 

<Paper ID = 231>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  4.1 ReferenceGoldvs. ClosestGold Table 2 shows the results of evaluating each sys- tem hypothesis against reference golds and clos- Figure 2: Performance by hypothesis rank (F-score) est golds for two datasets – BEA (English) and againstReferenceGold(RG)vs. ClosestGolds(CGs) RULEC (Russian). </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Asweshowedinthepre- We have shown above that the quality of the vious section, this is not the case and using RGs hypotheses does not degrade with rank, and in severelyunderestimatessystemperformance,and, some cases hypotheses at lower ranks even re- asaconsequence,post-editingeffort. sultinhigherF-scorethanthetop-rankedhypoth- esis, when scored against CG, while the evalua- We now use CGs to estimate hypotheses qual- tion against RGs is strongly biased against lower- ity in terms of post-editing effort in Table 3. We ranked hypotheses. (The number that it matches its corresponding CG. This qual- of proposed, gold, and correct edits also appears ityestimationapproachthatconsidersthenumber in Tables 2 and 7 but is shown here in Table 3 for ofeditsrequiredto“ﬁx”thehypothesisisusedin convenience). The post-editing effort is shown in Machine Translation (Snover et al., 2006), where the last column, estimated as the number of ed- the quality of a system output is measured as the its required to make the hypothesis output ﬂuent minimumnumberofeditsneededtotransformthe and grammatical, i.e. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  respectivelyareofthistype. Thisissimilartothe results for the English datasets in Table 5, how- ever in the most challenging categories (lexical and“other”), whichbothcomprisewordchanges, the situation is more severe: the top-ranked hy- pothesis proposes 0 changes. Overall, the under- correction phenomenon is more pronounced for theRussianlanguage. </Extractive Summary>  </Table ID = 5>  </Paper ID = 231> 

<Paper ID = 232>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Baselines, Hierarchical Transformers and text-pair BERT + Transformers models’ performance in the differentconditions(seesection3). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 232> 

<Paper ID = 233>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Train- ing quality, lowering delay) or wait for too many ingiscompletedwithin20hoursonasingleV100 words(increasingquality,increasingdelay). Both GPU.5 scenarios resultingfrom thisnoisy alignment are Table 3 presents the Transformer results on not catastrophic as it still depends on the inter- IWSLTandWMT.Firstweseethatourtransformer preter’sabilitytoguesstranslationoutputswithout resultsarecompetitiveorbetterthantheLSTMon appropriateinputcontext. IWSLT dataset (compare with Table 1). </Extractive Summary>  </Table ID = 3>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Both GPU.5 scenarios resultingfrom thisnoisy alignment are Table 3 presents the Transformer results on not catastrophic as it still depends on the inter- IWSLTandWMT.Firstweseethatourtransformer preter’sabilitytoguesstranslationoutputswithout resultsarecompetitiveorbetterthantheLSTMon appropriateinputcontext. IWSLT dataset (compare with Table 1). These results are similar to the Ma et al. </Extractive Summary>  </Table ID = 1>  </Paper ID = 233> 

<Paper ID = 234>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  FortheartiﬁcialMNLI-12dataset,all line uses the same BERT architecture as our ap- thetrueevidencesareguaranteedtobeincluded. proachdescribedinSection2.2,butistrainedwith Table 1 shows that our method achieves sig- only the relevancy loss (Eq 5) and therefore only niﬁcant improvements on both datasets. On considertherelevancywhenselectingevidence. Amoreinterestingexampleisgiven the settings and hyperparameters used in (Harel atthebottomwherethetop-50onlycoversonesup- et al., 2019) for the DRN model. Table 1 shows porting passage. The BERT baseline selects two theperformance. </Extractive Summary>  </Table ID = 1>  </Paper ID = 234> 

<Paper ID = 235>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Average number of named-entities and the is complementary to such existing approaches as precsscores(%)inthegroundtruthsummary. </Abstractive Summary>  <Extractive Summary> =  et al., 2016) and XSUM (Narayan et al., 2018). Theauthorsin(Krys´cin´skietal.,2019)proposed Table 1 shows that among the three datasets, the to train a neural network model to classify if a summaryisfactuallyconsistentwithagivensource Newsroom CNNDM XSUM document,similartoanaturallanguageinference train val test train val test train val test avg.N(t) 2.08 2.10 2.09 4.36 5.09 4.87 2.08 2.06 2.08 task. In the dialogue generation setting, authors avg.N(t∩s) 1.88 1.90 1.90 4.21 4.92 4.70 1.64 1.64 1.64 in(Lietal.,2019)proposedusingunlikelihoodto precs(%) 90.6 90.6 90.5 96.5 96.7 96.6 79.0 79.5 79.3 surpresslogicallyinconsistentresponses. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  If a ﬁltered dataset does not contain hallucination of named-entityinthesummaryconsistsofmultiple entities(prec = 1)inthegroundtruthsummary. words, we consider it a match as long as any n- s The dataset size before and after the ﬁltering is gram of the named-entity can be found in the shown in Table 2. About a third of examples are source document. For each dataset, we train During training, we simply add the classiﬁcation two separate models: using the training data loss for each token at the encoder to the original before and after entity-based data ﬁltering as sequence-to-sequenceloss. shown in Table 2. We evaluate both models More precisely, let {(cid:0)xi,yi(cid:1)}N be a dataset on the “clean” test set after entity-based data i=1 of N examples where xi = xi,...,xi are 1 ts(i) 3Our code is available at https://github.com/ the tokens of the ith source document and amazon-research/fact-check-summarization 2729Newsroom CNNDM XSUM train val test train val test train val test original 922,500(1.58) 100,968(1.60) 100,933(1.59) 287,112(3.90) 13,368(4.13) 11,490(3.92) 203,540(1.0) 11,301(1.0) 11,299(1.0) afterﬁltering 855,975(1.62) 93,678(1.64) 93,486(1.64) 286,791(3.77) 13,350(3.99) 11,483(3.77) 135,155(1.0) 7,639(1.0) 7,574(1.0) Table2: Numberofexamplesinthreedatasetstogetherwiththeaveragenumberofsentencesinthegroundtruth summary(inparentheses)beforeandafterentity-basedﬁltering. ﬁltered out (c.f. Table 2), which explains the more noticable impact on the ROUGE scores. Accuracyofentitylevelmetrics: Asourentity The results in Table 3 suggest that entity-level level metrics are based on automatic NER tools data ﬁltering is a simple yet effective approach and heuristics matching rules, errors in both to achieve higher entity-level factual consistency steps can lead to inaccuracy in the metrics. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Comparison of models trained using original data, with entity-based data ﬁltering, with an additional classiﬁcationtaskandwithJAENS.Scoresareallinpercentages,averagedover5runsandshownwithstandard deviations. </Abstractive Summary>  <Extractive Summary> =  Fairseqlibrary(Ottetal.,2019)toﬁne-tuneonthe Wethenassignthe(B)eginning-(I)nside-(O)utside 3summarizationdatasets.3 Theappendixcontains labels to each token of the source document to additionaldetailsofexperimentalsetup. denoteifthetokenisbeginning,insideoroutside In Table 3, we show the effect of the entity- of a summary-worthy named-entity, respectively. based data ﬁltering. Table 2), which explains the more noticable impact on the ROUGE scores. Accuracyofentitylevelmetrics: Asourentity The results in Table 3 suggest that entity-level level metrics are based on automatic NER tools data ﬁltering is a simple yet effective approach and heuristics matching rules, errors in both to achieve higher entity-level factual consistency steps can lead to inaccuracy in the metrics. aswellasgeneralsummarizationquality. </Extractive Summary>  </Table ID = 3>  </Paper ID = 235> 

<Paper ID = 236>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Action hallucination percentages at different levels of predictive uncertainty. </Abstractive Summary>  <Extractive Summary> =  modelishallucinated. Hallucinationprobabilities Actionpredictionsarebinnedaccordingtotheir are calculated by binning all object token predic- uncertainty values, and the results are shown in tionentropyandcountingthepercentageofhallu- Table 1. We can observe that action tokens with cinatedobjectsineachbin. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Pearson correlation coefﬁcients between hal- lucinationandepistemic/aleatoricuncertaintyinimage 0 captioning task. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Evaluation results for candidates with high, medium, and low average predictive uncertainty values for ToTTo validation set. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Average sentence length and total number of CHAIRi CHAIRi objects detected in the captions generated by BUTD (c)BUTD (d)Transformer model with varying uncertainty penalty weight λ. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  </Paper ID = 236> 

<Paper ID = 237>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  2.5 2.0 % usingthe6:2:2ratioovertheentiredataset. Table 1.5 2presentsthestatisticsaboutthesedataportions. 1.0 NotethatsimilartosomepriorEDwork(Nguyen 0.5 andGrishman,2015;Chenetal.,2015),ourFED 0.0 problemisformulatedasawordclassiﬁcationprob- 0 50 100 150 200 250 Sentence Length lemwheregivenawordinaninputsentence,the Figure 3: The distribution of the sentence lengths for modelsneedtopredicttheeventtypefortheword. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ﬁne-grainedeventtypesaswedo. Results: Table 3 shows the performance of the OurFEDtaskisalsorelatedtoﬁne-graineden- models on the test set of FedSemcor. From the tity typing that aims to classify entity mentions table, weseethatGCNhasthebestperformance intoaﬁne-grainedsetoftypes(Karnetal.,2017; amongthemodelswithword2vecwhileBERT- Shimaokaetal.,2016;LinandJi,2019). </Extractive Summary>  </Table ID = 3>  </Paper ID = 237> 

<Paper ID = 238>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The ForallexperimentsonSNIPS,weuseADAMwith smaller models here may not be competitive on alearningrateof0.0001andabatchsizeof64. GLUEbutareadequateforpracticaltaskssuchas 5 Results spokenLU.Wecomparewithtwostrongbaselines: • BERTBASE (Chenetal.,2019a)withintentand GLUE: Table 1 shows results on downstream IOBslottagspredictedusingthe[CLS]andthe GLUEtasksandmodelsizesforourproposedmod- ﬁrstWPtokensofeachwordrespectively,and els, BERT , and baselines. Our mod- BASE/LARGE • StackProp(Qinetal.,2019),whichusesaseries els consistently improve upon the identically pa- ofsmallerrecurrentandself-attentiveencoders. </Extractive Summary>  </Table ID = 1>  </Paper ID = 238> 

<Paper ID = 239>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  λadv istunedto0.8 accuracyandTPR/TNRGap. basedonthestandard(single-discriminator)adver- As shown in Table 1, even the Fixed Encoder sariallearningmethod,andthissettingisusedfor model leaks protected information, as a result of all other adversarial methods. When tuning λadv, implicitbiasesduringpre-training. </Extractive Summary>  </Table ID = 1>  </Paper ID = 239> 

<Paper ID = 240>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We found that automatic creation leads to wordembeddings: BERT (Devlinetal.,2018)and ﬂawsinthedataset;thus,wemanuallyanalyze600 counter-ﬁtting(Mrksˇic´etal.,2016). Weassignase- questionsfromthetestdevbalancedsplitofGQA manticscoretoeachalternativeanswerbytextural dataset, and identify six issues shown in Table 1. entailmentandintroducetheAASmetric. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  AAS LXMERTonGQAdatasetwiththisobjective. Table 3 shows the results of LXMERT trained 5.3 EvaluationofAAS with AAS compared with the baseline. Not sur- prisingly,theperformanceevaluatedontheorigi- To validate the correctness of AAS, we measure nalmethoddropsbecausethemodelhasahigher thecorrelationbetweenhumanjudgmentandAAS. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: The IoU scores between human annotations and garet Mitchell, Dhruv Batra, C Lawrence Zitnick, AASbasedonﬁveapproaches. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 240> 

<Paper ID = 241>  <Table ID = 1>  <Abstractive Summary> =  Table 1: F-1 scores acquired after training the aspect extractor on German side of parallel data and passing the validation sets of each data set through trained aspect extractors. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Pleasenotethat M30k 79.39 90.63 our goal in this experiment is not to achieve the IWSLT 77.80 88.34 state-of-the-artﬁne-grainedpart-of-speechtagging WMT 82.13 91.42 resultsasouraspectextractorsreceivetheirinput TIGER 84.64 92.64 fromBERTanddonotdirectlyaccessthetagged input sentences. Table 2 contains the results of Table2: F-1scoresofﬁne-grainedpart-of-speechpre- comparisonbetweenpredictionsofdifferentaspect dictionofTIGERcorpustestdata(BERTencoded)fed extractorclassiﬁersandTIGERgoldlabels. toeachofthetrainedaspectclassiﬁers. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Classiﬁcation scores of each aspect classiﬁer throughout training is selected as the model with when fed with other extracted aspect vectors. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 8>  <Abstractive Summary> =  Table 8: The transformer model settings for each trainallmodelsusingNoamOptoptimizer(Rush, dataset given the training data size. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  </Paper ID = 241> 

<Paper ID = 242>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Nine Chinese linguistic phenomena covered by CLiMP with acceptable and unacceptable sentence ex- amples. </Abstractive Summary>  <Extractive Summary> =  3.3 LinguisticPhenomena CLiMP covers 9 major linguistic phenomena in 3.5 ComparisonwithBLiMP Mandarin Chinese, cf. Table 1. They are picked BLiMP consists of 67 datasets, each containing fromacomprehensiveChinesegrammarbookby 1,000MPsandorganizedbyphenomenoninto12 Po-ChingandRimmington(2015). </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We further compare word-level and OurLMsstugglemostwiththebaˇ construction, character-level models (cf. Table 2). For evalu- binding,andﬁller-gapdependencies. </Extractive Summary>  </Table ID = 2>  </Paper ID = 242> 

<Paper ID = 243>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Faithfulness metric within different part-of- Fperm increasedfaithfulnessfrom90%to95%for speech(POS)tags. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: BLEU score of the baseline and the model Figure4: Theseexamplesshowsomecaseswherethe trained with F . </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Average entropy and average normalized en- predictionforcesthemodeltolearnbettertransla- tropy of the baseline, the proposed model (F ), and tionsbyforcingittojustifyeachoutputinaright all themodeltrainedwithattentionentropyregularization. </Abstractive Summary>  <Extractive Summary> =  Figure 4 showssomeexamplesofhowourproposedmodel canproducebettertranslations. 6 RelatedWork 5.5 Do the New Models Have Sparser Atten- Attention and Different Axes of Interpretabil- tion? ity Whileseveralstudieshavefocusedonunder- standingthesemanticnotionscapturedbyattention Table 4 shows the average entropy and average (GhaderandMonz,2017;VigandBelinkov,2019; normalizedentropyforthebaseline,theproposed Clarketal.,2019),evaluatingattentionasaninter- model(F ),andthemodeltrainedwithattention pretabilityapproachhasgarneredalotofinterest. all entropyregularizationrespectively. </Extractive Summary>  </Table ID = 4>  </Paper ID = 243> 

<Paper ID = 244>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Samples of the generated questions. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 244> 

<Paper ID = 245>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Average metrics on Nell-One and Wiki-One few-shot link prediction tasks. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 245> 

<Paper ID = 246>  </Paper ID = 246> 

<Paper ID = 247>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Comparison of methods on the Synthetic SETTING1withrespecttoincreasingprobabilitywith whichalabelisrandomlydropped.Syntheticdataused herehas12000trainingand8000testsamples. </Abstractive Summary>  <Extractive Summary> =  SETTING 1inwhichforeachtraininginstance, k i=1 log2(i+1) oneoftheannotatedlabelsaredropped,uniformly Here, rel is the graded relevance at position i. i at random: In Table 1 we note the performances Thisresultisitselfaveragedoverallquerylabels ofthedifferentapproacheswithincreasingrateat l ∈ H . ∗ which labels are dropped. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Statistics of the datasets used in the experi- ments. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  HiLAPlearns label assignment policy using the reinforcement learningframework. In Table 5, we compare the performance of HIDDENjnt against HiLAP model as reported in Mao et al. (2019). </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Hyperbolicityδ is0fortrees. aremultiplelabelswithmorethanoneparentand As shown in Table 7, RCV1 has 0 hyperbolic- thusitislesstree-likethantheNYThierarchy. It 2840Figure5: HiddenHierarchyintheSyntheticexperiments isduetothisDAG-nessthatourmethoddoesnot performaswellonYelpastheotherdatasets. </Extractive Summary>  </Table ID = 7>  </Paper ID = 247> 

<Paper ID = 248>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  is a variant of the ofﬁcial MATERIAL program metriccalledActualQueryWeightedValue(NIST, 8 Results 2017,AQWV),isarecall-orientedrankmetricthat measureshowwellweordertheretrievalcollection 8.1 MT withrespecttoqueryrelevance. Table 3 shows the results of the MT evaluation. AQWViscalculatedastheaverageof1−(Pm+ Thebestnon-referencesystemforeachlanguage β∗Pfa)foreachquery,wherePmistheprobability and MT system is in bold. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  latedARI,whereahigherARI(andthusahigher 8.2 Document-LevelCLIR quartile)indicatesamorecomplexdocument,and present the average document-level BLEU score MQWVontheTest(Small)andTest(Large)par- foreachquartileinTable6. Intheinterestofspace, titions are shown in Table 4 and Table 5 respec- wepresentresultsforBulgarianandforNMT,and tively. On the Test (Small) partition, we see that deferotherlanguagesandSMTtoAppendixA. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  latedARI,whereahigherARI(andthusahigher 8.2 Document-LevelCLIR quartile)indicatesamorecomplexdocument,and present the average document-level BLEU score MQWVontheTest(Small)andTest(Large)par- foreachquartileinTable6. Intheinterestofspace, titions are shown in Table 4 and Table 5 respec- wepresentresultsforBulgarianandforNMT,and tively. On the Test (Small) partition, we see that deferotherlanguagesandSMTtoAppendixA. </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Document-level BLEU score for the models segmentationmodel. </Abstractive Summary>  <Extractive Summary> =  FINDINGS OF THE IWSLT 2020 EVALU- ATIONCAMPAIGN. InProceedingsofthe17thIn- Results Table 7 shows the effects of the model ternationalConference onSpokenLanguage Trans- ablations on MT system BLEU score. On both lation,pages1–34,Online.AssociationforCompu- NMT systems, we see that there is a roughly tationalLinguistics. </Extractive Summary>  </Table ID = 7>  <Table ID = 1>  <Abstractive Summary> =  Table 10: Farsi BLEU scores on Test (Small) (tran- scribed portion) when separated into quartiles by sen- scribed portion) when separated into quartiles by sen- tencecomplexity(asmeasuredbyARI). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 248> 

<Paper ID = 249>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Nevertheless,theapproachproduces 20,587 positive pairs. Table 1 shows pairs with theirSTSannotationscoresandcosinesimilarity with BoW and USE embeddings. There is broad agreement,buttheannotatedsimilarityisnotfully Figure4:Distributionofcountsofpositivepairs(score capturedbyeitherBoWorUSE.USEprovidesa ≥3)ofannotationsforeachtask(validationsplit). </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Text ↔ Text retrieval performance on MS- Table4: Image↔ImageretrievalperformanceonMS- COCO5ktestsetusingCxCannotations. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Spearman’s R Bootstrap Correlation (×100) jointintra-andinter-modallearning,themultitask onMS-COCO5ktestsetusingCxCannotations. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 249> 

<Paper ID = 250>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Each entry in the table denotes the average ofprojectionbasedclassiﬁersbysubjectingthem dropinaccuracy(%)whenclassiﬁersaresubjecttotest tocommonmisspellings,followedbyananalysis inputs with misspellings, P = 0.2. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Comparisonof projection based models vs BiLSTMs subjectto various types and amounts of perturba- tions. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Avg. </Abstractive Summary>  <Extractive Summary> =  Tothat misspelledwordforanothervalidword. end, we take a large corpus enwik92 (vocabulary Also from Table 4, we found that the changes in sizeof500kand129M words)toanalyzetheaver- ageHammingdistancebetweenLSHprojectionsof the LSH-projection, ∆P(x) due to perturbations isdirectlyproportionaltoLSHprojectiondimen- thewordsinthecorpus. Next,wecomputetheav- sion,K andperturbationprobability,P asin, eragechangesintheprojectionrepresentationsby perturb subjectingthemtothecharacterperturbationsfrom ∆P(x) ∝ K ·Pperturb. </Extractive Summary>  </Table ID = 4>  </Paper ID = 250> 

<Paper ID = 251>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Results on the ACE05 test data. </Abstractive Summary>  <Extractive Summary> =  Thus, we also re- 100% 86.9 65.2 61.7 portthisresultforcomparison. 75% 86.8 65.3 62.5 50% 87.1 64.6 61.5 25% 86.9 66.1 63.5 3.1 ResultsonACE05 15% 87.3 65.9 63.6 First, we compare the proposed pre-training 5% 87.4 65.9 63.4 method with previous works in Table 1. In gen- Table3: ResultsontheACE05testdatabyvaryingthe eral, the relation performance of ENPAR signiﬁ- sizeofpre-trainingdata. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Results on the ACE05 test data in differ- ingthemodelwithmulti-tasklearning. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Results on the SciERC test data (upper part) on large-scale unlabeled text corpora, and then andtheNYTtestdata(bottompart). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: The entity performance of “BERT” and “ENPAR”fordifferententitytypesonACE05testdata. </Abstractive Summary>  <Extractive Summary> =  A MoreEvaluations arXivpreprintarXiv:1909.03546. Table 6 and Table 7 show the performances of Shaolei Wang, Yue Zhang, Wanxiang Che, and Ting “BERT” and our “ENPAR” on each entity type Liu. 2018. </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  Table 7: The relation performance (exactly match) of “BERT” and “ENPAR” for different relation types on ACE05testdata. </Abstractive Summary>  <Extractive Summary> =  A MoreEvaluations arXivpreprintarXiv:1909.03546. Table 6 and Table 7 show the performances of Shaolei Wang, Yue Zhang, Wanxiang Che, and Ting “BERT” and our “ENPAR” on each entity type Liu. 2018. </Extractive Summary>  </Table ID = 7>  </Paper ID = 251> 

<Paper ID = 252>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ModelsandExperimentalProcedures. Fortext Table 2 summarizes results for data augmen- classiﬁcation, we use BERT (Devlin et al., 2019) tation in the MTV using γ =γ =0.5 com- O aug (bert-base-uncased from HuggingFace) to pared with traditional augmentation for the best- extractfeaturesbyaveragingthelasthiddenstates performing augmentation strength from α ∈ oftheinputtokens. Toreducethenumberofmodel {0.05,0.1,0.2,0.3, 0.4,0.5}. </Extractive Summary>  </Table ID = 2>  </Paper ID = 252> 

<Paper ID = 253>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  This over-conﬁdenceisevidentfromtheextremelyhigh Table3: Importanceofrelationtypes. selectedconﬁdenceparametersofγ = −0.005for GNNmodel(line2inTable2)andγ = −0.0025 for context-aware model (line 3 in Table 2), sug- answers only in tables (8% of all questions) and gestingthatonlyhigh-valueandhigh-conﬁdence thedifﬁcultyincalibratinganswersfromtablesand scoresareconsideredfromthetable-basedmodel. textagainsteachother. </Extractive Summary>  </Table ID = 2>  </Paper ID = 253> 

<Paper ID = 254>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  WeincludetheuniversalPOStagsas 3.3 Results inputsbyconcatenatingtheembeddingswiththe wordembeddingsintheinputlayer. Weacknowl- Table 1 shows the main results. We observe that edgethattheinclusionofgoldPOStagsdoesnot ﬁne-tuningviaself-trainingalreadyhelpsDT,and reﬂectarealisticlow-resourcesettingwheregold by incorporating multiple high probability trees tags are not available, which we discuss more in with PPT, we can push the performance slightly Section 3.3. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Comparison of projective and non-projective nmod directtransfer(DT),PPT,andPPTX-PRAG.Scoresare LAS,averagedover5runs. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ar es Therefore,PPTX-PRAGS hasthebeneﬁtofensem- DT 28.1 64.1 blingandmultilingualitybutnotdata. PPT 30.8 67.3 Table 3 reports their LAS on the development PPTXEN5 30.9 66.3 setofArabicandSpanish,averagedoverﬁveruns. PPTX-PRAGS 36.5 70.3 We also include the results of PPTX-PRAG that PPTX-PRAG 36.5 71.9 enjoys all three beneﬁts. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  periments presented in the paper. Table 4 shows MohammadSadeghRasooliandMichaelCollins.2019. the hyperparameter values of our English source Low-resource syntactic transfer with unsupervised parserexplainedinSection3.1. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Hyperparameter values of ST, PPT, PPTX, PPTX-REPR, PPTX-PRAG, projective PPT, and pro- jective PPTX-PRAG. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 254> 

<Paper ID = 255>  <Table ID = 1>  <Abstractive Summary> =  Table 1: A snippet of two different emotionally in- ple decoders. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Inaddition,ourmodeloutper- forms multiple strong baselines on automatic we have acquired by learning similar tasks. For evaluation measures such as F1 and BLEU instance, in Table 2 the context history has sev- scores, thus resulting in more ﬂuent and ade- eralutteranceswithHappy,Fearful,Disgustedand quateresponses. Curious to dive deeper emotion and the target re- sponsesarelabelledwithFearfulandHappyemo- 1 Introduction tion. </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Wealsotesttheimportanceoffocallossusing in the data, the models may only have learned to the EmoHRED-SA model. As shown in Table 5, predictthemostfrequentutterances. Sincethedia- afterweeliminatethefocalloss,thereissigniﬁcant loguesareinherentlyambiguous,predictingthem dropinEAandF1whichjustiﬁesouruseoffocal accuratelywouldrequiremoredata. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  For exam- sentences. ple,inTable6,thetargetresponseforUtterance5 In Table 6, we present few examples of the re- should have the emotion ‘Happy’ but it gets con- sponses generated by one of the baseline model fusedwiththeemotion‘Surprised’andgenerates (HRED)andourproposedmodelgiventhedesired an irrelevant response. Table 4 shows the distri- emotion. </Extractive Summary>  </Table ID = 6>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ple,inTable6,thetargetresponseforUtterance5 In Table 6, we present few examples of the re- should have the emotion ‘Happy’ but it gets con- sponses generated by one of the baseline model fusedwiththeemotion‘Surprised’andgenerates (HRED)andourproposedmodelgiventhedesired an irrelevant response. Table 4 shows the distri- emotion. As shown in the table, the responses bution of emotion classes present in the dataset. </Extractive Summary>  </Table ID = 4>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  As shown in the table, the responses bution of emotion classes present in the dataset. predictedbytheEmoHRED-SA-FL-CLmodelhas More detailed examples can be found in Table 8 mostly predicted adequate and emotionally rele- andTable9oftheAppendix. 2925Utterance1 doyouenjoysports? Neutral EmoHRED-SA-FL-CL ido!it’sveryinteresting.iusedtoplayvideogameswheniwasakididon’thaveanytime. Weobservethatthepredictedresponsesasshown LifengShang,ZhengdongLu,andHangLi.2015. Neu- in Table 8 fails to generate adequate as well as ral responding machine for short-text conversation. emotionallyrelevantresponses. </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In Development of Multimodal Interfaces: Weobservethatthepredictedresponsesasshown Active Listening and Synchrony, pages 169–181. in Table 9 are very close to the ground truth re- Springer. sponseandarealsoemotionallyaccurate. </Extractive Summary>  </Table ID = 9>  </Paper ID = 255> 

<Paper ID = 256>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Avg. </Abstractive Summary>  <Extractive Summary> =  Results. Table 1 shows performance of clas- siﬁers trained on CLPSYCH and MULTITASK by Tomeasuretheperformancebiasacrossdemo- demographicgroup. ModelstrainedonCLPSYCH graphic groups we report performance on each tendtoperformworseonfemalePoCuserscom- demographic group. dataset size. We continue adding data after a de- We copy MULTITASK column from Table 1 (la- mographic group has been fully saturated to un- beled as full) for ease of comparison. There is a derstandhowinformationfromoverly-represented performance difference between male PoC users groupscangeneralizetounder-representedgroups. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Avg. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Avg. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 256> 

<Paper ID = 257>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Summary statistics of the MTOP dataset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Results on compositional decoupled representation for 6 languages. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 257> 

<Paper ID = 258>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Domain-wise raw data statistics for chosen patternsoccurfrequentlyincrucialcontextssuch medicalspecialties astakingmedications,followinglifestylechanges suggestedbydoctors,measuringvitalsigns,etc. </Abstractive Summary>  <Extractive Summary> =  We and cover events with a variety of temporalities collectedasetof4999de-identiﬁedclinicalnotes ranging from intervals with ﬁxed duration (e.g., from40specialties,byscrapingmtsamples.3 The pregnancy),tointervalswithindeterminableend- notes are reference samples provided by various points (e.g., long-term cardiac failure). Table 1 users, with names and dates edited for conﬁden- givesanoverviewofthenumberofnotesandcon- tiality. They are freely available to print, share, versationsineachspecialty. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Dataset statistics. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 258> 

<Paper ID = 259>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Example question perturbations from synthetic and natural noise challenge sets for three types of inter- faces: AutomaticSpeechRecognition(ASR)systems,KeyboardandMachineTranslation(MT)systems. Table 10: Performance of the different QA models on different human annotator voices: Indian Female (H1), RussianFemale(H2),IndianMale(H3),andScottishMale(H4). Table 11: Effect of question repair and data augmentation on BERT performance on both synthetic and natural noiseforthethreeinterfacetypes. </Abstractive Summary>  <Extractive Summary> =  Syntheticnoisesetswere voices using Google English Text-to-Speech sys- usedfordevelopmentandtuninginallexperiments. temwithfourdifferentaccentsettings(Australian, Table 11 also breaks down data augmentation re- British, Indian, and US) and two gender settings sults by the speciﬁc augmentation noise source. (maleandfemalevoices). </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Example outputs of different ASR systems generatoremploystheback-translationtechnique onarecordedquestionfromSQuAD(Rajpurkaretal., (Sennrichetal.,2016;Dongetal.,2017;Yuetal., 2016). </Abstractive Summary>  <Extractive Summary> =  Thisislikelyduetothegenera- torsnotbeingequallycontrollable: whilewecan arbitrarilymakethesynthetickeyboardsetnoisier speechusingGoogleSpeech-to-Textoptimizedfor by increasing the corruption rate, synthetic ASR English–US.BesidesGoogleASR,weuseKaldi andMTpipelinesincludeblack-boxcomponents ASpIRE(Poveyetal.,2011;Peddintietal.,2015) which also make the task easier for the interface andESPnetCommonVoice(Watanabeetal.,2018; bydesign(TTSsynthesizesidealizedspeech,back- Ardilaetal.,2020)open-sourcesystems,asshown translationmimicsMTtrainingconditions). in Table 2. We choose the former for analyzing In this section, we investigate how robust QA thedownstreameffectofout-of-vocabularyword modelsaretotheseinterfaceerrors. </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  WecompareKaldi,whichoutputsanUNK settingcomparedto6%forGerman. token for unknown words (Peddinti et al., 2015), Table 5 shows example translations from four and Google’s large-vocabulary ASR model. On XQuADlanguagesandhighlightstheirdivergences our set of human voices, Kaldi produces at least one UNK token for ∼50% of the questions, and from the original questions. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Effect of question repair and data augmentation on BERT performance on three types of natural noise. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Performance of different QA models in the TTS-ASR pipeline with different synthetic voices. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  </Paper ID = 259> 

<Paper ID = 260>  </Paper ID = 260> 

<Paper ID = 261>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Examples of global context selected via N- gram similarity (top) and cosine similarity (bottom). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Results on the ASNQ Test Set. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 261> 

<Paper ID = 262>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  WecompareSyntax-BERTwithvanilla inSection4.1,whichisdesignedtostudythesyn- baselinesandLISA-enhancedmodels. Theresults tacticandsemanticcompositionalityofsentiment are listed in Table 2. As shown in the table, our classiﬁcation. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  last,wepresenttheanalysisofthestructuralprobes LISA captures the syntactic information through inSection4.5. an additional attention head, whereas our frame- Thestatisticsofalldatasetsadoptedinthispa- workincorporatessyntacticdependenciesintoorig- per are summarized in Table 1. For each dataset, inal pre-trained attention heads and increases the weoptimizethehyper-parametersofSyntax-BERT sparsity of the network. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Afterthemodel Syntax-BERT-Base(Ours) 87.7 84.9 istrained,wemakepredictionsonthetestdataand Syntax-BERT-Base+LISA(Ours) 87.8 84.9 sendtheresultstoGLUEonlineevaluationservice1 BERT-Large 88.4 86.8 toobtainﬁnalevaluationscores. LISA-BERT-Large 88.8 86.8 Syntax-BERT-Large(Ours) 88.9 86.7 TheevaluationscoresonalldatasetsinGLUE Syntax-BERT-Large+LISA(Ours) 89.0 87.0 benchmark are illustrated in Table 4. The perfor- mances of BERT-Base, BERT-Large, RoBERTa- Table3: ComparisonwithSOTAmodelsonNLIdatasets. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  eachwordpairinasentence. Weprobemodelsfor As shown in Table 5, all datasets beneﬁt from theirabilitytocapturetheStanfordDependencies theusageofsyntacticinformation. Generally,par- formalism(deMarneffeetal.,2006). InEMNLP,pages65–74. Table 5, a random decomposition of the network Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, alsoresultinmoderateperformanceenhancement. and Jingbo Zhu. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Generally,par- formalism(deMarneffeetal.,2006). Asshownin ent/child masks are of more importance than the Table 6, for both metrics, the syntax-aware mod- sibling masks. Moreover, the topical attention els get better scores than corresponding baseline layeriscrucialtotheperformanceofSyntax-BERT models,indicatingthatSyntax-BERTisabletoin- model, indicating the advantage of decomposing corporatemoresyntaxinformationthanitsvanilla self-attentionintodifferentsub-networksandper- counterparts. </Extractive Summary>  </Table ID = 6>  </Paper ID = 262> 

<Paper ID = 263>  </Paper ID = 263> 

<Paper ID = 264>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Adjacent-branching baseline and maximum UUAS decoding accuracy per PUD treebank, expressed as best scoreand bestlayer/head combination forUUAS decoding. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 264> 

<Paper ID = 265>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Results on the test set of CNN/DailyMail i vationtopredictbinarylabels: dataset. </Abstractive Summary>  <Extractive Summary> =  We evaluated our proposed model on the bench- mark CNN/DailyMail dataset (non-anonymized 3.3 ResultsandAnalysis version)(Hermannetal.,2015). Weusedthestan- Table 1 shows the results on CNN/DailyMail darddatasetsplit,whichcontains287,227/13,368 dataset. TheﬁrstpartcontainstheLEAD-3base- / 11,490 documents for training, validation, and line and Oracle upper bounds. courseparsing. Forcoreferenceresolution,weused As Table 1 shows, our proposed model out- thespanBERT-based(Joshietal.,2020)versionof performs the BertSum(EDU) baseline by a sig- the end-to-end coreference resolver proposed by niﬁcant margin (0.88/0.78/0.96 on F of R-1/R- 1 Leeetal.(2017). 2/R-L).Ourproposedmodelalsooutperformsthe SincetheCNN/DailyMaildatasetonlycontains BertSum(sent) model and other sentence based abstractivegoldsummaries,wehavetoconstruct extractive summarization baseline models. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Xu portance of coreferent EDUs differently. Table 2 et al. (2020) also introduces an end-to-end EDU indicatesacommonpatternoftheimprovedcases basedextractivesummarizationmodel. </Extractive Summary>  </Table ID = 2>  </Paper ID = 265> 

<Paper ID = 266>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  WMT-16SharedTask First,weusethebench- CommonCrawl Sextet Previous datasets share mark dataset provided for WMT-16 Shared Task the same domains that are heavily biased toward on Bilingual Document Alignment. We evalu- French content (see Table 3). We leverage a ateandcompareCDAwithotherEnglish–French monthly crawl from CommonCrawl, speciﬁcally 5https://github.com/christianbuck/ 6A typical multilingual domain can have thousands to wmt16-document-alignment-task/blob/ millionsofpages;e.g. This Czech 29 241 77 7,264 7,248 section aims to verify this in a multilingual set- Danish 137 2,932 693 39,488 38,996 Deutsch 5,525 8,863 83,663 170,932 113,851 ting, mainly when operating with a signiﬁcantly Dutch 599 2,407 8,228 79,293 79,146 Farsi 1,316 1,404 14,697 13,875 6,122 higher number of languages and domains using Finnish 170 1,313 355 12,403 12,229 WMT-16DeepCrawlandCommonCrawlSextet, French 115,671 143,972 2,653K 3,568K 1,411K Hebrew 209 140 7,742 5,295 83 respectively. Hungarian 1,253 1,382 10,494 6,158 4,448 Indonesian 368 551 625 1,204 900 Italian 6,644 7,310 57,977 94,098 55,802 On WMT-16 Deep Crawl Table 3 shows the Japanese 823 1,475 6,593 14,720 11,138 number of parallel documents and sentences ex- Korean 913 136 13,365 2,229 224 Malay 1,040 1,904 8,467 13,088 7,213 tractedbyanend-to-endSTRANDpipeline,after Norwegian 67 1,875 196 35,362 35,273 Polish 557 934 9,685 21,528 17,255 ﬁlteringandduplicationremoval,describedinSec- Portugese 1,545 6,200 12,294 104,561 96,850 tion 4.1. The result shows that CDA contributes Russian 3,984 2,475 36,565 55,010 40,722 Slovak 170 850 211 2,157 2,106 anextraof53%,75%,and195%incleanparallel Spanish 8,334 21,765 114,874 317,430 252,523 sentencescomparedtoURLforFrenchalone,and Swedish 83 2,394 2,773 49,420 49,238 Thai 82 10 830 259 40 whenFrenchisandisnotconsidered,inthismore Turkish 1,057 2,041 9,598 18,412 10,789 extensiveandmorerealisticsetting,respectively. </Extractive Summary>  </Table ID = 3>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Summary of Benchmark Datasets for Docu- NOVALINCS mentAlignmentTask. </Abstractive Summary>  <Extractive Summary> =  French) or translated text (documents in English and The process results in a dataset of 600+ domains theEnglishtranslationsofdocumentsinFrench) with 17.6MM and 4.1MM pages in English and six selected languages. Table 1 summarizes the datasetsconsideredinourexperiments. 4.2.2 Resource therepresentations. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Comparison of the baseline URL, top-3 per- Japanese, Russian, and Turkish. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  extractedviaCDA. On CommonCrawl Sextet Table 4 shows the result in English parallel tokens extracted from notprovidesomedetailsoftheexperimentsetting, thepipelineusingURLandCDAinthedocument whicharenotcriticaltoillustrateourﬁndings. alignmentstep. </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Number of parallel English tokens (in MM) cross-lingual information retrieval and enabling fromtwodeepcrawlsseededbyURLandCDA. </Abstractive Summary>  <Extractive Summary> =  mentforwebdata,CDA,whichprojectsmultilin- WethencomputedtheyieldofparallelEnglishto- gualdocumentstoacommonspaceforsimilarity kens extracted from each setting. Table 6 shows calculation. Wealsodescribedourefforttocollect theresults. </Extractive Summary>  </Table ID = 6>  </Paper ID = 266> 

<Paper ID = 267>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Dataset statistics in training, validation, and testsets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Confusion matrix of Pointer-Generator pointer-generator-basedandBERT-based,respec- Supervised-Att(Glove)prediction. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  and column), while the highest confusions were The effect of copy mechanism We evaluated forcopyingfromthecaptionandgenerationfrom ourpointer-generator-basedmodelusinganabla- the vocabulary. The accuracy of generating cor- tion test, as shown in Table 3. As we can see, rect metric-type tokens from the vocabulary was the performances of our generation model with- 27.78%,andtheaccuracyofcopyingametric-type out a copy mechanism decreased. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  confusion was for copying from the header. We also computed the accuracy of generated metric- The effect of segment embeddings Table 4 typetokensandfoundthatjust58.7%ofthegener- shows the effect of segment embeddings in our atedtokenswerecorrect. BERT-basedmodel. </Extractive Summary>  </Table ID = 4>  </Paper ID = 267> 

<Paper ID = 268>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 268> 

<Paper ID = 269>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Mapping between French tenses and tense labels used to build our corpus. </Abstractive Summary>  <Extractive Summary> =  the Stanza models. 4This mapping is more precisely defined in Ap- 6More precisely: 75% of sentences in the past, 88% pendix A Table 2 ofsentencesinthepresentand85%ofsentencesinthe 3082Chinese French micro prec. macro prec. sentencewasdeducedfromtheautomaticmor- phological analysis of its root verb using the 4. Temporal adverbs: We have deter- mapping defined in Table 2. minedalistofadverbswithtemporalcon- French passé composé describes a sit- notation. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Results achieved by our different models in the tense prediction task. </Abstractive Summary>  <Extractive Summary> =  crafted features designed to capture the infor- Results We evaluate the results of the tense mation identified as relevant for determining prediction task using both micro and macro thetenseofaChinesesentence. Werelyonthe precisiontoaccountfortheimbalancebetween theoreticalstudyofSmithandErbaugh(2005) classes.8 Results are reported in Table 1.9 to define the features:7 indicators to describe As expected, the best results both for French the presence of aspectual markers (e.g. 了 or and Chinese are achieved by fineBert, the 过) or temporal adverbs (e.g. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Precision achieved by the different classifiers for each class in tense prediction tasks Chinese French micro macro micro macro sim. </Abstractive Summary>  <Extractive Summary> =  for each class 2. Aspect markers: Perfective aspect Table 3 presents the results of different clas- marker (了 le, 过 guo) and the imperfec- sifiers for each tense category. It shows that tiveaspectmarker(着zhe).Wedidn’tcon- morefrequentdataaremorelikelytobebetter sider another imperfective aspect marker predicted: the Present class (67% of the exam- (在 zai) because Stanza didn’t annotate ples) gets the highest score for all classifiers this marker. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  These words FineBert have been annotated by Stanza with the dependency label nmod:tmod. A Table 4 details the performance of FineBert complex sentence could contain multiple and the impact of the similarity between the nmod:tmod words. We only consider the trainandtestsentences. </Extractive Summary>  </Table ID = 4>  </Paper ID = 269> 

<Paper ID = 270>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Monolingual ENResults. Table 2 quantiﬁes the effectsofapplyingIPTwiththeEN-EWTUDtree- SIQA we train in batches of size 8, whereas on bank to BERT and RoBERTa. We report down- Multi-NLI and PAWS we train in batches of size streamLUperformanceonNLI,PAWS,andSIQA. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  9WeexplainthismismatchintheAppendix.XNLI PAWS-X XCOPA Transformer ParseFT DE FR TR ZH DE FR ZH TR ZH None 71.0 73.7 63.0 70.3 85.1 86.3 76.4 52.0 61.2 mBERT Standard 71.4 72.9 61.5 70.4 85.4 86.9 79.8 57.4 65.4 Adapter 71.7 74.8 62.5 70.2 85.8 87.1 78.7 50.4 61.6 None 77.1 78.1 73.4 73.8 88.3 89.3 81.4 61.2 66.4 XLM-R Standard 76.1 77.2 73.1 73.8 86.4 89.2 81.1 59.2 67.4 Adapter 77.8 76.4 73.9 74.7 86.7 88.7 80.7 57.4 65.6 Table3: Performanceofmultilingualtransformers,mBERTandXLM-R,inzero-shotlanguagetransferfordown- streamLUtasks,withandwithoutpriorintermediatedependencyparsingtrainingontargetlanguagetreebanks. transfer setup, for both mBERT and XLM-R, in None Parsing MLM 90 Table 3. Again, these results do not particularly 86.387.186.8 favortheintermediateinjectionofexplicitsyntac- 80 78.778.6 ticinformationingeneral. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  com/huggingface/transformers (v.2.7). Table 4 details the LM-pretrained transformer models from this framework which we exploited inthiswork. BesidestheTransformerslibrary,our code only relies on standard Python’s scientiﬁc computinglibraries(e.g.,numpy). </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Universal Dependencies treebanks used in our study. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 270> 

<Paper ID = 271>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Training and evaluation data sizes in num- Table1: ExamplesofdifferencesininputdatainETA bers of sentences. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Results of automatic evaluation metrics BLEU and term translation accuracy (Acc.). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 271> 

<Paper ID = 272>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  𝑝 < 0.05 level (𝑝 = 0.038) was found. Post hoc comparisons using one-sided paired t-tests 5 Results showedthatthemeanaccuracyoftheApertium- enhancedmodelissigniﬁcantlygreatercompared Table 1 shows the results for all three enhanced to the the Default (𝑝𝑎𝑑𝑗 = 0.0005), Lexicon systemsandtwobaselines. TheApertiummodel outperformsothermodelsformostlanguages, al- 5Theresultsforbe_hsewereextremeoutliersandwerenot includedinthecomparison. The augmented system are those words in the test set that were not seen usesApertiumtocreateextralabeledtrainingdata, by the model during training. The results are whileourenhancedmodelusesApertiumtogener- shown in the right-most section of the Table 1. ateadditionallemmacandidatestothewordsofthe TheimprovementsontheOOVwordsarevariable, sameinitialtrainingset. Theresultsoftheseevaluations cascadeofthreesteps: 1)iftheSFispresentinthe areshowninTable3. Thesetoflanguagesinthis lexicon, then the lemma is immediately retrieved table is slightly diﬀerent than in Table 1, only in- fromthelexicon. 2)IftheSFisnovelandismissing cludingthoselanguagesforwhichtheUDlexicons fromthelexicon,aneditoperationisgeneratedthat areexistent. tended with both UD and 8K lexicons remain notperformbetterthantheDefaultbaseline(see roughly the same. However, when extending the results in Table 1), adopting additional UD or StanzawithUDlexicons,mostlanguagesimprove 8Klexiconsduringtesttimeincreasestheresults atleastslightly,asshownwiththeunderlinedscores to the same level with the Apertium-enhanced inthecolumnStanza+UD.Overall,onaverage,the model. This shows that our proposed approach simplelexiconextensionmethodfallsconsiderably doesnotneedadditionalresourcesduringtraining— behindourApertium-enhancedmodel(97.63vs. SinceStanzausesthe columns of the right-hand block of Table 3 the lexiconresourcesasaﬁrststepinthecascade,itis results of two Lexicon-enhanced models (recall pronetopotentialerrorsandnoiseinthelexicons. Section 4 and Table 1), similarly extended with The dual-encoder model is safer against noise in theUDand8Klexicons. TheLexicon-enhanced thisrespectbecausethelemmacandidatesarenot modelhasaccesstothesamedataastheStanza simplychosenasthepredictionifpresentbutare modelduringbothtraining(trainingset+thetrain- ratherfedthroughthesystemthatcandecidehow ingsetbasedlexicon)andtesting. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Comparison of the enhanced models with the augmentation method: Def is the Default model, Apt istheApertium-enhancedmodel, Def+8KandApt+8KarethesameDefaultandApertium-enhancedmodels with augmented data. </Abstractive Summary>  <Extractive Summary> =  Theresults which candidate how much to take. On average, in Table 2 show that, on average, extending the there are 0.71 lemma candidates per input word, Apertiumsystemwiththeseparticularlexiconsdo and 1.09 lemma candidates per input word when notaddanybeneﬁt. Thereasonsforthatcanbetwo- excluding those words that do not have external fold: 1) The UD lexicons are for most languages lemmacandidates. Theresults Figure 2 shows the improvement in accuracy oftheseexperimentsareshowninthe right-most over the Default model trained with POS-tags blockofTable2. only of 1) the Default model trained with both We ﬁrst show in Table 2 that the results of the POS-tagsandmorphologicalfeatures,2)theAper- Apertium-enhancedmodelstrainedwithdropout tium-enhancedmodeltrainedwithonlyPOS-tags, are equivalent to the results obtained without and3)theApertium-enhancedmodeltrainedwith dropoutasevidencedbythecolumnApt . Next, bothPOS-tagsandmorphologicalfeatures. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thus, we also show in the last two usageoftheexternaldata. SinceStanzausesthe columns of the right-hand block of Table 3 the lexiconresourcesasaﬁrststepinthecascade,itis results of two Lexicon-enhanced models (recall pronetopotentialerrorsandnoiseinthelexicons. Section 4 and Table 1), similarly extended with The dual-encoder model is safer against noise in theUDand8Klexicons. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Examples for Ukrainian, German, and Be- larusian words corrected by the enhanced model. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 272> 

<Paper ID = 273>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Comparisons of mean story length (L ), story datasource,acollectionofnewsstoriesfromthe summary length (L ), and compression rate summ WanliperiodofMingDynasty(1573–1620). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: ROUGE F1 scores (%) of standard cross- great temporal gap (up to 400 years for DE and lingual summarisation. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 273> 

<Paper ID = 274>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  these classes. Intuitively, the lower the FPR∗, the 3146R ↓ R ↓ R ↓ Table 2 shows results for lexical bias reduc- NOI OI ONI tion using both debiased training approaches, as Original 0.0445 0.2641 0.6718 well as models trained on datasets ﬁltered us- Random 0.0345 0.2603 0.6683 ain AFLite 0.0434 0.2458 0.6016 ing AFLite and all three regions from DataMaps. %tr DataMaps-Ambig. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Lexical associations between toxicity and approaches also hurt in-distribution test perfor- TOXTRIG mentions in the original dataset (Founta mance, indicating that ONI and other TOXTRIG et al., 2018) and various ﬁltered counterparts. </Abstractive Summary>  <Extractive Summary> =  The second contains the word p*ss DataMaps. As shown in Table 1, subsets given whichtheannotatorsmayhavereliedfortheiras- by AFLite and the ambiguous and hard regions sessment. Thethirdencouragesviolence/abuseto- produced by DataMaps reduce the overall asso- wardsanidentitywhichisn’ttypicallythetargetof ciations between TOXTRIG words and toxicity, violence. Best performance for each training set size is in boldface. Takeaway: Both debiasing approaches improve per- sistent with Table 1). Notably, DataMaps-Hard formanceoverbaselines,withDataMaps-Hardproving performs the best at dialectal debiasing, both in the most effective at debiasing. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Dialectal bias evaluation for all debiasing methods (§5), as well as the relabeling approach (§6) Figure 2: Challenge set evaluation for lexical biases, on the Founta et al. </Abstractive Summary>  <Extractive Summary> =  Thedebiasedtrainingapproaches settings. improveoverthebaselines;dataﬁlteringmethods Results in Table 4 show that almost all data ﬁl- do not. One reason for this might be that data tering and debiasing methods reduce dialectal bi- ﬁltering methods were trained on much less data ases, with DataMaps-Easy as the exception (con- 3148Test R ↓ F ↑ FPR ↓ AAE 1 AAE Vanilla 0.4079 92.33 16.84 0.0 0.3 LMIXIN-Dialect - 92.260.1 16.070.4 Random 0.4027 92.18 16.67 n 0.1 0.6 ai AFLite 0.3577 91.940.1 16.840.8 tr DataMaps-Ambig. 6.75 12.17 5.42 1.80 3% DataMaps-Hard 6.36 11.67 5.31 1.84 Then, we evaluate the dialectal bias of AAE- 3 DataMaps-Easy 8.46 16.30 7.83 1.94 relabeled and quantify the dialect and racial pre- AAE-relabeled 6.93 10.60 3.67 1.53 diction biases from a RoBERTa-large classiﬁer trainedonAAE-relabeled,following§5. Asshown Table 5: Racial disparity in toxicity prediction re- in the last row of Table 4, this relabeling scheme ported on Preo¸tiuc-Pietro and Ungar (2018). W-Tox. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Racial disparity in toxicity prediction re- in the last row of Table 4, this relabeling scheme ported on Preo¸tiuc-Pietro and Ungar (2018). </Abstractive Summary>  <Extractive Summary> =  There- ing the difference in rates of ﬂagging tweets by fore,weturnourattentiontotheroleoflabelnoise African American authors and those by white au- indatasets. Partlyinspiredbyourqualitativeanal- thors,followingSapetal.(2019).13 yses of debiased models’ predictions, we design Listed in Table 5, our results show that auto- a proof-of-concept study where we automatically matic debiasing methods do not consistently de- correctthelabeloftweetsusinga(nautomatic)di- crease the racial discrepancy in ﬂagging toxicity. alectal translation of the tweet, inspired by previ- Notably, the toxicity rates on tweets by African ous work showing that highlighting AAE tweets’ American authors—and the diferences compared dialect led them to be labeled as less toxic (Sap to white authors—are similar across all debias- etal.,2019). ing these biases. As shown in Table 5, the model trainedonAAE-relabeledhasthelowestracialdis- parity in toxicity ﬂagging rates compared to all Focusing on dialectal bias, our key assumption othermethods. is that an AAE tweet and its corresponding WAE These results highlight that debiasing meth- versionshouldhavethesametoxicitylabel,there- ods are much less effective at mitigating dialec- fore toxic AAE tweets whose WAE versions are tal dataset biases compared to data relabeling. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Examples of AAE tweets with their GPT-3 based WAE translation, and original gold standard and new annotations based on AAE-relabeled. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  To prevent over the dataset. Table 8 shows that FPR of the the LEARNED-MIXIN ensemble from ignoring bi, debiasedmodelincreaseinsteadexceptforthe OI Clark et al. (2019) add an entropy penalty (H) to category and Table 9’s results behave in-line with theloss: Table4. </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  Table 9: Dialectal bias evaluation for all debiasing methods,onbothin-distributiontestsetaswellasout- of-distributiondialectandraceprimingtestsets. </Abstractive Summary>  <Extractive Summary> =  Table 8 shows that FPR of the the LEARNED-MIXIN ensemble from ignoring bi, debiasedmodelincreaseinsteadexceptforthe OI Clark et al. (2019) add an entropy penalty (H) to category and Table 9’s results behave in-line with theloss: Table4. R =αH(softmax{g(x )logb }) C Few-shot AAE-to-WAE Translation i i (cid:80) Note that we do not recommend the following Where H(z) = − z logz is the entropy and j j j approach to build large scale parallel data for αisahyperparameter. </Extractive Summary>  </Table ID = 9>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Lexical and dialectal associations between toxicity in the original dataset (Davidson et al., 2017) and various ﬁltered counterparts. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  </Paper ID = 274> 

<Paper ID = 275>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Results for machine translation on How2 dataset. </Abstractive Summary>  <Extractive Summary> =  So, another simple way to improve perfor- vidualunimodalbaselines(RefertoTable1.) The mance would be to introduce visual and acoustic resultsrevealthatbothauditoryandvisualmodal- attentionmechanismsaswell;however,wewould ities always contribute towards enhanced trans- stillneedtoaddressthecoreproblemheterogene- lation, but the contribution of visual modality is ity. slightlylower(indicatedbythelowerincreasein On training GANs: Training GANs is known BLEU scores in Table 1.) This is also consistent to be difﬁcult. However, we employed various withtheﬁndingsofGro¨nroosetal.(2018). </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Results for machine translation on Multi30K pre-process the raw audio ﬁles to obtain a lower- dataset.Note:Allmethodsuse‘v’and‘t’asthesource modalitiesexceptUnimodalSeq2Seq. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Precision (P), Recall (R), F1-score (F), and includeinputnormalization,batchnormalization, Accuracy(A)foremotionrecognitiononIEMOCAP. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 275> 

<Paper ID = 276>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ﬁne-tuneBERTusingFARM15 foradefaultdura- tionoftenepochsinaleave-one-outstyle,training BERTBaseline Sincethestandardunsupervised on all texts except one and evaluating on the re- methodsdonotperformwellforourtask,webuild maining. Table 2 shows that, while performing asimplesupervisedbaseline. Tothisend,weﬁne- muchbetterthantheunsupervisedbaselinesfrom tuneapre-trainedBERTmodel14 tobinaryscene the previous paragraph, BERT is not capable of segmentationinthefollowingway: Weconstruct providingasatisfactorysegmentation. </Extractive Summary>  </Table ID = 2>  </Paper ID = 276> 

<Paper ID = 277>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Statistics of the datasets (Abbreviations: Cl: (OC, WTP), Non-noisy (MT, PE, VG, WD). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We trends in other datasets and the overall weighted- progress with the 32-dimensional representation average score as well. We also perform experi- ofBERTasweobservenoelevationinresultson mentsonotherpermutations,andtheirresultsare using the 768-dimensional representation, as can listed in Table 5. Finally, we combine both POS be seen in Table 5. We also perform experi- ofBERTasweobservenoelevationinresultson mentsonotherpermutations,andtheirresultsare using the 768-dimensional representation, as can listed in Table 5. Finally, we combine both POS be seen in Table 5. Besides, the latter results in andDEPmoduleswiththeBERTarchitecture(aka. isdemonstratedbyasigniﬁcantincreaseof∼ 2% m-F1fromcombinedviewpointtoseparateview- 6 ExperimentalResults pointsexperiment. Table 5 shows m-F1 and c-F1 for different vari- A. BaselinesandComparativeAnalysis ants of LESA. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Macro F1 (m-F1) and F1 for claims (c-F1) in the in-domain setup. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  </Paper ID = 277> 

<Paper ID = 278>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Cross-Attchtransformation patterns. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Then,wemapselectedexamplestotheir *arsi(im|pa|per|arsi) 26/1.00 Self-Attsubpatterns. Table 4 presents the queries *car*(ri|mb|ec|car|si) 3/1.00 and extracted patterns.11 For each query, we list Q:goldtarget=*ono&!(*scono) &MSD=msd it 41/0.95 themostfrequentpatterns(sortedbyfrequencyina *ere(ri|otten|ere) 19/0.95 decreasingorder)alongwithonesegmentedlemma *dere(te|le|ve|dere) 10/1.0 example mapped to the pattern.12 The segments *ger*(cos|par|ger|si) 3/1.00 oflemmaexamples,identiﬁedassalient(andpre- Table4: Self-AttsubLemmaPatterns. Qisaqueryregu- sentedinthepatterns)arehighlightedinbold. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The model does not search for the segment. Wereportaccuracyandeditdistanceon cluesinthevowelpatternsofthestembutchooses the test set in Table 5. Additionally, we provide asmartstrategytofocusdirectlyontheinﬂection information on the number of trained parameters endingsforlemmas: theyarefrequentandalready for each model. </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Finnish Self-AttsubPatterns. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  </Paper ID = 278> 

<Paper ID = 279>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  eachSQLqueryinthetestset,weretrievethek−1 most relevant templates from the template set to 5.1 AutomaticEvaluation generatek−1differentquestions. Wealsoinclude onequestionwithtemplate<BEG> <A> <END> The automatic evaluation results are reported in to evaluate the model’s capability in generating Table 1 and 2. Our model ERI outperforms all a complete sentence. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  As the BLEU calculates the over- pointscaleforeachSQL-questionpairregarding lappingofn-grams,itdoesnotnecessarilyreﬂect the(1)Fluency: grammaticalcorrectness,(2)Con- thequalityoftemplate-basedgeneration. Weillus- sistency: the semantic alignment with the corre- trate that by an example of our model in Table 4. sponding SQL query, and (3) Diversity: the di- AlthoughtheBLEUscoresarelow,thegenerated verseexpressionofthegeneratedquestions. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Ourmodelperformscompeti- the scores. The results are presented in Table 3. tivelywithQGLVonWikiSQLandshowsasub- Ourmodeloutperformsthebaselinemodelsindi- stantial improvement on Spider. </Extractive Summary>  </Table ID = 3>  </Paper ID = 279> 

<Paper ID = 280>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Result of the sentiment classiﬁcation experi- ing.Thenoiselevelistheprobabilitythateachwordin ment.Thenoiselevelistheprobabilitythateachword the dataset will be replaced by typos. </Abstractive Summary>  <Extractive Summary> =  level of the evaluation dataset and intentionally producingmisspelledwordstocomparerobustness tothetypos. 4.5.2 Results Table 3 shows that Ko-ft has the highest perfor- manceinthesentimentclassiﬁcationexperimentre- sultsinthecleanenvironment.Besides,Word2vec showedsigniﬁcantlyreducedperformancethanKo- ft.ThisisbecauseseveralOOVswereincludedin thetestset.However,asthenoiselevelincreases, theperformanceofmisKovertakesthatoftheKo-ft. ItconﬁrmsthattheperformanceofmisK decreases relativelysmoothly asthe amountof noisein the Figure4:Cosinesimilaritybetweenthegeneratedword datarises.Inotherwords,misK ismorerobustto vectorandpre-trainedwordvector. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: In OOV experiments, words that are related to the original word are highlighted in bold font. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 280> 

<Paper ID = 281>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Results for the test sets 2016, 2017 and 2018 (averaged over 3 runs): * marks statistically signiﬁcant increasesinBLEUw.r.t.RL-base(p-value≤0.05). </Abstractive Summary>  <Extractive Summary> =  Thesecond tion(OC)featuresareusedasacontrastivesetting. block in Table 1 shows the deterministic policy Wait-2 and Wait-3 approaches. RL-base per- • Unimodal RL baseline (RL-base): This formsonparwiththeWait-2(English-French)and baseline follows (Gu et al., 2017) where the Wait-3(English-German). </Extractive Summary>  </Table ID = 1>  </Paper ID = 281> 

<Paper ID = 282>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Our model outperforms BERT for Train/Val/Testforeachtopic. thetestset,eventhoughitemploysasigniﬁcantly Table 5 shows the experimental results for the smallersetofparameters(5xlessparameters)and different mathematical topics. Initially, we ex- isnotpre-trainedonalargecorpusasBERTis. </Extractive Summary>  </Table ID = 5>  </Paper ID = 282> 

<Paper ID = 283>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Examples of partially matched answer string tains the title entity of the ﬁrst paragraph, End pairs. </Abstractive Summary>  <Extractive Summary> =  However, during error c c w 11.9 8.5 6.1 analysis,weﬁndthatmanypredictedanswersthat c w c 16.4 17.2 16.5 partially match the gold answers should also be c w w 6.5 3.9 3.4 regarded as correct. Some representative exam- w c c 4.2 4.0 4.5 ples are shown in Table 1. Although these pre- w c w 12.1 11.1 15.2 dicted answers have zero EM scores, they are se- w w c 3.1 1.9 5.6 manticallyequivalenttothecorrectanswersgiven. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  g a predicted answer text a , they partially match p ifeitheroneofthefollowingtworequirementsis thesub-questionsofcorrectlyansweredmulti-hop satisﬁed: questions,wecollectthecorrectnessstatisticswith regard to each individual example. Table 3 and f1 > 0.8 Table 4 present the results. The ﬁrst four rows showthepercentageofexampleswhosemulti-hop f1 > 0.6∧{(a containsa )∨(a containsa )} g p p g questioncanbecorrectlyanswered. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  g a predicted answer text a , they partially match p ifeitheroneofthefollowingtworequirementsis thesub-questionsofcorrectlyansweredmulti-hop satisﬁed: questions,wecollectthecorrectnessstatisticswith regard to each individual example. Table 3 and f1 > 0.8 Table 4 present the results. The ﬁrst four rows showthepercentageofexampleswhosemulti-hop f1 > 0.6∧{(a containsa )∨(a containsa )} g p p g questioncanbecorrectlyanswered. </Extractive Summary>  </Table ID = 4>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The ﬁrst four rows showthepercentageofexampleswhosemulti-hop f1 > 0.6∧{(a containsa )∨(a containsa )} g p p g questioncanbecorrectlyanswered. Amongthese (1) examples,wenoticethatthereisahighprobability Table 2 shows the performance of the three that the models fail to answer at least one of the models on multi-hop questions and their single- sub-questions, asshownin rows2 to4. Werefer hop sub-questions. </Extractive Summary>  </Table ID = 2>  </Paper ID = 283> 

<Paper ID = 284>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  This has two drawbacks: it rectedifnecessary. Inaddition,German,Hindiand reducesthenumberoftriplesthatcanbeconsidered Japanesetemplateswerecheckedbynativespeak- drasticallyandhindersperformancecomparisons ers to assess translation quality (see Table 2). To acrossLMswithdifferentvocabularies. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: p1 for BERT, mBERT queried in English, Table2:EffectofmanualtemplatemodiﬁcationonUn- mBERTpooledonLAMAandLAMA-UHN. </Abstractive Summary>  <Extractive Summary> =  Surprisingly,weﬁndaper- jority of languages. Table 3 shows that pooled formance difference of 1 percentage points (0.23 mBERT outperforms mBERT[en] by 6 percent- vs. 0.24, p1 averaged over languages) in favor of age points on LAMA, presumably in part be- the machine translated templates. </Extractive Summary>  </Table ID = 3>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We conclude that template modiﬁcations guages for the same relation. However, Table 1 arenotaneffectivesolutionforthetypingproblem. showsthatmBERTexhibitslanguage-speciﬁcbi- ases;e.g.,whenqueriedinItalian,ittendstopredict 4.2 TranslationQuality Italy as the country of origin. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Comput- ingResearchRepository,arXiv:2002.01808. 3256A LanguageBias Table 4 shows the language bias for 10 relations. (cid:945)(cid:1011)(cid:941)(cid:910)(cid:913)(cid:1010)(cid:979)(cid:989)(cid:910)(cid:980)(cid:1011)(cid:941)(cid:913)(cid:917)(cid:999)(cid:909)(cid:941)(cid:1005)(cid:1011)(cid:937)(cid:997)(cid:1005)(cid:927)(cid:917)(cid:910)(cid:995) (cid:997)(cid:1005)(cid:943)(cid:910)(cid:995)(cid:899)(cid:916)(cid:961)(cid:947)(cid:909)(cid:1005)(cid:913)(cid:941)(cid:1011)(cid:910)(cid:979)(cid:989)(cid:937)(cid:999)(cid:1011)(cid:987)(cid:925)(cid:910)(cid:919)(cid:999)(cid:903)(cid:993)(cid:919)(cid:1011) For each relation we aggregated the predictions (cid:916)(cid:1011)(cid:913)(cid:941)(cid:955)(cid:991)(cid:909)(cid:1010)(cid:1003)(cid:1005)(cid:979)(cid:1005)(cid:947)(cid:1005)(cid:987)(cid:752)(cid:991)(cid:916)(cid:1011)(cid:995)(cid:947)(cid:941)(cid:991)(cid:909)(cid:916)(cid:976)(cid:992)(cid:991)(cid:909) (cid:1317) (cid:1317) (cid:1317)(cid:1317)(cid:1317) acrossalltriplesandshowthemostcommontwo (cid:1317) (cid:1317) (cid:1317) (cid:1317) (cid:1317) predictedentitiestogetherwithitscount(inbrack- ets). </Extractive Summary>  </Table ID = 4>  </Paper ID = 284> 

<Paper ID = 285>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  documents in training set which contain user- 3.4 ResultAnalysis provided keywords. We pretrain a CNN model Table 4 shows that our method VWS-PR outper- using pseudo samples as the training set. In the formsVWSby4%,2%,and1%onYelp,IMDB, CNNmodel,fourdifferentﬁltersizes{2,3,4,5} andAmazondatasetsrespsectively. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  When a target word andanopinionwordsatisfyadependencyrelation, 3.5 Hyper-parametersSensitivityAnalysis we will extract the opinion word. The details of dependencyrelationandexamplesareprovidedin WeﬁrstshowF1scoresonthreedatasetswithvar- Table 3. When a pair of words satisfy one rule, iedβ inFigure1(a). </Extractive Summary>  </Table ID = 3>  </Paper ID = 285> 

<Paper ID = 286>  <Table ID = 2>  <Abstractive Summary> =  Table 2: WSD results in F scores on SemEval-2007 1 (SE07)andtheconcatenationofallthedatasets(ALL). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  of the art among single and ensemble systems Acknowledgments trained only on SemCor without the use of addi- tionaltrainingdataorresourcesexternaltoWord- Theauthorsgratefullyacknowledge Net such as Wikipedia, surpassing the previous thesupportoftheERCConsolida- state-of-the-artnon-ensemblesystemofVialetal. torGrantMOUSSENo.726487un- (2019) by 2.0% in F1 score (signiﬁcant with p < dertheEuropeanUnion’sHorizon 0.05, χ2 test), as shown in Table 1. When fur- 2020 research and innovation pro- thertrainedontheWordNetglossesandexamples, gramme. </Extractive Summary>  </Table ID = 1>  </Paper ID = 286> 

<Paper ID = 287>  </Paper ID = 287> 

<Paper ID = 288>  <Table ID = 1>  <Abstractive Summary> =  Table 1: An example each of a cognate and a false cross-lingual models is, yet again, a painful task. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: T-test statistics for average ﬁxation duration falsefriendpairsobtainedfromthisdataandcon- timeperwordforPositivelabels(Cognates)andNega- structwhatwecall“D1”. </Abstractive Summary>  <Extractive Summary> =  error tolerance limit (α) is set to 0.05. The t-test analysis, presented in Table 3, shows that for all 4 FeatureSetsforCognateDetection participants, a statistically signiﬁcant difference exists between the average ﬁxation duration per Inthissection,wediscussthevariousfeaturesused wordforcognatepairsvsfalsefriendpairs. forthetaskofcognatedetection. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We These models are generated by aligning two dis- collectgazedataforonly200word-pairs(D2)with joint monolingual vector spaces through linear thehelpof9annotatorswhichprovidesuswitha transformations,usingasmallbilingualdictionary totalof1800datapointsfortrainingandvalidation. forsupervision(Dovaletal.,2018;Artetxeetal., As reported in Table 5, the initial results on D1 2017). Additionally,thecross-lingualembeddings usingdifferentcross-lingualembeddingsshowthat model trained using Artetxe et al. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Additional results in terms of weighted Precision (P), Recall (R), and F-scores (F) using 5-fold cross- validation using different feature sets as described above. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  </Paper ID = 288> 

<Paper ID = 289>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  standardmulti-classclassiﬁerswiththeannotated relationphrasesforeachstatementinthetraining set while the training label is the strength of the similartoLampleetal.(2016)inadditiontoaset statement. Since BERT-SL performs best among of novel features as described in Table 2 in the allrelationphraselabelingapproaches,wepresent CRF layer6 along with the hidden state vector of all our subsequent results for this case only. We the BiLSTM layer. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Wepredictthestrengthofastatementbasedonits relation phrase. For this purpose we build classi- 5.1 Relationphraselabeling ﬁersthattakebagofwordsvectoroftherelation- We note down the main results in Table 3. The shipphraseasidentiﬁedbyBERT-SLandpredict table shows that BERT-SL outperforms all other its strength. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Number of perfect matches for the compet- theother. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Strength classiﬁcation results. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  If variousclassiﬁersacrossdifferentstrengthclasses wefeedthedirectBERTembeddingofthesource (6,4and2). Inthetable,entriesforthetruerela- andthetargetstatementtothesetofclassiﬁers(RF, tionphrasesrepresenttheobtainedmicro-F1and SGD and XGB in Table 6) the results are much macro-F1scorescorrespondingtoatestsetwhere worsethanourapproach. Furtherifweadaptthe theground-truthannotatedrelationphrase(instead BERTtextclassiﬁer(asusedintextualentailment of what is obtained from BERT-SL) is taken into detectionmodule)tosolveourproblem(‘Neural’in account. </Extractive Summary>  </Table ID = 6>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Weapplya heuristicsLONG VB,LONG VB NSUBJ DOBJ sequenceofheuristicfunctionsh1,h2,h3,... andLONG NOUNmatchandreturnphrases,how- to recover the relational phrase (see Table 8 ever,sinceLONG VBhasthehighestpriorityso forthelistofheuristics). Ifh1returnsanode, therespectivephrase‘improved’isreturnedasthe weidentifyitsconstituentphraseasourrela- relation phrase. </Extractive Summary>  </Table ID = 8>  <Table ID = 1>  <Abstractive Summary> =  Table 10: Location of the statement with the main classiﬁerthereforeconfusesthestrengthclass. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  whichleadstoanerrorinthestrengthclasspredic- tion. Forinstance,considerstatements1,2and3 Incorrectrelationinterpretation: Insomecases, shown in Table 9. In all these cases the presence likestatement6, thesystemisunabletodifferen- ofthemodiﬁerssuchas‘little’,‘any’etc. </Extractive Summary>  </Table ID = 9>  </Paper ID = 289> 

<Paper ID = 290>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (Mean) of the ground truth response, that is, the Results on VisDial v1.0 testset. As presented average rank of the ground truth answer in the in Table 1, our best model (baseline model with model’soutputrankedlist(lowerisbetter). bothsoftconstraints)signiﬁcantlyoutperformsall thepreviouspublishedmodelsandreachnewstate- 4.3 TrainingDetails of-the-artperformanceonMRR,R@1,R@5and Inspired by the open source code4 of Murahari R@10. Here, we use p to denote the posinequation11. d−1 1 (cid:88)2 PE ·PE = sin(w p )·sin(w p ) 1 2 k2 i 1 i 2 i=0 +cos(w p )·cos(w p ) i 1 i 2 d−1 1 (cid:88)2 = cos(w (p −p )) k2 i 1 2 i=0 d−1 1 (cid:88)2 = cos(w ∆p) k2 i i=0 A 3 Table 1 in this section shows the results of the fourmodelsontheVisDialv1.0developmentset. Similartotheresultsobtainedbytestingonthetest set,themodelwithbothconstraints(Model+C1+ C2)hasthebestperformanceacrossallevaluation metrics, and addinganyoneof theproposed soft constraintimprovestheperformanceofthebaseline model, which indicates the effectiveness of our approachalsoduringtraining. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ple,“isitdaytime?”. Wetestourmodelsandthe baselinemodelusingthesefoursmalldatasets,and the results are illustrated in Table 2. Comparing theresultsinTable2withthatinTable1,insome casestheperformanceinTable2isbetter, which meansthatthedifﬁcultyofthesesampleddatado not higher than that of the test set. </Extractive Summary>  </Table ID = 2>  </Paper ID = 290> 

<Paper ID = 291>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thetableallowstoconcludethat obtainedbyﬁne-tuningPLMsonthefulltraining forgettableexamplescontainmoreminorityexam- setfor3epochs(usingthedefaulthyperparameters plesthanarandomsubsetofthesamesize. foreachtask)andthenontheforgettableexamples In Table 3, we perform a similar analysis for only, for 3 more epochs with a smaller learning thepresenceofcontradictionwordsinthesecond rate. SeeBinAppendixformoredetails. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Results of our BERT models ﬁne-tuned on needpriorknowledgeofspuriouscorrelationsand differentsourcesofforgettableexamples.Foreachline, exploittheminorityexamplesexplicitlybyfurther the accuracy on MNLI and HANS are shown, as well astheiraverage. </Abstractive Summary>  <Extractive Summary> =  Weconsidertherecentconﬁdenceregularization or“Reg-conf”techniquefromUtamaetal.(2020), 4.3 Results asourmainbaselineinallthreetasks. Thismethod 4.3.1 MNLIandHANS is an improvement to the earlier related work in makingmorerobustNLPmodels(Heetal.,2019; In Table 4, we present the results of our models Clarketal.,2019). Speciﬁcally,Reg-confclaims andfourrecentbaselines. aticgeneralizationcapabilities(Loulaetal.,2018; LakeandBaroni,2018;Baanetal.,2019;Hupkes Other diagnostic evaluations Fine-tuning on etal.,2018)thusseeminglylackingcompositional theforgettableexamplesofsimplebiasedmodels behavior(Montague,1970). Whilethismightseem improvesrobustnessinthethreechallengingbench- atoddswiththecommonbeliefthathigh-levelse- marksHANS,FEVER-SymmetricandPAWS.We manticrepresentationsoftheinputdataareformed additionally evaluate the trained models listed in (Bengioetal.,2009b),therelianceonhighlypre- Table 4 on Stress tests (Naik et al., 2018), adver- dictivebutbrittlefeaturesisnotconﬁnedtoNLU sarialNLI(Nieetal.,2019)andMNLI-matched- tasks. Itisalsoaperceivedshortcomingofimage hard(Gururanganetal.,2018). </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (Moreanalysisispresentedin onFEVERdev,andsymmetricv1andv2datasets. Appendix.) 4.3.2 QQPandPAWS 4.3.3 FEVER Here we report the results of our method applied In Table 6, we report the results of our method to QQP and PAWS as out-of-distribution dataset. appliedtotheFEVERdevelopmentandsymmet- ResultscanbefoundinTable5. </Extractive Summary>  </Table ID = 6>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Average accuracy across random seeds on Table7:ResultsofBERT modelsﬁne-tunedonthe BASE QQPandPAWSforBERTandXLNETbaseandlarge setofforgettableexamplesonly. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  Table 9: Average accuracy over seeds on FEVER and modelcalibration. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 9>  <Table ID = 1>  <Abstractive Summary> =  Table 10: Fine-grained accuracy results of BERT BASE on the MNLI dev set split by word-overlap between hypothesisandpremise. Table 11: Fine-grained accuracy results of BERT on QQP development set before and after ﬁne-tuning on for- gettables. </Abstractive Summary>  <Extractive Summary> =  layer feedforward network. To compute p or h, the BoW model max-pools the bag of word em- C Forgettablesandword-overlapin beddings,whiletheBiLSTMmodelmax-poolsthe QQP top-layer hidden states of a 2-layer bidirectional In Table 11, we show the performance of our LSTM.ThehiddensizeoftheLSTMsissetto200. method on the QQ evaluation set as a function Overall,BoWandBiLSTMcontain560Kand2M ofword-overlap,themainheuristicPAWSwasde- parameters,respectively. B.1 Forgettablesandword-overlapinMNLI Model Entailment Non-Entailment All High Low All High Low BERT 84.0 89.9 76.0 84.9 85.5 84.6 BERT+ F 80.2 85.1 73.4 85.6 86.9 85.0 BOW BERT+ F 79.9 85.2 72.4 85.6 87.4 84.8 BILSTM Table 10: Fine-grained accuracy results of BERT BASE on the MNLI dev set split by word-overlap between hypothesisandpremise. In Table 10, we show the performance of our methodontheMNLIdevsetasafunctionofword- overlap, the main heuristic HANS was designed against. We split the evaluation set into High (> 3331Paraphrase Non-Paraphrase Model All High Low All High Low BERT 90.0 90.8 88.9 92.2 85.6 95.0 BERT+ F 85.2 84.9 85.8 93.0 87.3 95.4 BILSTM BERT+ F 87.3 87.2 87.4 92.6 86.4 95.2 BOW Table 11: Fine-grained accuracy results of BERT on QQP development set before and after ﬁne-tuning on for- gettables. </Extractive Summary>  </Table ID = 1>  </Paper ID = 291> 

<Paper ID = 292>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  As all perburba- examplesbeforeperturbation. tions in our meaning-preserving test sets do not changemeaningsoforiginalutterances,itimposes As shown in Table 2, SOTA semantic parsers newresearchchallengesonhowtomakesemantic sufferfromsigniﬁcantperformancedropinalmost parsersresistmeaning-preservingperturbationsas all meaning-preserving test sets compared to the well as how to avoid overﬁtting on semantically results on standard test sets. The performance insigniﬁcantwords. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  sure. As shown in Table 3, TRANX is superior to Our evaluation framework supports also in- theothertwoparsersonJOBS,andCOARSE2FINE depth analysis on the impact of different pertur- is the clear winner on GEOQUERY. This rank- bation method on semantic parsers. </Extractive Summary>  </Table ID = 3>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  When we further investi- the meaning-preserving test sets. The shared ad- gate the deleted or replaced function words, such versarialexamplesamongtheparsersvarysigniﬁ- as in Table 1, semantic parsers are not expected cantly across domains. More than 50% of the ad- 3338(a)JOBS (b)GEOQUERY (c)ATIS Figure1: Contributionsofperturbationoperationsonthesharedadversaries. </Extractive Summary>  </Table ID = 1>  </Paper ID = 292> 

<Paper ID = 293>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  This large number also cov- (Yangetal.,2019;Nogueiraetal.,2019;Daiand ersavarietyofIRsolutions,rangingfromsimple Callan,2019b),wefedtothemodelthefollowing traditionalIRtotransformerinteraction-basedmod- input“[CLS]querytokens[SEP]documenttokens els. Table 2 shows our best results, a ninth-place [SEP]”, where “[CLS]” and “[SEP]” are special out of 100 in the ﬁrst round and a second-place boundarytokens,withtheﬁrstonebeingaclassi- out of 73 in the third round. In both rounds, our ﬁertokenthatisfurtherfedtoamulti-layerpercep- systemwasabletobeattraditionalIRtechniques trontocomputetheﬁnaldocumentrelevance. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  widely adopted in the literature. Moreover, we also include distilBERT since it is currently one Table 4 presents the measured times using a ofthefastesttransformer-basedmodelsavailable, batch size ﬁxed at 16. As illustrated, our sim- andALBERT,givenitscomparativelysmallnum- ple model can run 32 times faster on the CPU ber of trainable parameters. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Complete training times, i.e., training all the left,whichcorrespondstotheexperimentsonthe layers, in seconds measured over 1104 samples on a CPU, the models seem to scale with the increase CPUandGPU. </Abstractive Summary>  <Extractive Summary> =  of the batch size linearly. Additionally, only our proposedmodeliscapableofkeepinganinference Table 5 shows the time required to fully train time under one second, which can be viewed as thetransformer-basedmodels,i.e.,trainingallthe an acceptable query latency time. Similarly, on layers plus the classiﬁer layer, in a pairwise set- the right side, corresponding to the experiments ting. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Training times, in seconds, only for the clas- thattheproposedmodelcouldachieveclosetostate siﬁcationlayermeasuredover1104samplesonaCPU oftheartperformanceinbothbiomedicalad-hoc andGPU. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  </Paper ID = 293> 

<Paper ID = 294>  </Paper ID = 294> 

<Paper ID = 295>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  All models produce separate sentence vectors Strikingly, we observe that even when models forthepremiseandhypothesis. Theyareconcate- aretrainedontasksthatdonotrequirethelinguis- nated with their element-wise product and differ- ticpropertyatallforthemaintask(rowswithPT ence (Mou et al., 2016), passed to a tanh layer in Table 2), probing classiﬁers still exhibit high andthentoa3-waysoftmaxclassiﬁer. Modelsare accuracy(sometimesupto∼80%). </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Performance comparisons of models initialized with pretrained word embeddings (Word) and models withrandomlyinitializedembeddings(Rand)onMNLIDevelopmentSet(Dev)andontheprobingtask(Probing). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  However,thesemethodsfail data. These results can be found in Table 4. We toisolatethecontributionofthetrainingtask. </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Descriptive statistics for NOISE, UNCOR- setofstringsS(cid:48) = (a|b)∗ ofmaximumlength30, RELATED, PARTIAL and FULL synthetic datasets, as andsimulatefourkindsofcorrelationsthatcould well as the dataset used to train the probing classi- occurinadatasetbyinsertingcatarandomposi- ﬁer(PROBE). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Number of train/dev/test examples in con- fp,𝛩 fh,𝛩 fp,𝛩 fh,𝛩 structedsyntheticdatasets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ingcorrelatedlinguisticproperties. 2)Information mightbeencoded,butmaystillnotberecoverable Results Table 8 reports the performance of the bythechoiceofprobingclassiﬁer12. 3)Adversar- adversarialandattackerclassiﬁersonthefourtest iallearningdoesnotremoveallinformationfrom sets. </Extractive Summary>  </Table ID = 8>  </Paper ID = 295> 

<Paper ID = 296>  <Table ID = 1>  <Abstractive Summary> =  Table 1: mSVDD with different settings of m. </Abstractive Summary>  <Extractive Summary> =  ofnegativesamples,i.e.,sentencesfromWikiText- 5.2 Results 2,whicharelabeledwith0. 5.2.1 ResultsofmSVDD Encoder For encoding the text input, i.e., Table 1 shows the performance of mSVDD with φ(x,W), weusedaBidirectionalLSTMwithat- different choices of m, i.e., the number of hy- tention (Hochreiter and Schmidhuber, 1997; Xu perspheres. Here, mSVDD(1) represents uni- etal.,2015),withthenumberofhiddenunitsbeing modal deep SVDD ((Ruff et al., 2018)). </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: CVDD with the proposed negative supervi- improvetheperformanceofmSVDD.Theexperi- sion. </Abstractive Summary>  <Extractive Summary> =  (GhafooriandLeckie,2020)showed thatfocusingonsome“good”hypersphereswould 5.2.3 ResultsofCVDDwithnegative supervision bebeneﬁcialratherthanoverallhyperspheres. In the calculation of attentions, we did not adjust δ Table 3 shows the results of CVDD with the pro- soastohavealargeweightforonespeciﬁchyper- posednegativesupervisionformSVDD.Asmen- sphere. Thismaycauselimitedimprovements. </Extractive Summary>  </Table ID = 3>  </Paper ID = 296> 

<Paper ID = 297>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  mains,allarehighlysimilar. Andthis,despitethe For instance, in Table 2, we can see that we are verydifferentsensegranularitiestheyhave. This able to sample sentences containing three senses meansthatdespitetheapparentdifferencesinthese ofthewordcount: (1)nobletitle,(2)determining resources, they all tend to produce similar poly- the total number of, and (3) taking into account. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Whilethiswillbeinves- isnotasgoodanindicatorofitspolysemyasone tigatedinfuturework,westillgive,asaproofof 11forRBO,D4L10andD4L8hadthesamescore. concept, an example in Table 3. In this example, 3397cosine spearman kendall 2 2 2 3 3 3 4 4 4 5 78 5 35 5 25 6 6 6 nb of pyramid levels 1311987 777246nb of pyramid levels 1311987 223050nb of pyramid levels 1311987 1250 15 70 15 15 15 10 17 17 17 10 19 19 19 2 3 4 5 6 7 8 910 12 14 16 18 20 2 3 4 5 6 7 8 910 12 14 16 18 20 2 3 4 5 6 7 8 910 12 14 16 18 20 nb of PCA dimensions nb of PCA dimensions nb of PCA dimensions (a) (b) (c) p@k ndcg rbo 2 2 2 3 30 3 3 4 4 4 5 5 5 6 6 30 6 20 nb of pyramid levels 1311987 2205nb of pyramid levels 1311987 2205nb of pyramid levels 1311987 15 15 15 15 15 10 17 17 17 15 19 19 19 2 3 4 5 6 7 8 910 12 14 16 18 20 2 3 4 5 6 7 8 910 12 14 16 18 20 2 3 4 5 6 7 8 910 12 14 16 18 20 nb of PCA dimensions nb of PCA dimensions nb of PCA dimensions (d) (e) (f) Figure4:Performance(colorscale)vs.numberofPCAdimensions(xaxis)vs.numberoflevelsinthehierarchy(yaxis). </Extractive Summary>  </Table ID = 3>  </Paper ID = 297> 

<Paper ID = 298>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Top 20 languages assigned by Twitter (left) Data. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Performance of our COVID-relevance mod- XLM-R 97.72 97.70 97.71 97.69 Base els on the Twitter data in CoAID, ReCOVery, and XLM-RLarge 97.92 97.90 97.95 97.93 CoAID+ReCOVery. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Performance of COVID-relevance models. </Abstractive Summary>  <Extractive Summary> =  Claims News Tweets Claims News Tweets Results. As shown in Table 5, XLM-RLarge ac- CoAID 839 837 10,900 376 2716 149,343 quiresbestresultswith97.95accand97.93macro ReCOVery - 665 26,418 - 1,364 114,402 F onTEST.Theseresultsaresigniﬁcantlybetter Total 839 1,502 37,318 376 4,080 263,745 1 than a majority class baseline (based on TRAIN) Table7: COVID-19MisinformationDatasets. andanotherarbitrarily-chosen(yetquitecompeti- tive)baselinemodelthatchoosestherelatedclass (majorityclassinTRAIN)75%ofthetime. ReCOVery(Zhouetal.,2020),bothofwhichare claimed by the authors to be completely (100%) CoAID. Cui and Lee (2020) present a Covid-19 relatedtoCOVID-19.9 heAthcaremIsinformationDataset(CoAID),with diverseCOVID-19healthcaremisinformation,in- As Tabel 6 shows, We do observe a drop in cludingfakenewsonwebsitesandsocialplatforms, modelperformanceascomparedtoourbestmodel along with related user engagements (i.e., tweets on our own TEST set in Table 5 (acc drops on average by 15.5% and 7.6% F ). However, the and replies) about such news. </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Wenowintroduce ber 1, 2019 to July 1, 2020. Table 7 shows class ourmisinformationmodels. distributionofnewsarticlesandtweetsinCoAID. Theyalsocollect140,820tweetsrelatedto DEV TEST Data Model thenewsarticles,consideringthosetweetsrelated Precision Recall F1 Precision Recall F1 to true articles to be true and vice versa. Table 7 LSTM 81.00 91.00 86.00 95.00 78.00 86.00 mBERT 91.00 84.00 87.00 94.00 87.00 90.00 showsclassdistributionofnewsarticlesandtweets CoAID XLM-RBase 93.00 87.00 90.00 87.00 88.00 88.00 inReCOVery. XLM-RLarge 98.00 86.00 92.00 97.00 93.00 90.00 Splits and Cleaning. </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  XLM-RLarge 98.00 86.00 92.00 97.00 93.00 90.00 Splits and Cleaning. Table 8 shows the distribu- LSTM 60.00 56.00 58.00 54.00 57.00 55.00 mBERT 81.00 59.00 68.00 87.00 55.00 68.00 tionoftweetsinCoAIDandRecoverybeforeand ReCOV XLM-RBase 72.00 58.00 64.00 75.00 55.00 64.00 afterthede-duplicationprocess. AsTable8shows, XLM-RLarge 89.00 52.00 66.00 89.00 51.00 65.00 de-duplicationresultsinsigniﬁcantlyreducingthe LSTM 79.00 58.00 67.00 66.00 70.00 68.00 sizesofDEVandTESTsetsinthetworesources. </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The table (bottom half) trarily train the LSMT for 3 epochs. As Table 9 alsoshowsthatretweetsarehighestcarriersofcon- shows, our best results for fake tweet detection tentpredictedasfake(3.67%),followedbytweets onTESTforCoAIDisat90%F (mBERT/XLM- 1 (2.3%). From the table, we can also deduce that R ),forReCOV68%(mBERT),andforthese Large only 2.45% of all English language Twitter con- two combined is 92%. </Extractive Summary>  </Table ID = 9>  <Table ID = 1>  <Abstractive Summary> =  Table 10: Distribution of predicted labels from our byactivelytrackingalistof22popularkeywords COVID-relevance and COVID-misinformation mod- els on randomly selected 30M English samples from suchas#Coronavirus,#Corona,and#Wuhancoro- Mega-COVdata. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 298> 

<Paper ID = 299>  <Table ID = 1>  <Abstractive Summary> =  Table 1: BLEU and METEOR scores on the Switch- by (He et al., 2020; Lample et al., 2017), a ran- boarddevandtestsets. </Abstractive Summary>  <Extractive Summary> =  ing described in the previous section and further ﬁne-tunedwithasupervisedcross-entropylossus- 4.2 Results ingsmallamountsofparalleldisﬂuent-ﬂuenttext. Table 1 shows BLEU and METEOR scores be- We do not use domain embeddings during semi- tween the gold ﬂuent and the disﬂuency cor- supervisedtraining;theinferenceisdoneasinthe rected output from ﬁve different models. We unsupervisedmodel,i.e.,withdomainembeddings. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Effect of ﬁne-tuning with a varying amount 50 SS(Transformer) of supervised parallel corpus to ﬁne-tune our model Seq2Seq(Bi-LSTM) BART trained in unsupervised manner; in effect, results of 40 20 40 60 80 100 semi-supervisedtraining. </Abstractive Summary>  <Extractive Summary> =  Transformer, conj: conjunctions and disc: discourse We obtain 77.34 and 77.97 BLEU on the dev disﬂuencies). andtestsetsusingbinaryembeddings,respectively, whereasthedisﬂuency-typeclassiﬁerembedding Semi-supervised Learning: Table 2 shows the yields 78.72 and 76.90 on the dev and test sets. performancewhenourunsupervisedmodelisﬁne- Theclassiﬁerembeddingsdomarginallyimprove tuned with varying amounts of parallel text. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Analysis of generated text across all models. </Abstractive Summary>  <Extractive Summary> =  (Edits are also hard to correct tothefullysupervisedapproachforsmaller(<10 becauseofthelackoftrainingdata.) token)utterances. Qualitative Analysis: Table 4 shows examples 3424Disﬂuent BART Seq-to-Seq US(Bi-LSTM) US(Trans.) SS Fluent disc., souhbeena beena beena beena beena beena beenadiffer- ﬁller differentturn differentturn differentturn differentturn differentturn differentturn entturn conj., butiiiﬁnd iﬁndthis iﬁndthis anywayi iﬁndthis iﬁndthis i ﬁnd this rep. thiswhole whole whole ﬁnditall whole whole whole restart it’syou’re you’retaking you’retaking it’syou’re it’staking it’staking you’retaking you’retaking wordsand wordsand taking wordsand wordsand words and wordsand developinga developinga chickenand developing developinga developing developinga picturein picturein tobacco andapicture picturein a picture in picturein yourmind yourmind wordsina inyourmind yourmind yourmind yourmind mind conj., andthenyou thenyou youhadto thenyou thenyouhad youhadto by the end disc., youknow thinkyou pickitbyby thinkyou topicksome pickofitby of the sec- restart youhadi hadtopickit theendof hadithink sortofmajor theendof ond you had thinkyou bytheendof thesecond youhadto youhadto topicksome hadtopickit thesecond youhadto pickitbyby picksome sortofmajor bybybythe youhadto picksome theendof major endofthe picksome sortofmajor thesecond secondyou sortofmajor youhadto hadtopick picksome somesortof sortofmajor major aside iforgot iforgot iforgotseen goshiforgot iforgot iforgotharry it’sacouple sally’slast sally’slast lastname lastnameit’s wordstart name name name anywayit’sa acoupleof lastname anywayit’sa anywayit’sa anywayit’sa couple years anywayit’sa couple couple couple couple Table 4: Analysis of generated text across all models. </Extractive Summary>  </Table ID = 4>  </Paper ID = 299> 

<Paper ID = 300>  </Paper ID = 300> 

<Paper ID = 301>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Ablation study for various choices in the frozen BART method, with validation set BLEU score. </Abstractive Summary>  <Extractive Summary> =  tuningofamonolingualmodelforMTcanresult An explosion of interest in large-scale pre- in worse performance than training from scratch training in Natural Language Processing has led (e.g. Table 1). For MT the more common mono- 3440 Proceedingsofthe16thConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics,pages3440–3453 April19-23,2021.©2021AssociationforComputationalLinguisticsontheseapproachesforbothbilingualandmulti- A B C D lingualMT(incontrasttopreviousworklargelyfo- cusingontextclassiﬁcation). The BART encoder ‘expects’ English input, and it may be 5 ResultsandDiscussion theInputModulewithextraﬁxedembeddingscan better account for the different word order in the 5.1 FrozenBART inputlanguage. Inthenextsectionwecompareto Table 1 shows the effects of various choices we mBARTandbaselines. madeinﬁne-tuningBARTforMT.Freezingisim- portant: weseean18.4BLEUpointimprovement 5.2 FrozenmBART fromﬁne-tuningafrozenBARTmodelcompared to ﬁne-tuning an unfrozen BART (both with an InTable3andTable5welistresultsfromfreezing InputModule;seesection3.1). shared embeddings between the of adding ﬂexibility are useful. In Table 10 we inputandsoftmaxlayers,andstrongregularisation presentadditionalresultsontheRo-Enpre-trained (e.g.0.4dropoutonhiddenstates,0.2dropouton model(seesection3.2ofthemainbody). attention scores, 0.2 label smoothing). </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Ablation study on improvement from ﬁne-tuning layer-norm. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  To test this we constructed a new Vi- mBARTresults. WenotemBARTwastrainedon En parallel dataset (Vi-En† in Table 3) using the moreEnglishdatathanBART,andwithdifferent someofthesamesourcesastheFlores(Guzma´n noisingfunctionhyper-parameters. et al., 2019) training data (the Si-En and Ne- 5.3 WhatShouldbeUnfrozen? En training sets used in this work), speciﬁcally GNOME/KDE/Ubuntu domain from the OPUS Layer-Norm We ﬁnd large beneﬁts to simply repository2 andBibletranslationsfromthebible- ﬁne-tuningtheweightsandbiasesofthepre-trained corpus3,andusethesametestandvalidationsets layer-normweights(recallthatafternormalisation, as the IWSLT15 Vi-En dataset. 0.5BLEUforfrozenBART Wealsoconsidertheeffectofthesizeoftheﬁne- (seeTable1)andanaverageof0.8BLEUacross tuningdataset. Ifweconstrainthetrainingdatatoa ﬁvelanguagesformBART(seeTable4compared randomsubsetof200ktrainingexamplesfromRo- to Table 3). Since these weights and biases are En(Table6),the‘ftenc-attn’methodoutperforms only2dparametersperlayer-norm,wheredisthe simple ﬁne-tuning. steps,and1e−4maximumlearningrate. A AdditionalAblationStudy Out-of-domain Vi-En Baseline To train a ran- domly initialised baseline for the out-of-domain InTable9wereproduceTable 4ofthemainpaper Vi-En data (Vi-En† in Table 3 of the main body) withmorecontexttostudytheeffectofunfreezing weusedthesamemodelarchitectureandtraining layer-normparameterswhenﬁne-tuningmBART. settings as those of Guzma´n et al. </Extractive Summary>  </Table ID = 3>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Validation set BLEU (unless stated otherwise) comparing freezing various parts of mBART and En-Ro mBART(pre-trainedonlyonEnandRodata),ﬁne-tunedonRo→Enparalleldata. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Test set BLEU score on many-to-one (Xx → En) multilingual MT with a simple round-robin training schedule. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The useful. We hypothesize that the pre-training task memorysavingsallowforroughly45-75%larger ofmBART(reconstructingnoisymonolingualsen- batchesforthemethodsweconsiderinthiswork tences) does not help with teaching the encoder- (see Table 8 for our mBART methods), but for decoderattentiontoalignsourceandtargettextof larger pre-trained models the proportion of GPU differentlanguages. memory freed up by freezing will increase. </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  Table 9: Validation BLEU score (unless stated otherwise) obtained by ﬁne-tuning layer-norm parameters and of adding adapters for mBART, for Xx → En. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 9>  </Paper ID = 301> 

<Paper ID = 302>  </Paper ID = 302> 

<Paper ID = 303>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The final train set of 606k word pairs liketippiandaddak,(b)littleuseofconjunctcon­ was created after deduplicating and creating train, sonantsunlikeotherIndianlanguages. test and dev splits (See Table 1 for a summary of the mined corpus). We estimate that the training 3 Analysis: MultilingualTransliteration set has 55% non­Indian origin words and 45% In­ We study multilingual transliteration models with dianoriginwords. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Addressing divergence between Tamil and 3.2 ResultsandDiscussion other Indic scripts. The Tamil script is highly under­specified and has fewer characters than Table 2 shows the top­1 accuracy of the different sounds in the English language (unlike other In­ models. FortranslationintoIndianlanguages,mul­ dic scripts). </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Asynchronouspipelinesforprocess­ mining inghugecorporaonmediumtolowresourceinfras­ tructures. InProceedingsoftheWorkshoponChal­ Table 3 shows the language­wise parallel corpora lengesintheManagementofLargeCorpora,pages statistics and Table 4 lists the various parallel cor­ 9 – 16, Mannheim. Leibniz­Institut für Deutsche pora used for transliteration mining. Leibniz­Institut für Deutsche pora used for transliteration mining. Table 3 also Sprache. showsthevocabsizesofmonolingualcorporaused Shantipriya Parida, Ondrej Bojar, and Satya Ranjan foreachlanguageforthemonolingualapproach. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Parallel Translation Corpora used for mining transliterations. </Abstractive Summary>  <Extractive Summary> =  Asynchronouspipelinesforprocess­ mining inghugecorporaonmediumtolowresourceinfras­ tructures. InProceedingsoftheWorkshoponChal­ Table 3 shows the language­wise parallel corpora lengesintheManagementofLargeCorpora,pages statistics and Table 4 lists the various parallel cor­ 9 – 16, Mannheim. Leibniz­Institut für Deutsche pora used for transliteration mining. </Extractive Summary>  </Table ID = 4>  </Paper ID = 303> 

<Paper ID = 304>  <Table ID = 1>  <Abstractive Summary> =  Table 12: Examples of discipline-speciﬁc FEs. </Abstractive Summary>  <Extractive Summary> =  method, section labels were normalised into ﬁve classes: introduction,methods,results,discussion, 2.3 CF-LabelledFEDatabases and other. Each sentence was assigned a section Table 1 describes the existing CF-labelled FE label; we did not use sentences belonging to the databases. Previous studies have shown that FEs ‘other’class. (All fourdisciplines. the CFs are listed in Table 13 in the appendix.) CoreFE is an FE that is shortened so that it can 3 Methods be used as a query for sentence retrieval (Gener- ally, longer phrases result in few or no results in 3.1 CorporaandDatasets sentenceretrieval). WeusedCoreFEstocreatethe 3.1.1 CorporaofScientiﬁcPapers CF-labelledsentencedataset. To show discipline-speciﬁc FEs, we cal- declarationofhelsinki culatedoddsratioforeachCFofeachdiscipline. Table 12 illustrates the top 5 high odds ratio FEs Table 12: Examples of discipline-speciﬁc FEs. The in the ‘description of the process’ CF in the in- completelistisprovidedintheappendix. 2004. Research Genres: Ex- each FE and Table 14 lists the top-5 general FEs plorationsandApplications. CambridgeUniversity foreachCF.FormostoftheCFs,generalFEswere Press. </Extractive Summary>  </Table ID = 1>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Accuracy scores of each range of the maxi- 4 Results mumvalueofthesoftmaxlayer,andtheproportionof sentencesinthecorpora. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  BERT.Table9and10showtheBERTresults;com- paredwiththeresultsshowninTable6and8,the twomodelsdidnotshowaconsiderabledifference. 4.2 CF-BasedSentenceClassiﬁcation 4.4 ConstructingCF-LabelledFEDatabase The classiﬁcation results are shown in Table 6. SciBERT worked well, which implies that this TheCF-labelledFEdatabasewasevaluatedbysam- BERT-based classiﬁer has the ability to capture pling200FEs. </Extractive Summary>  </Table ID = 6>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Average accuracy scores. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  </Paper ID = 304> 

<Paper ID = 305>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Statistics for query and document length for IRthequeryisalongdocument(e.g.,aregulation) IRdatasetsusedinliterature. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  LEGAL-BERT(Chalkidisetal.,2020) 57.6 90.1 C-BERT(ours) 83.8 92.9 ENSEMBLE(BM25+C-BERT) 86.5 95.0 4.5 ExtractingrepresentationsfromBERT Recently there has been a lot of research on un- Table4: Pre-fetchingresultsacrosstestdatasets. derstanding the effectiveness of BERT’s different 5 Experimentalresults layers(Liuetal.,2019;HewittandManning,2019; Jawahar et al., 2019; Goldberg, 2019; Kovaleva Pre-fetching: Table 4 shows R@100 on the test et al., 2019; Lin et al., 2019). Figure 4 shows datasets for the various pre-fetchers considered. </Extractive Summary>  </Table ID = 4>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  OnEU2UK ENSEMBLEperformsbetter thantheothertwopre-fetchers. Interestingly,neu- ralre-rankersfallshortonimprovingperformance and are comparable (or even identical) with EN- SEMBLEinmostcases,possiblybecauseverysimi- lardocumentsmayberelevantornot(Section2.2, Table 2), leading to contradicting supervision.15 Aswehypothesized(Section3.2),re-rankersover- utilizethepre-fetcherscorewhencalculatingdoc- ument relevance, as a defense mechanism (bias) Figure 5: Relevant documents according to their against contradicting supervision, which eventu- chronologicaldifferencewiththequeryon EU2UK de- velopmentdata. ally leads to the degeneration of the re-ranker’s term matching mechanism. </Extractive Summary>  </Table ID = 2>  </Paper ID = 305> 

<Paper ID = 306>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Clearly, this can lead to concerns re- pressingwordembeddings. Theresultsareshown garding the levels of accuracy that can be guar- in Table 3, reaching up to a 25.85% space reduc- anteedbyfunctionstakingtheseprecision-limited tion for GloVe vectors. We also calculate an up- vectors as inputs. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Performance when varying the precision Φ Table 4: Size in bytes of 50-dimensional GloVe em- ofinputGloVeembeddingstosentence-levelsentiment beddingsdataset,theirCRT-compressionandtheupper analysisusingaCNN. </Abstractive Summary>  <Extractive Summary> =  BASE ify the feasibility of this use case, by conduct- ing experiments with varying precisions of 50- 6 DiscussionandFutureWork dimensional and 300-dimensional GloVe embed- These preliminary results suggest that the vec2int dings. Table 5 shows that sentence-based senti- algorithm would be an efﬁcient way of encoding mentanalysiswithGloVeembeddingsremainsal- wordvectorsforspeciﬁcNLPtasks,namelythose mostunchangedevenatΦ = 1ontheMRdataset, which would beneﬁt from arithmetic-supporting on which Arora et al. (2020) shows that BERT vectorcompression(whichtarandziparenot). </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Size in bytes of 50-dimensional GloVe em- ofinputGloVeembeddingstosentence-levelsentiment beddingsdataset,theirCRT-compressionandtheupper analysisusingaCNN. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 306> 

<Paper ID = 307>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Experiment results. </Abstractive Summary>  <Extractive Summary> =  4.1 TaggerPerformance The questions are grouped into paraphrase clus- ters, of which 1,809 clusters have more than one Experimental results and descriptions are shown question. in Table 1. Compared to 118k semantic clusters Quora Question Pairs (QQP) contain human- inMSCOCO,ComQAandParabankEvalcontain annotated duplicate English questions, with 50k only400and1809clusters,respectively. </Extractive Summary>  </Table ID = 1>  </Paper ID = 307> 

<Paper ID = 308>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We use 20 3 EvaluationTasksandExperiments random permutations of each document for both training and testing, excluding the permutations Inthissection,wepresenttheperformanceofthe that match the original one. Table 1 summarizes coherencemodelsonstandardsynthetictasks(i.e., thedatasetsusedintheglobaldiscriminationtask. Global/LocalDiscrimination),followedbytheex- We randomly select 10% of the training set for periments where we apply the coherence models developmentpurposes. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  included in the dataset. Table 3 summarizes the Smith et al. (2016) evaluated traditional (non- datasets. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Machine Translation setting results on WMT2017-2018 data. </Abstractive Summary>  <Extractive Summary> =  One the global discrimination tasks, but only manage of the given translations is the reference, used as 48.67%and43.36%onthistaskrespectively. acontrol,andtovalidateourassumptionthatthe Wealsoreporttheagreementwithhumanrank- referenceismorecoherentthanthesystemtransla- ings on the study data in Table 5. Overall, only tions. We report the accuracy of the coher- mariesfrompopularneuralabstractivesummariza- ence modelstrained onthe global discrimination tionsystemsforCNN/DMdataset(Hermannetal., taskindistinguishingthemorecoherentreference 2015;Nallapatietal.,2016). Sinceabstractivesys- textfromthelesscoherentsystemtranslationsin temsvaryintheirarchitecturesandlossfunctions, Table 5. We can see that most models perform they may produce very different summaries. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Wethenuseouraforementionedassumption Results. Table 8 summarizes the results on the (coherencemodelsshouldscoreP higherthanN) reﬁneddatasetsfortheutterancerankingtask. In for the evaluation. </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  However, it is quite likely that this is sim- Results. Table 9 reports the accuracy of the re- plyapoorapproximationofreal-worldcoherence trained models and the results of the model rank- problems. ConsiderforexamplethatMTsystems ing comparison against human rankings. </Extractive Summary>  </Table ID = 9>  </Paper ID = 308> 

<Paper ID = 309>  <Table ID = 5>  <Abstractive Summary> =  Table 5: There is a strong relation between the political bias of the media sources and the groups we computed using our propaganda score. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  However, the results pagandacontent. arenotstatisticallysigniﬁcant(p > 0.05)andthe differences are close, as seen in Table 6. Hence, Onefollow-upquestionthatweaskishowmany wecanconcludethatthebots’automaticactivity oftheseusersarebots. </Extractive Summary>  </Table ID = 6>  </Paper ID = 309> 

<Paper ID = 310>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  A qualitative analyses is also pre- itymaynotbefullyincorporatedinthemodels. sentedwherewecompareBERTandGloVeresults Qualitative analysis: In Table 3, in P1, the simi- oftheﬁveNCsinTable1(whichshowsthenatu- larity scores between NC in Table 1 and their re- ralisticsentencesforeachNC,togetherwiththeir respectiveNC andNC )8. Wealsodiscuss spective NCsyn for BERT and GloVe models are syn synW shown. Allofthese indicate that these models cannot distinguish the partial semantic overlap between more composi- tionalNCsandtheircomponentsandtheabsence ofoverlapforidiomaticNCs. Qualitative analysis: The P2 results in Table 3 show the highest similarity scores between each example in Table 1 and one of its components. These high similarity scores highlight the priori- tisationoflexicaloversemanticoverlapmentioned above. More- in-out tutionofindividualcomponentsbytheirsynonyms. over,acomparisonwiththesimilaritiesforthesyn- Qualitative analysis: For P3, Table 3 shows the onymsinP1resultedinsim(P4) > sim(P1) and in-out-NEU NC-NEU similarities scores at NC level between each NC sim(P4) (cid:39) sim(P1) , which indicates that these in-out-NAT NC-NAT and their NC counterpart. Again, similarity modelsconsidertheNCoutofcontexttobeabet- synW scores for GloVe are considerably lower than for ter approximation for the NC in context than its BERT.AsexpectedforGloVe,sim(P3) = 0.69 synonym. </Extractive Summary>  </Table ID = 3>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  A qualitative analyses is also pre- itymaynotbefullyincorporatedinthemodels. sentedwherewecompareBERTandGloVeresults Qualitative analysis: In Table 3, in P1, the simi- oftheﬁveNCsinTable1(whichshowsthenatu- larity scores between NC in Table 1 and their re- ralisticsentencesforeachNC,togetherwiththeir respectiveNC andNC )8. Wealsodiscuss spective NCsyn for BERT and GloVe models are syn synW shown. Allofthese indicate that these models cannot distinguish the partial semantic overlap between more composi- tionalNCsandtheircomponentsandtheabsence ofoverlapforidiomaticNCs. Qualitative analysis: The P2 results in Table 3 show the highest similarity scores between each example in Table 1 and one of its components. These high similarity scores highlight the priori- tisationoflexicaloversemanticoverlapmentioned above. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  50compoundswithhighestsim(P1) sharesurface Moreover,contrarytowhatwasexpected,amod- NC-NAT tokens, whethertheNCsaremorecompositional eratecorrelationwasfoundbetweenmostmodels (e.g.,musicjournalistvs. musicreporter)ormore and the idiomaticity scores (P1 in Table 2), indi- idiomatic(e.g.,ghosttownvs. abandonedtown). </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Spearman ρ correlation between naturalistic showed higher variations (e.g., melting pot, with sentencelengthandcosinesimilarity,p≤0.001. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Retroﬁtting contextualized word em- beddings with paraphrases. In Proceedings of the Table 7 includes naturalistic examples in Por- 2019 Conference on Empirical Methods in Natu- tuguese. We include the compositionality scores ral Language Processing and the 9th International provided by the annotators and the BERT and Joint Conference on Natural Language Processing GloVeresultsatNClevel. </Extractive Summary>  </Table ID = 7>  </Paper ID = 310> 

<Paper ID = 311>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thisworkcanpotentiallyencouragethe 3 Methodology developmentofmorechallengingbenchmarksthat evaluateMRCmodelswithrespecttoNLUcapabil- Foreachofthesevenidentiﬁedskills,wedeﬁned itiesthatrequirediscourserelationsunderstanding. an ablation method, as shown in Table 1. The 2 RequisiteSkills Skill NLUTested AblationMethod Asmentionedabove, weidentiﬁedasetofseven shufﬂingmethod reasoning-relatedskillsthatrequiretheunderstand- s Explicitdiscourse Shufﬂe the order 1 ingofexplicitdiscourserelations,asshowninTa- relations between of the sentences ble1. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  reasoningcapabilitiesasthebadperformancecan stemfrommanydifferentfactors(e.g.,distribution 4.2 ResultsandDiscussion shiftinducedbydroppingnumerouswords). Inthissection,wereporttheresultsfortheskills 4 Experiments in Table 2. In this table, for each of the abla- tion method used for skills s to s , there are 2 7 In this section, we describe our experimental set- two versions of experimental results, shown in tings, present the results of our experiments and thewhiteandshadedareas, respectively. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Senses and their associated explicit connec- tivesannotatedinthePDTB3.0corpus(Webberetal., 2019). </Abstractive Summary>  <Extractive Summary> =  The ﬁrst thesenseswithonlyexclusiveexplicitconnectives metric “uniqueness” measures the number of could meet the three metrics, they might not be unique explicit connectives in each sense. For enoughforourdataablationpurposes,asmostof instance, as can be seen from Table 4, the sense theexplicitconnectiveswereeliminated. Consider- “Contingency.Condition.Arg1-as-cond”wasanno- ingthis,weneedtoﬁndabalancebetweenpreserv- tatedfortwouniqueexplicitconnectives: “and(22), ingthenumberandtypesofexplicitconnectivesin then(1)”. Subsequently, we averaged these Weproposethatthetwometricscanreﬂectthe highestvaluesandobtainedthethreshold,whichis breadth and importance of these senses in a pas- about69%. Finally,wechoseexplicitconnectives sage of text as Table 4 was developed from the wherethehighestproportionofthesenseforwhich large-scalePDTB3.0corpus(Webberetal.,2019), theywereannotatedexceeds69%andeliminated whichprovidesacertaindegreeofrepresentation. thosebelowthethreshold. </Extractive Summary>  </Table ID = 4>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (b) (a) Figure 3: Number of unique explicit connectives and total number of explicit connectives for which each sense (processed)wasannotated(sortedfromlargesttosmallest). 3575“exclusiveness”isretainedtoacertainextent:3 F TheSixIdentiﬁedSensesandTheir AssociatedExplicitConnectives • Sense2: Temporal.Asynchronous.Precedence Table 7 shows the six identiﬁed senses and their associatedexplicitconnectives. Foreachsense,the • Sense3: associatedexplicitconnectiveswereselectedusing Temporal.Asynchronous.Succession thethreshold-basedmethoddetailedinAppendix • Sense4: B. </Extractive Summary>  </Table ID = 7>  <Table ID = 5>  <Abstractive Summary> =  Table 5: The hyperparameters used to ﬁne-tune the BERT-base and ALBERT-xxlarge model on each dataset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Results (%) on the development set of SQuAD 2.0 for subsets with normal (Has-Ans) and no-answer (No-Ans)questions. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  </Paper ID = 311> 

<Paper ID = 312>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Human annotation statistics (in percentage of cases, out of the total number of 96 instances) on the MBTI-MTurkdataset(HolidaysandHobbies)andthesubsetoftheMBTI-Twitterdataset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: An MBTI-Twitter instance which did not contain sufﬁcient signals and could not be annotated (left) as opposed to another MBTI-Twitter instance which was correctly annotated by both annotators across all four dimensions(right). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  </Paper ID = 312> 

<Paper ID = 313>  <Table ID = 5>  <Abstractive Summary> =  Table 5: A generated example from SM dataset. </Abstractive Summary>  <Extractive Summary> =  Yk is formed after we go through all the t’s. By For example in Table 5, "Group Consolidation" insertingYk intoXk,weobtainthenextsequence may be split into "handling Group s project / Xk+1. The iterative process stops when all the Consolidation". </Extractive Summary>  </Table ID = 5>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Summary of the datasets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 313> 

<Paper ID = 314>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  and offer concrete insights on how best to guide themeta-learningprocesswhenmultipletasksare Algorithm1OurMeta-learningApproach inthepicture. Input: D set of TLPs for meta training train 3 Methodology (AlsoD forparametrisedsampling) dev SamplingStrategy(Temperature/Mul- Our setting is pivoted on a grid of tasks and tiDDS) languages (with some missing entries as shown Output: Theconvergedmulti-taskmultilingual in Table 1). Each row of the grid corresponds modelparametersθ∗ to a single task. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Detailedresultsoftemperaturebasedheuristicsamplingfordifferentselectionssettings. </Abstractive Summary>  <Extractive Summary> =  Since the 5 ResultsandAnalysis datasetsizevariesacrosstasks(asalsoacrosslan- guages),weusetemperaturesamplingwithineach Table2presentsallourmainresultscomparingdif- setting for τ = 1, 2, 5 and ∞. (In Table 4 of the ferentdataselectionandsamplingstrategiesused AppendixCinthesupplementarymaterial,were- for meta-learning. Each column corresponds to portresultsfordifferentchoicesofTLPselection a target TLP; the best-performing meta-learned anddifferentvaluesofthetemperature.) models for each target TLP within each data se- WithrespecttotheInputinAlgorithm1,there lection setting have been highlighted in colour. </Extractive Summary>  </Table ID = 4>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Main results comparing different data selection and sampling strategies. </Abstractive Summary>  <Extractive Summary> =  We see the oppo- Investigating Sampling Strategies. In Table 2, site seems to hold for POS and NER where the allthescoresshownfortheTempsamplingstrat- Lang-Limited models are almost always bet- egy are the best scores across four different val- terthantheTask-Limitedmodels. WithPOS ues of T, T = 1,2,5,∞. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  compareagainstTask-Limited MTLandAll Acknowledgements TLPs MTLthataremorecompetitive. An interesting observation from the zero shot Wethankanonymousreviewersforprovidingcon- results in Table 3 is that for every external lan- structive feedback. We are grateful to IBM Re- guage, on the ‘shallower’ NER and POS tasks, search,India(speciﬁcallytheIBMAIHorizonNet- theTask-Limitedvariantofmeta-learningper- works - IIT Bombay initiative) for their support forms better than both the variants of MTL, viz., andsponsorship. </Extractive Summary>  </Table ID = 3>  </Paper ID = 314> 

<Paper ID = 315>  </Paper ID = 315> 

<Paper ID = 316>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Last, we outperform Experimental setup We conduct our experi- the BERT version of LPAQA by more than 4 base mentsontheLAMAdataset(Petronietal.,2019; points. Jiang et al., 2020), a recently introduced unsu- Table 2 presents example rewrites that are out- pervisedknowledge-extractionbenchmarkforpre- put by our model. It can be seen that rewrites trainedLMs. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  queries,allextractedfromWikidata. Fortrainingourmodel,weuseaseparatetrain- ingset,createdbyJiangetal.(2020),calledT-REx- Ablation Study In Table 3 we present P@1 re- train. ThisdatasetisconstructedfromWikidataand sultsontheT-RExtestsetafterablatingdifferent hasnooverlapwiththeoriginalT-RExdataset. </Extractive Summary>  </Table ID = 3>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (2019) and report precision at one (P@1)macro-averagedoverrelations(weﬁrstav- Part Of Speech Analysis To better understand erage within relations and then across relations). whattypesofchangesourrewriterperforms,Table As shown in Table 1, BERTese outperforms all 4showsthedistributionoverpart-of-speech-tagsre- three baselines. Compared to the zero-shot set- placedbytherewriter. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Part-of-speech analysis of rewrites from the We thank Yuval Kirstain for his contribution in T-RExtest-set. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 316> 

<Paper ID = 317>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Foreachsen- baselines(likeGPT(Radfordetal.,2018))without tencepair(s ,s ),wepassthemthroughourmodel 1 2 oureventembeddings(denotedas“w/o”). Results and obtain their respective h , given by vectors C in Table 2a indicate that a simple enhancement (u,v). We concatenate these vectors (u,v) with procedureatthepenultimatestepcanoffersigniﬁ- the element-wise difference |u − v| and feed to cantperformancegains. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Wereport BERT-large 68.7 67.9 the model’s accuracy in assigning a higher simi- (a)SocialIQADataset larityscoreforsimilarpairsthandissimilarpairs. Table 2: Accuracy scores (%) of different models on: Table 1b shows that our model outperforms the (a)SocialIQAdev&testset,and(b)TwitterURLPara- state-of-the-artmethodforthistask. phrasing corpus, TwitterPPDB. </Extractive Summary>  </Table ID = 1>  </Paper ID = 317> 

<Paper ID = 318>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Number of positive and negative seed rules cluding3,048trainingand800testsentences. Table 11: Micro F1 performance on each develop- training epoch, we use the default setting, 5, for ment data set. Table 13: Manually selected seeding rules for BC5CDR (Disease) dataset. </Abstractive Summary>  <Extractive Summary> =  rules. Numbersofbothpositiveandnegativeseed- ing rules used in our experiments are shown in SwellShark (Friesetal.,2017)isanextension Table 1.4 Our manually selected seed rules are ofSnorkelthatwasdevelopedforbiomedicalNER. SameasSnorkel,itusesanaiveBayesgenerative 3In our experiments, we used the linked rules from model to denoise manual labeling rules. exceptthatSurfaceFormrulesusesalistofstop- words as negative seeding rules. For some types ofseedingrules,thereareonlyafewavailablethat Discriminative Model Table 10 presents the are not good enough for training the propagation hyperparameter conﬁguration for training dis- model. Therefore, for the rule types that do not criminative model (BiLSTM-CRF). Allmodelshave∼110M portthemanuallyselectedseedingrulesforNCBI, parameters. BC5CDR-Disease,BC5CDR-Chemical,andLap- B PerformanceonDevelopmentData topReview in Table 12, Table 13, Table 14, and Table15,respectively. In Table 11, we present the performance of the LinkedHMM model on developement sets, with 3647Rules NCBI Surface pos - neg - Suffix pos *skott,*drich,*umour,*axia,*iridia neg *ness,*nant,*tion,*ting,*enesis,*riant,*tein,*sion,*osis,*lity Prefix pos carc*,myot*,tela*,ovari*,atax*,carcin*,dystro* neg deﬁ*,comp*,fami*,poly*,chro*,prot*,enzym*,sever*,develo*,varian* exclusivePreNgram pos suffer from *,fraction of *,pathogenesis of *,cause severe * neg -pron *,suggest that *,- cell *,presence of *,expression of *,majority of * loss of *,associated with *,impair in *,cause of *,defect in *,family with * inclusivePreNgram pos breast and ovarian *,x - link *,breast and *,stage iii *,myotonic * neg enzyme *,primary *,non - *, exclusivePostNgram pos neg * and the,* cell line,* in the inclusivePostNgram pos * - t,* cell carcinoma,* muscular dystrophy,* ’s disease,* carcinoma,* dystrophy neg * muscle,* ataxia,* ’system,* defect,* other cancer,* i,* ii DepRule pos FirstTokenDep:amod LastToken:dystrophy,FirstTokenDep:punct LastToken:telangiectasia, FirstTokenDep:compound HeadSurf:t,FirstTokenDep:amod LastToken:dysplasia SecondLastTokenDep:compound LastToken:syndrome, neg FirstTokenDep:amod LastToken:deﬁcienc,FirstTokenDep:amod LastToken:deﬁciency FirstTokenDep:amod LastToken:defect,FirstTokenDep:pobj LastToken:cancer SecondLastTokenDep:compound LastToken:cancer, SecondLastTokenDep:compound LastToken:disease SecondLastTokenDep:appos LastToken:t,SecondLastTokenDep:compound LastToken:t Table12: ManuallyselectedseedingrulesforNCBIdataset. Allmodelshave∼110M portthemanuallyselectedseedingrulesforNCBI, parameters. BC5CDR-Disease,BC5CDR-Chemical,andLap- B PerformanceonDevelopmentData topReview in Table 12, Table 13, Table 14, and Table15,respectively. In Table 11, we present the performance of the LinkedHMM model on developement sets, with 3647Rules NCBI Surface pos - neg - Suffix pos *skott,*drich,*umour,*axia,*iridia neg *ness,*nant,*tion,*ting,*enesis,*riant,*tein,*sion,*osis,*lity Prefix pos carc*,myot*,tela*,ovari*,atax*,carcin*,dystro* neg deﬁ*,comp*,fami*,poly*,chro*,prot*,enzym*,sever*,develo*,varian* exclusivePreNgram pos suffer from *,fraction of *,pathogenesis of *,cause severe * neg -pron *,suggest that *,- cell *,presence of *,expression of *,majority of * loss of *,associated with *,impair in *,cause of *,defect in *,family with * inclusivePreNgram pos breast and ovarian *,x - link *,breast and *,stage iii *,myotonic * neg enzyme *,primary *,non - *, exclusivePostNgram pos neg * and the,* cell line,* in the inclusivePostNgram pos * - t,* cell carcinoma,* muscular dystrophy,* ’s disease,* carcinoma,* dystrophy neg * muscle,* ataxia,* ’system,* defect,* other cancer,* i,* ii DepRule pos FirstTokenDep:amod LastToken:dystrophy,FirstTokenDep:punct LastToken:telangiectasia, FirstTokenDep:compound HeadSurf:t,FirstTokenDep:amod LastToken:dysplasia SecondLastTokenDep:compound LastToken:syndrome, neg FirstTokenDep:amod LastToken:deﬁcienc,FirstTokenDep:amod LastToken:deﬁciency FirstTokenDep:amod LastToken:defect,FirstTokenDep:pobj LastToken:cancer SecondLastTokenDep:compound LastToken:cancer, SecondLastTokenDep:compound LastToken:disease SecondLastTokenDep:appos LastToken:t,SecondLastTokenDep:compound LastToken:t Table12: ManuallyselectedseedingrulesforNCBIdataset. Allmodelshave∼110M portthemanuallyselectedseedingrulesforNCBI, parameters. BC5CDR-Disease,BC5CDR-Chemical,andLap- B PerformanceonDevelopmentData topReview in Table 12, Table 13, Table 14, and Table15,respectively. In Table 11, we present the performance of the LinkedHMM model on developement sets, with 3647Rules NCBI Surface pos - neg - Suffix pos *skott,*drich,*umour,*axia,*iridia neg *ness,*nant,*tion,*ting,*enesis,*riant,*tein,*sion,*osis,*lity Prefix pos carc*,myot*,tela*,ovari*,atax*,carcin*,dystro* neg deﬁ*,comp*,fami*,poly*,chro*,prot*,enzym*,sever*,develo*,varian* exclusivePreNgram pos suffer from *,fraction of *,pathogenesis of *,cause severe * neg -pron *,suggest that *,- cell *,presence of *,expression of *,majority of * loss of *,associated with *,impair in *,cause of *,defect in *,family with * inclusivePreNgram pos breast and ovarian *,x - link *,breast and *,stage iii *,myotonic * neg enzyme *,primary *,non - *, exclusivePostNgram pos neg * and the,* cell line,* in the inclusivePostNgram pos * - t,* cell carcinoma,* muscular dystrophy,* ’s disease,* carcinoma,* dystrophy neg * muscle,* ataxia,* ’system,* defect,* other cancer,* i,* ii DepRule pos FirstTokenDep:amod LastToken:dystrophy,FirstTokenDep:punct LastToken:telangiectasia, FirstTokenDep:compound HeadSurf:t,FirstTokenDep:amod LastToken:dysplasia SecondLastTokenDep:compound LastToken:syndrome, neg FirstTokenDep:amod LastToken:deﬁcienc,FirstTokenDep:amod LastToken:deﬁciency FirstTokenDep:amod LastToken:defect,FirstTokenDep:pobj LastToken:cancer SecondLastTokenDep:compound LastToken:cancer, SecondLastTokenDep:compound LastToken:disease SecondLastTokenDep:appos LastToken:t,SecondLastTokenDep:compound LastToken:t Table12: ManuallyselectedseedingrulesforNCBIdataset. BC5CDR-Disease,BC5CDR-Chemical,andLap- B PerformanceonDevelopmentData topReview in Table 12, Table 13, Table 14, and Table15,respectively. In Table 11, we present the performance of the LinkedHMM model on developement sets, with 3647Rules NCBI Surface pos - neg - Suffix pos *skott,*drich,*umour,*axia,*iridia neg *ness,*nant,*tion,*ting,*enesis,*riant,*tein,*sion,*osis,*lity Prefix pos carc*,myot*,tela*,ovari*,atax*,carcin*,dystro* neg deﬁ*,comp*,fami*,poly*,chro*,prot*,enzym*,sever*,develo*,varian* exclusivePreNgram pos suffer from *,fraction of *,pathogenesis of *,cause severe * neg -pron *,suggest that *,- cell *,presence of *,expression of *,majority of * loss of *,associated with *,impair in *,cause of *,defect in *,family with * inclusivePreNgram pos breast and ovarian *,x - link *,breast and *,stage iii *,myotonic * neg enzyme *,primary *,non - *, exclusivePostNgram pos neg * and the,* cell line,* in the inclusivePostNgram pos * - t,* cell carcinoma,* muscular dystrophy,* ’s disease,* carcinoma,* dystrophy neg * muscle,* ataxia,* ’system,* defect,* other cancer,* i,* ii DepRule pos FirstTokenDep:amod LastToken:dystrophy,FirstTokenDep:punct LastToken:telangiectasia, FirstTokenDep:compound HeadSurf:t,FirstTokenDep:amod LastToken:dysplasia SecondLastTokenDep:compound LastToken:syndrome, neg FirstTokenDep:amod LastToken:deﬁcienc,FirstTokenDep:amod LastToken:deﬁciency FirstTokenDep:amod LastToken:defect,FirstTokenDep:pobj LastToken:cancer SecondLastTokenDep:compound LastToken:cancer, SecondLastTokenDep:compound LastToken:disease SecondLastTokenDep:appos LastToken:t,SecondLastTokenDep:compound LastToken:t Table12: ManuallyselectedseedingrulesforNCBIdataset. “-”meansnoseedingrulesselected,andweonlyuse therulesprovidedinbaseline. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Micro F1 performance of baselines and our method on test sets. </Abstractive Summary>  <Extractive Summary> =  includedinAppendixC. 4.1 ResultswithAbundantSeedingRules Inthissubsection,wereportboththeresultofour 3.4 ImplementationDetails generative model with augmented rules in Table 2andtheresultofourdiscriminativeNERmodel In our experiments, we create a graph for each thatistrainedusingweaklabelsfromthegenerative typeofrulesforeachdatasetandlearnnewrules modelinTable3. independently with the same setup. thatourdiscriminativemodelperformsslightlybet- ter on NCBI and Laptop, but worse on BC5CDR NCBI CDR Laptop ∆ thanthegenerativemodel,whichisconsistentwith LinkedHMM 78.7 85.9 68.2 thatfrom(Safranchiketal.,2020). GLARA +DepRule 79.3 - - +0.6 4.2 ResultswithLimitedSeedingRules +PostNgram - 86.0 - +0.1 Though weakly supervised state-of-the-art meth- +PreNgram 79.6 86.2 69.3 +0.5 ods reported in Table 2 achieved relatively good +Preﬁx - - 70.8 +1.5 performance,theyrequireasigniﬁcantamountof +Sufﬁx 79.8 86.3 71.7 +0.4 manualeffortfromdomainexpertsfordesigning +Surface 80.2 - 72.2 +0.5 andtuninglabelingrules. Well-performingmeth- odsthatrequirelessmanualeffortareoftenmore Table 4: Impact of each type of rules used in our gen- desirable. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Results of discriminative models using weak labelsgeneratedbydifferentmethods. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Impact of each type of rules used in our gen- desirable. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Results of our generative model using aug- most effective on the Laptop dataset, producing mented rules learned by our graph neural network +1.5F1pointimprovement. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  “ ”denotesthespacecharacter. 4.5 QualityAnalysisofLearnedNewRules ∗demiaislearnedﬁrst,thenlearning∗edemiarule willnothelpbecausetheentitiesmatchedbythese In Table 6, we present top 5 learned rules for tworuleshavelargeoverlaps. Second,someofthe each rule type that we automatically learned on rules learned from unlabeled data may not be ap- the NCBI data set. </Extractive Summary>  </Table ID = 6>  </Paper ID = 318> 

<Paper ID = 319>  <Table ID = 2>  <Abstractive Summary> =  Table 2: DocREDdataset splitused forend-to-end re- extractiondatasettodate(6entityand96relation lationextraction. </Abstractive Summary>  <Extractive Summary> =  There- different types. Table 2 contains statistics of our fore,weevaluateJEREXbothasarelationclassi- end-to-endsplit. Wereleasethesplitasareference ﬁer(tocompareitwiththestate-of-the-art)andas forfuturework. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Single-task performance of the joint model CorefRo(Yeetal.,2020)∗ 57.90 60.25 (left) and separate models (right) on the end-to-end JEREX(MRC)∗ 58.44 60.40 split (∗ joint results are for MRC except for the last row). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Ablation studies for the multi-level relation model also outperforms complex methods based classiﬁer(MRC)usingtheend-to-endsplit. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 319> 

<Paper ID = 320>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (2018)’s approach estimates WER directly intotrain,devandtestsetsforeachdataset. Ascan with a 6 class classiﬁcation model (with classes be seen in Table 1 of Section B of the Appendix, corresponding to 0%,25%,50%,75%,100% and thecharacteristicsofthe100and10hourdatasets 150%). Oncethemodelistrained,thepredictions arequitedifferent. However,withinadataset,the arecalculatedasfollows: train, dev and test sets have similar distributions WER (s) = P (s)·WER (2) of WER and other characteristics. Table 1 lists a Pred softmax fixed few examples with each row having the ground where s is a sample, WER is the predicted Pred text,thetranscriptobtainedfromGoogleCloud’s WER for s, P (s) is the softmax probabil- softmax Speech-to-TextAPI,theTrueWERandtheWER itydistributionoutputbytheclassiﬁcationmodel, predictedbyourproposedmodel. WER = [0,0.25,0.5,0.75,1.0,1.5] is the fixed https://cloud.google.com/speech-to-text. (2018) setting. Since the fectivenessofWER-BERTisparticularlyevident modelencountersheavilyimbalancedlabelsinthe in Table 1 where we see that it is able to predict later setting, its predictions also reﬂect the same. WER which is very close to the True WER. </Extractive Summary>  </Table ID = 1>  </Paper ID = 320> 

<Paper ID = 321>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  TRE (Alt et al., 2019) and Sent+KG (Dai et al., AUC and P@N Evaluation. Table 2 further 2019). The results shown in Figure 7 and Ta- presents the results in terms of AUC and P@N. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Some examples of attention distri- manualanalysissoastobettertheefﬁciency;and bution over paths from “Sent+UG” (Base) and “Sent+UG+Ranking+Pretrain” (Prop.), where (cid:51)(or (cid:55)) (2) instead of random walk, we may collect UG represents the correct (or incorrect) prediction of the pathsbyadoptingmoresophisticatedmechanisms targetrelation. </Abstractive Summary>  <Extractive Summary> =  Case Study. Table 4 shows the UG path examples that are scored with highest (“High”) Acknowledgement or lowest (or lower than 1.0 × 10−3) (“Low”) attention by the base model and our proposed This work was supported by JST CREST Grant framework. The paths in the table generally Number JPMJCR1513, Japan and KAKENHI mean “Beta-2... </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The main reasons to manually restrict i t t i theentitytypeisbecause(1)weobservethatmost hsent = tanh(Wsentzsent+bsent), of the Medline abstracts discuss the relationship t t amongtheseentitytypes;(2)theseconcreteentities hpath = tanh(Wpathzpath+bpath), t t could prevent semantic drift while searching UG zstent = [vts−en(kt−1)/2;...;vts+en(kt−1)/2], paths. zpath = [vpath ;...;vpath ] t t−(k−1)/2 t+(k−1)/2 A.2 ParameterSettings Allofthehyperparametersusedinourexperiments are listed in Table 5. Most of them follow the hyperparameter setting in (Dai et al., 2019) and (Han et al., 2018a). </Extractive Summary>  </Table ID = 5>  </Paper ID = 321> 

<Paper ID = 322>  <Table ID = 1>  <Abstractive Summary> =  Table 1: An example of APE triplets from the WMT proaches have become prevalent also in the dataset(Bojaretal.,2017). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Detaileddatastatisticsare ditioned on a pair of src and ref. The output presented in Table 2. We tokenized all words in mt then composes a new synthetic triplet (src, ourdatasetsintosub-wordunitsbyusingSenten- BG 3687PBSMT NMT MODELS Test16 Test17 Test18 Avg. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Statistics of synthetic mt produced by each v proposedscheme. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 322> 

<Paper ID = 323>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  avoidcollectingtheobjects’compoundwords. For 3.3 ComparisonwiththeState-of-the-Art eachobject,weselectedsentenceswhereinfewer Results thantwowordswereinbetweentheobjectandits Table 2 lists the results of our model compared dependentadjectivetopickupthesentenceslikely to describe the object in detail. We used spaCy7 withthepreviousstate-of-the-artresults. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  3.4 AblationStudy hancesperformance. The degraded performance of w/o image sug- Table 3 lists the results of our model obtained in geststhatobjectlabelsthemselvesareinsufﬁcient theablationstudies. Wetestedtheablationofthe todescribeimagescorrectly. </Extractive Summary>  </Table ID = 3>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Wedeﬁne other for object labels and the other words12. Table 6 theprecision(P),recall(R)andF1score(F)ofS againstTm asfollows: P = |S∩Tm|,R = |S∩Tm|, presentstheresults. Incontrasttoobjectlabels,our |S| |Tm| outputs’vocabularyisaboutﬁvetimessmallerthan F = 2· P·R . </Extractive Summary>  </Table ID = 6>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Forexample,ageneralcaption Tm wereexcludedfromthecalculation. ∗ such as “a man with a bike” can correctly cover Table 5 shows the results. Overall, the scores various scenes in which a man is riding/sitting ondetectedobjectlabels(Detected)areabouttwo on/leaningon/standingnear/... </Extractive Summary>  </Table ID = 5>  </Paper ID = 323> 

<Paper ID = 324>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Statistics of performance tensors for four tasks. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 324> 

<Paper ID = 325>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Cohen Kappa Agreement for Metrical Stress andFootBoundaries.Corr.istheagreementoftheﬁrst Table 1: Cohen Kappa Agreement for Main Accents versionagainstthecorrectedversion. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Cohen Kappa Agreement for Main Accents versionagainstthecorrectedversion. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Size of manually annotated corpora. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We use the gold annotation ter boundaries, while syllable count is the more of the TIGER corpus (modern newspaper), and important ﬁgure to determine the proper length pre-tagged sentences from DTA, including anno- of a line. As seen in Table 4, the BiLSTM-CRF tatedpoetry(Lyrik),ﬁction(Belletristik)andnews (Zeitung).14 TheSTTStagsetisused. Wetrainand 8Syllabipydeterminesboundariesbasedonthesonority testConditionalRandomFields(CRF)15 todeter- principle, Pyphen uses the Hunspell dictionaries, and Hy- pheNNisasimplefeedforwardnetworkthatistrainedon mine a robust POS model.16 See Table 7 for an characterwindows. </Extractive Summary>  </Table ID = 4>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  As seen in Table 4, the BiLSTM-CRF tatedpoetry(Lyrik),ﬁction(Belletristik)andnews (Zeitung).14 TheSTTStagsetisused. Wetrainand 8Syllabipydeterminesboundariesbasedonthesonority testConditionalRandomFields(CRF)15 todeter- principle, Pyphen uses the Hunspell dictionaries, and Hy- pheNNisasimplefeedforwardnetworkthatistrainedon mine a robust POS model.16 See Table 7 for an characterwindows. overview of the cross-genre evaluation. </Extractive Summary>  </Table ID = 7>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  However,FORBdoesnotcontain thistomonosyllabicwords,aspolysyllabicwords readily available foot boundaries, and in PROS typicallyhavealexicalstresscontour. Theresult footboundariesareoccasionallysetaftereachsyl- is a hierarchy of stress that we report in Table 6. lable.20 Table5showsthenumberoflinesineach Attheendsofthespectrum,weseethatnounsare 17https://nlp.stanford.edu/software/ usuallystressed,whilearticlesareseldomstressed. </Extractive Summary>  </Table ID = 6>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Learningthesequenceofmetricalsyllablestress First, we trained a single task model for each withBERTcannotcompeteourothermodels,pos- annotationlayer,thenalltasksjointly(+all),and siblyresultingfromanimpropersyllablerepresen- ﬁnally pair-wise combinations (+<auxiliary tation,astheword-piecetokenizersegmentsword task>). In Table 9, we report the accuracy on chunksotherthansyllables. syllablelevelforeachmaintaskwiththeirrespec- We also experiment with framing the task as tiveauxiliarytasks. </Extractive Summary>  </Table ID = 9>  </Paper ID = 325> 

<Paper ID = 326>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Indetail,whenleveragingword dependencies,werunexperimentsonourproposed model to explore the effect of learning from dif- 3.3 ImplementationDetails ferent parts of the input, i.e., we try word depen- denciesfromthreesources: only, only, and We adopt BERT-base-uncased and BERT-large- X A uncased7 astheencodersinourapproach,which both and (see§2.2). Experimentalresultsare X A reported in Table 2 with the preﬁxes of KVMN aredemonstratedtobethemosteffectiveencoders for many NLP tasks (Strakova´ et al., 2019; Bal- denotingwhichpartisencodedfrom. dini Soares et al., 2019; Xu et al., 2019). </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  fromthesentence-levelsentiment. Particularly,wetrysecond-andthird-orderword dependencies and compare their results with the 4.3 ComparisonwithPreviousStudies previousﬁrst-orderones.Theresultsonalldatasets, aswellasaveragecoverage(%)ofwordsineach To further demonstrate the effectiveness of sentencewithrespecttodifferentdependencyor- our approach, we compare our best-performing ders,9 are reported in Table 3, where (a) and (b) model,i.e.,theBERT-largeencoderwithsecond- show the results of models with BERT-base and orderworddependenciesincorporatedthrough - A BERT-large encoders, respectively. From the re- KVMN, with previous studies, where the com- sults,itisfoundthatinmostcases(e.g.,forboth parisons on all datasets are reported in Table 4, baseandlargeBERT),modelsusingsecond-order wheretheresultsofBERT-largebaseline,aswell worddependenciesachievetheoverallhighestper- astheonesusingBERT-base,arealsoreportedfor references. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Particularly,wetrysecond-andthird-orderword dependencies and compare their results with the 4.3 ComparisonwithPreviousStudies previousﬁrst-orderones.Theresultsonalldatasets, aswellasaveragecoverage(%)ofwordsineach To further demonstrate the effectiveness of sentencewithrespecttodifferentdependencyor- our approach, we compare our best-performing ders,9 are reported in Table 3, where (a) and (b) model,i.e.,theBERT-largeencoderwithsecond- show the results of models with BERT-base and orderworddependenciesincorporatedthrough - A BERT-large encoders, respectively. From the re- KVMN, with previous studies, where the com- sults,itisfoundthatinmostcases(e.g.,forboth parisons on all datasets are reported in Table 4, baseandlargeBERT),modelsusingsecond-order wheretheresultsofBERT-largebaseline,aswell worddependenciesachievetheoverallhighestper- astheonesusingBERT-base,arealsoreportedfor references. Itisobservedthat, ourmodelconsis- 9Thismetricisusedtopresenthowmanywordsineach tentlyoutperformstheBERT-largebaselineonall inputsentenceareinvolvedwhendifferentordersareapplied datasets and achieves state-of-the-art on three of forextractingworddependencies,soastoillustratehowmuch informationinasentenceishelpfulforASA. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Results on ﬁve datasets from our full models (base and large BERT with -KVMN and second-order A worddependencies)anditsvariantswherekeys(“ KEYS”)aTnhde v faalluaefesl(w“as raVthAeLr U oEvSe”r)coaorkeeda b laantde dd.rie d bruetf …ers fitnoe     thedropofaccuracyandF1scorewhenkeysorvaluesareexcludedfromthefullmodel. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In Proceedings of the 27th International Con- ference on Computational Linguistics, pages 1077– 1087. Appendix A.ModelSizeandRunningSpeed Table 6 reports the number of trainable parame- ters and inference speed (sentences/second)10 of baseline(i.e.,theoneswithoutusingKVMNand thedependencyinformation)andourbestperform- ingmodels(i.e.,theoneswith -KVMNandthe A second-orderdependencies)onalldatasets. B.Hyper-parameterSettings Table7reportsthehyper-parametersweusedfor tuningourmodels. </Extractive Summary>  </Table ID = 6>  </Paper ID = 326> 

<Paper ID = 327>  </Paper ID = 327> 

<Paper ID = 328>  </Paper ID = 328> 

<Paper ID = 329>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Comparison between reported performance completelymodular: theresearcherisabletopick andreproducedperformanceonOpenSQuAD. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Open SQuAD performance using Wikipedia dumps from different years. </Abstractive Summary>  <Extractive Summary> =  Inthereadertrain- Weconductthelastexperimenttotesttherobust- ing stage, we train the model using BERT-large- nessofthestate-of-the-artsystemagainsttemporal cased model, also with global normalization to shifting. Results are reported in Table 3. We ob- makethespanscorecomparable. </Extractive Summary>  </Table ID = 3>  </Paper ID = 329> 

<Paper ID = 330>  </Paper ID = 330> 

<Paper ID = 331>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  4.2 RelationExtraction Table4: Statisticsofrecognizedentities. As shown in Table 3, we extracted 40.5 million Model /corpus /abstract relations including 29.8 million unique relations. en ner craft md 1.8M 6 Among the relation extraction methods, OpenIE en ner jnlpba md 3.1M 11 outputsthelargestnumber. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  4.3 EntityRecognition KennethWardChurchandPatrickHanks.1990. Word As shown in Table 4, a total of 6.4M entities associationnorms,mutualinformation,andlexicog- were recognized from the corpus with the four raphy. ComputationalLinguistics,16(1):22–29. </Extractive Summary>  </Table ID = 4>  </Paper ID = 331> 

<Paper ID = 332>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  This record the time needed for ES and US to com- complexityiscomparablewiththoseofpublicdi- plete the tasks. Table 2 describes and compares alogue datasets, like Multiwoz or Taskmaster-1 the time taken on the admin task for the two su- (Budzianowskietal.,2018;Byrneetal.,2019). pervisorsacrossthethreelanguagesconsidered. </Extractive Summary>  </Table ID = 2>  </Paper ID = 332> 

<Paper ID = 333>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Generalizability of BERT SHARED-NORM swers. </Abstractive Summary>  <Extractive Summary> =  Theencoderinputisasinglesentence differentcombinationsofthesedatasets. and the decoder output is a question, where the Table 2 shows the results of this experi- inputsentencecontainstheanswertothequestion. ment. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  It’snotclearwhatthisquestion sults for each QG model in terms of answer F1 isasking. accuracy are shown in Table 3, compared along- 3: Questionisstrangelyworded,vague,orcon- sidetheresultforhuman-authoredquestions. tains errors. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Overalltheseresultsagainshowthe questions corresponding to 175 inputs. Table 5 beneﬁtofaugmentingthetrainingdatawithauto- in the appendix shows examples of these items. matically generated questions. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Accordingly, our Participantsreadtheinputsentenceinitsparagraph democurrentlyrunstheAUGMENTEDmodel. 444 GeneratingQ&APairs Table 6 shows an example of a generated Q&A list for one text. We conducted an evaluation of We combined our best-performing QG and QA the informativeness of these pairs with 38 AMT modelsintoasystemthattakesatextasinputand participants. </Extractive Summary>  </Table ID = 6>  </Paper ID = 333> 

<Paper ID = 334>  <Table ID = 2>  <Abstractive Summary> =  Table 2: In-domain type-aware F1 score for test set Weconductthreeexperimentsontheninedatasets on each dataset with current SoTA. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Type-ignored F1 score in cross-domain setting over non-lower-cased English datasets. </Abstractive Summary>  <Extractive Summary> =  We also andXLM-RLARGE modelsalongwithcurrentstate- report the accuracy of the same XLM-R model of-the-art(SoTA).Onecanconﬁrmthatourframe- trainedoveracombineddatasetresultingfromcon- work with XLM-RLARGE achieves a comparable catenationofalltheabovedatasets. SoTAscore,evensurpassingitintheWNUT2017 In Table 3, we present the type-ignored F1 re- dataset. Ingeneral,XLM-RLARGE performsconsis- sultsacrossdatasets. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Cross-lingual type-aware F1 results on vari- thealphabet. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Cross-lingual type-aware F1 score over WikiAnndatasetwithXLM-R . </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 334> 

<Paper ID = 335>  </Paper ID = 335> 

<Paper ID = 336>  </Paper ID = 336> 

<Paper ID = 337>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  of the tasks, particularly for sentence segmenta- tion (+3.24%), POS tagging (+1.44% for UPOS and +1.55% for XPOS), morphological tagging ModelPackage Trankit Stanza (+1.46%), and dependency parsing (+4.0% for MultilingualTransformer 1146.9MB - UASand+5.01%forLAS)whilemaintainingthe Arabic 38.6MB 393.9MB Chinese 40.6MB 225.2MB competitive performance on tokenization, multi- English 47.9MB 383.5MB wordexpansion,andlemmatization. French 39.6MB 561.9MB Spanish 37.3MB 556.1MB 5.3 NERresults Totalsize 1350.9MB 2120.6MB Table 3 compares Trankit with Stanza (v1.1.1), Table5: Modelsizesforfivelanguages. Flair (v0.7), and spaCy (v2.3) on the test sets of 11consideredNERdatasets. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Flair (v0.7), and spaCy (v2.3) on the test sets of 11consideredNERdatasets. FollowingStanza,we reporttheperformanceforothertoolkitswiththeir 5.4 SpeedandMemoryUsage pretrained models on the canonical data splits if Table 4 reports the relative processing time for theyareavailable. Otherwise,theirbestconfigura- UDandNERofthetoolkitscomparedtospaCy’s tionsareusedtotrainthemodelsonthesamedata CPU processing time5. </Extractive Summary>  </Table ID = 4>  </Paper ID = 337> 

<Paper ID = 338>  </Paper ID = 338> 

<Paper ID = 339>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Forbothtasks,we alogisidentiﬁedbyauniqueresourceidentiﬁer employedBidirectionalEncoderRepresentations (URI).Itisconnectedtoamediatornodethatrep- fromTransformers(BERT)(Devlinetal.,2019). resentsthemultiaryrelationassociatedwiththe Table 1 shows some example results of claim entry. For example, Figure 3 shows a question matchingandstancedetection. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Correlation between the percentage of con- willremainatopperformerunderallfourclasses. </Abstractive Summary>  <Extractive Summary> =  Be- tweetsrelativetothetotalnumberoftweetsfrom causeotherrecentstancedetectionmethods(Mo- a country. The Pearson correlation coefﬁcients htarami et al., 2018; Fang et al., 2019) only re- between them are in Table 4. We ﬁnd that the portedmacro-F1scorescalculatedusingallfour numberofmisinformationtweetsmostpositively classesincluding“unrelated”,wecannotreport correlates with the number of conﬁrmed cases. </Extractive Summary>  </Table ID = 4>  </Paper ID = 339> 

<Paper ID = 340>  </Paper ID = 340> 

<Paper ID = 341>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  moresuitablefordeploymentduetoitssmallsize. Theclassiﬁcationresultsappearinatablesothat Table 2 lists performance of SVMs version com- earlierresultscanbereferredto. Werecognizethat paredtousingBERT.WhencomparingtoBERT usersmaywanttoclassifymanytweetsinonego models, we ﬁne-tuned AraBERT(Antoun et al., withouthavingtotypethemoneatatime. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  via POST requests. Table 3 lists available API InProceedingsoftheFifthArabicNaturalLanguage routesandFigure3illustratesexampleusage. Re- ProcessingWorkshop,pages97–110. </Extractive Summary>  </Table ID = 3>  </Paper ID = 341> 

<Paper ID = 342>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Number of concepts linked to ConceptNet rainingcatsanddogs,etc. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 342> 

<Paper ID = 343>  <Table ID = 1>  <Abstractive Summary> =  Table 1: MSA tools implemented in SAFAR 6 Almost all integrated MSA tools have their own license. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: MSA resources implemented in SAFAR Also, a corpus for language identification tasks 3.2 Moroccan Dialect is available with SARAF. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Moroccan dialect resources and tools implemented in SAFAR specifying appropriate parameters according to 3.3 Machine learning models their needs. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 343> 

<Paper ID = 344>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  These examples show- went out to buy groceries. After an hour he got case how the ﬁne-grained analysis enabled by In- up.” (Figure 5b and Table 1). These two exam- terpreT affords a higher level of insight that is ples show how changing a single token (“back” indispensable for interpreting model behavior for became “up”) signiﬁcantly alters the sentence se- complexlanguageunderstandingtasks. </Extractive Summary>  </Table ID = 1>  </Paper ID = 344> 

<Paper ID = 345>  </Paper ID = 345> 

<Paper ID = 346>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 346> 

<Paper ID = 347>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Ascreenshot intotrain/dev/testsplits,weﬁrstcreatechunksof of the output of the system is shown in Figure 1. size 100,000 samples in which all samples of an Moreover, Table 1 shows the glossary extracted acronym are assigned to the same chunk. Since from the text of this paper using the rule-based eachacronymappearsonlyinonechuck,wetrain component of the system. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Fi- precision,recall,andF1scorefortheacronymand 1 2 n nally,theconcatenationofthetextrepresentation, long-formpredictionandalsotheirmacro-averaged i.e., h¯, and the acronym representation, i.e., h , F1 score. The results are shown in Table 2. This a is fed intoa 2-layer feed-forward neural network tableshowsthatourmodeloutperformsbothrule- whose ﬁnal layer dimension is equal to the total based and more advanced feature-based or deep number of long-forms in the dataset (i.e., dataset learningmodels. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Performance of models for acronym disam- biguation(AD) ularExpressions(BADREX)(Gooch,2011)orAb- breviationExpander(ABBREX)(ABBREX2018), unfortunately, they are incapable of acronym dis- andZettlemoyer,2020),DECBAE(Jinetal.,2019) ambiguation. </Abstractive Summary>  <Extractive Summary> =  To our knowledge, the most simi- andGAD(PouranBenVeysehetal.,2020d). The lar work to ours is proposed by Ciosici and As- results are shown in Table 3. This table demon- sent(2018). </Extractive Summary>  </Table ID = 3>  </Paper ID = 347> 

<Paper ID = 348>  </Paper ID = 348> 

<Paper ID = 349>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Scores of single task models on test data for ingcanbeenabledinthehyperparametersconﬁgu- threepopulardatasetsandavarietyoftasks. </Abstractive Summary>  <Extractive Summary> =  Forthesedatasets, EN(Bojaretal.,2014),IWSLT15EN-VI(Cettolo thecombinationofsmoothinganddatasetembed- etal.,2014))usingmBERTasourembeddings.10 dings are the most promising settings. Perhaps Table 2 reports our results on the test sets com- surprisingly, the zero-shot datasets (<1k) have a paredtopreviouswork. ForallUDtasks,wescore higherLASascomparedtothesmalldatasetsand slightlyhigher,whereasforGLUEtaskswescore usingaseparatedecoderbasedontheproxytree- consistentlylowercomparedtothereferences. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Average LAS scores on test splits of UD Table 3: Average results over all development sets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Average results over all development sets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: The scores (accuracy) per dataset on the GLUE tasks (dev) for a variety of multi-task settings (ordered by size, indicated in number of sentences in trainingdata). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 349> 

<Paper ID = 350>  </Paper ID = 350> 

<Paper ID = 351>  </Paper ID = 351> 

<Paper ID = 352>  </Paper ID = 352> 

<Paper ID = 353>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Language coverage per category of the ser­ ready being ingested together with two reposito­ vicestointegrateinELG ries related to ELG, LINDAT/CLARIAH­CZ and ELRA­SHARE­LRs(LRspublishedatLREC). </Abstractive Summary>  <Extractive Summary> =  Our goal is to provide ser­ Ourmaingroupsofusersare: (1)LT/LRproviders vicesofallclassesforallofficialEUlanguagesand – companies or research organisations with tools, forotherEUandnon­EUlanguagesthatareofso­ services or data that can be provided through the cial or strategic interest in the EU. Table 1 shows ELG;(2)Developersandintegrators–companies theoveralllanguagecoverageofeachcategoryof andresearchinstitutionsinterestedinusingLT;(3) servicesacrossallconsortiumpartners;languages General LT information seekers; (4) Stakeholders have been divided into four groups: (A) EU offi­ whowishtoprovideinformationabouteventsetc.; ciallanguages;(B)otherEUlanguageswithoutof­ (5) Casual visitors. We provide three ways of ac­ ficial status, plus languages from candidate coun­ cess: RESTAPIs,webUIs,Pythonpackage. </Extractive Summary>  </Table ID = 1>  </Paper ID = 353> 

<Paper ID = 354>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Evaluation results of rescaled 5-scale scores (%). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 354> 

<Paper ID = 355>  </Paper ID = 355> 

<Paper ID = 356>  </Paper ID = 356> 

<Paper ID = 357>  </Paper ID = 357> 

<Paper ID = 358>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Subse- quently,weremovedthedocumentswithlessthan 5 words for 20 Newsgroups and BBC News and lessthan3wordsfortheotherdatasets. Table 1 reports some statistics about the cur- rentlyavailablepre-processed. Figure1: WorkﬂowoftheOCTISframework Avg# #Unique Dataset Domain #Docs words Theframeworkcanbeusedbothasapythonli- words indocs braryandasadashboard. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  portantly,thehyper-parameterselection. Inthefol- Table 2 summarizes the main features of the 266existingtopicmodelingframeworksandcompares byallowingagivenruntobeexecutedbeforeoth- themwithOCTIS. ers. </Extractive Summary>  </Table ID = 2>  </Paper ID = 358> 

<Paper ID = 359>  </Paper ID = 359> 

<Paper ID = 360>  </Paper ID = 360> 

<Paper ID = 361>  </Paper ID = 361> 

<Paper ID = 362>  </Paper ID = 362> 

<Paper ID = 363>  </Paper ID = 363> 

<Paper ID = 364>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Germanisaninﬂectedlan- ofTrepresentsamoreevendistributionbetween guagei.ethewordorderchangesaccordingtothe thetasks,whenTishighenough,thevalueofλ function in the sentence. Most word orders are j 316 5Model Language TextMode Period Comma Question Overall Spoken 84.8 47.0 47.6 59.6 Hindi Written 89.2 34.7 55.2 59.7 Joint-BILSTMNCRF+FastText Spoken 59.8 40.3 19.4 39.8 Tamil Written 47.2 24.9 14.9 29.0 Spoken 88.7 67.3 41.1 65.6 Hindi Written 92.3 70.8 43.4 68.5 Joint-XLMRobertaNCRF Spoken 75.9 58.7 20.3 56.6 Tamil Written 70.5 43.6 20.3 44.8 Spoken 90.6 66.6 68.8 75.3 Hindi Written 93.7 74.9 59.6 76.1 Joint-MultilingualBERTNCRF Spoken 85.3 71.8 66.6 74.6 Tamil Written 74.8 50.1 43.1 56.0 Table3: Resultsonlowresourcelanguages deﬁnedintermsofﬁniteverb(V),incombination Table 3 . We obtained the best result using the withSubject(S),andobject(O).InGerman, this Joint-Multilingual BERT NCRF model. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Ablation Study on our dataset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 364> 

<Paper ID = 365>  </Paper ID = 365> 

<Paper ID = 366>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Weselectthethree The datasets we used are widely adopted in the tasksforadiverseyetpracticalbenchmarkforpre- NLP community. Quantitative details of datasets trainedmodelswithoutconstrainingthemodelson can be found in Table 2. The selected tasks are sentence-level classiﬁcation tasks. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  There- costbecausetheyvarydramaticallybetweentasks. fore, during model pretraining, after every thou- To simplify the process, we compute the ratio of sand pretraining steps, we use the current pre- BERT ’stimeandcosttothatofeachmodel LARGE trained model for ﬁne-tuning and see if the ﬁne- as the normalized measure, as shown in Table 3 tuned model can reach our cut-off performance. andTable4. extensive experiments on given hardware (RTX Here, our cut-offs were selected by observing 2080Ti GPU) with different model settings as therecentstate-of-the-artmodel’sperformanceon the selected dataset for the task4. A wise choice shown in Table 3 and Table 4. We also collect thedevelopmentsetperformancewithtimeinﬁne- wouldbechoosingtheperformanceofsomeclassic tuningtoinvestigatehowthemodelisﬁne-tuned methodslikeLSTM-CRForBi-LSTMmodelsas fordifferenttasks. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  extensive experiments on given hardware (RTX Here, our cut-offs were selected by observing 2080Ti GPU) with different model settings as therecentstate-of-the-artmodel’sperformanceon the selected dataset for the task4. A wise choice shown in Table 3 and Table 4. We also collect thedevelopmentsetperformancewithtimeinﬁne- wouldbechoosingtheperformanceofsomeclassic tuningtoinvestigatehowthemodelisﬁne-tuned methodslikeLSTM-CRForBi-LSTMmodelsas fordifferenttasks. </Extractive Summary>  </Table ID = 4>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In the MNLI task, such a trend does 333notapplybecauseoftheincreaseddifﬁcultylevel andthenumberoftraininginstances,whichfavors alargermodelcapacity. Even though ALBERT model has a lot fewer BERT-BASE RoBERTa-LARGE ALBERT-BASE BERT-LARGE XLNet-BASE ALBERT-LARGE parameters than BERT, according to Table 1, the RoBERTa-BASE XLNet-LARGE ALBERTmodel’sﬁne-tuningtimeissigniﬁcantly more than BERT models because ALBERT uses 0.8 largehiddensizeandmoreexpensivematrixcom- putation. Theparametersharingtechniquemakes 0.6 1(Dev) it harder to ﬁne-tune the model. </Extractive Summary>  </Table ID = 1>  </Paper ID = 366> 

<Paper ID = 367>  </Paper ID = 367> 

<Paper ID = 368>  </Paper ID = 368> 

<Paper ID = 369>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Highest constituency parsing scores of all models. </Abstractive Summary>  <Extractive Summary> =  scribesthisalgorithm. Table 1 describes the S-F1 scores of the best Constituency parsing is a word-level task, but attentionheadsofBERTandRoBERTa. Wealso BERTusesbyte-pairtokenization(Sennrichetal., choosethebestrecallforeachphrasetype. </Extractive Summary>  </Table ID = 1>  </Paper ID = 369> 

<Paper ID = 370>  </Paper ID = 370> 

<Paper ID = 371>  <Table ID = 1>  <Abstractive Summary> =  Table 1: EN2EL and EL2EN NMT evaluation results utilizes the BERT model for text embeddings and & comparison with other models. </Abstractive Summary>  <Extractive Summary> =  using FP16 precision to address our hardware limitations by reducing the memory consumption 5.1 NMT performance and time spent in memory. The produced models Table 1 shows the evaluation of our models (lower- (4 in total) are as follows: i. a lower-case EL2EN case and mixed-case) on the Tatoeba3 and XNLI4 and a lower-case EN2EL model from the first setup test sets. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: mixed-cased models were evaluated on the original reference translations, while the lower-case models Model Prec. Table 2: PENELOPIE evaluation results on the reduce the expansion of the vocabulary by translated CaRB testset & comparison with Multi2OIE. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 371> 

<Paper ID = 372>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Number of sentences in the extracted dataset ments,theevokedframes,andthecontextelements’ anddistributionofFrameNetrelationsbetweenoriginal roles in these frame for a given sentence. </Abstractive Summary>  <Extractive Summary> =  We stances,whicharefewerinnumberthantheother foundthatmosteditscouldbecategorizedintoone categories,underasingleOthercategoryandleave ofthefollowingframerelationsbetweentheframes further inspection to future work. A distribution evokedbytheoriginalandrevisedverbframes: of instances over categories is shown in Table 2. Apartfrominstancesfromthe‘Other’category,we 1. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  A toas‘other’inTable2). vaguesenseclassiﬁerfordetectingvaguedeﬁnitions In Table 4, we provide examples where the in ontologies. In Proceedings of the 14th Confer- enceoftheEuropeanChapteroftheAssociationfor model failed using both FastText and BERT. </Extractive Summary>  </Table ID = 4>  </Paper ID = 372> 

<Paper ID = 373>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Results on UD. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 373> 

<Paper ID = 374>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Number of articles per institute in the gold- convergence. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  All three models show their best performanceforthesamesubject,Commencement Theoverallperformanceforallourmodelsisde- ceremonies. Both DT and ANN have a non-zero picted in Table 2. Overall, the BERT model per- F-score for each subject. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The DT model e.g.,Clinicalpsychology. achieves an average F1 score of 0.37, whereas Wealsoassesstheperformanceofeachmodel the lowest F1 score (0.30) is observed for fre- per document type, as reported in Table 3. For quency range [300,400). </Extractive Summary>  </Table ID = 3>  </Paper ID = 374> 

<Paper ID = 375>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Results on Universal Triggers with shows that our method is much more capable of BiDAF (BERT-speciﬁc triggers unavailable publicly). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  of 5. A uniform dropout (Srivastava et al., 2014) Table 2 details the results with BiDAF on of 0.2 is applied at the CNN layer for character AddSent.1 Here, we also see signiﬁcant perfor- embedding,allLSTM(HochreiterandSchmidhu- mance gains over the vanilla model and the SLN ber,1997)layersandatthelayerbeforethelogits. baseline. </Extractive Summary>  </Table ID = 2>  </Paper ID = 375> 

<Paper ID = 376>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We believe this behavior is rooted investigated another types of retrieved rele- intwopossiblereasons: (1)Thenumberofunique, vant documents: UR and Re−ranker,Full−ranker relevant documents, which BERT-based ranker UR whicharedeﬁnedasfol- Full−ranker,Re−ranker ﬁnds,thatBM25doesnotconsider;(2)theBERT- lows: basedfull-rankercanﬁndmorehighly-rankednew documents. • URRe−ranker,Full−ranker (Unique Relevant foundbyRe-ranker): therelevantdocuments Method MAP@100 NDCG@20 retrievedbytheRe-rankerbutnotretrievedby BM25 0.3274 0.4714 theFull-ranker Re-ranker 0.4198‡ 0.5525‡ Full-ranker 0.4404‡ 0.5670‡ • URFull−ranker,Re−ranker (Unique Relevant foundbyFull-ranker): therelevantdocuments Table1: ResultsforLearningRate=2e−5,Epoch=2, retrievedbytheFull-rankerbutnotretrieved BatchSize=32,MaxLength=256 bytheRe-ranker To investigate these possible reasons, we went Table 3, and Figure 4 show the full-ranker’s through the unique documents each model re- powerincomparisonwiththere-ranker. Itisworth trieves. </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Itisworth trieves. Table 2 presents the results for the per- mentioning that the re-ranker and the BM25 re- centageofretrievedrelevantdocumentsthatBM25 trieved documents are the same in depth@100, found that were not found by the full-ranker, but they have different rankings as the re-ranker and vice versa, for different depths. We are in- changesthedocuments’rankingscores. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  It retrieved by BM25 but not retrieved by the isalsoworthmentioningthatthereare67.6%new Full-ranker documents in the BERT-based full-ranker model in total (both relevant and irrelevant) at top-100 • UR (Unique Relevant Full−ranker,BM25 results, which makes it a substantially different foundbyFull-ranker): therelevantdocuments rankerthanBM25. retrievedbytheFull-rankerbutnotretrieved Table 4 investigates the relevance degree (RD) bytheBM25 of the unique, relevant documents found by each Figure 3 demonstrates these results in more modelattheirtop-100results. Thecollectionhas detail, showing the full-ranker’s power to ﬁnd four different grades indicating the relevancy of 62Method depth@10 depth@20 depth@100 UR 0.08 0.04 0.009 BM25,Full−ranker UR 0.19 0.11 0.02 Full−ranker,BM25 Table2: UniqueRelevant(UR)percentofdocumentsfoundbyBM25andthefull-rankeratdifferentdepths Method depth@10 depth@20 depth@100 UR 0.1 0.05 0.009 Re−ranker,Full−ranker UR 0.13 0.08 0.02 Full−ranker,Re−ranker Table3: UniqueRelevant(UR)percentofdocumentsfoundbythere-rankerandthefull-rankeratdifferentdepths Method RD1 RD2 RD3 RD4 andthe9thInternationalJointConferenceonNatu- BM25/Re-ranker 0.26 0.45 0.22 0.07 ralLanguageProcessing(EMNLP-IJCNLP),pages 3490–3496, Hong Kong, China. </Extractive Summary>  </Table ID = 4>  </Paper ID = 376> 

<Paper ID = 377>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Comparing accuracy, AUC score and train- and AUC for Siamese LSTM, Single LSTM, Vanilla ingtimeforSiameseLSTMusingdifferentproductat- NNandRandomForest. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Comparing the time needed for predicting Accuracy AUC attribute(s) time complementarityamong1M pairsofproductsandthe Title 85% 93% 13min time complexity for creating the embeddings in the Title+ 89% 95% 58min NN. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 377> 

<Paper ID = 378>  <Table ID = 1>  <Abstractive Summary> =  Table 1: SentEval Task Results Using Fixed Sentence Encoder. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 378> 

<Paper ID = 379>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Reported similarities between manually de- composed questions (gold) and decompositions gener- F1 0.89±0.02 0.97±0.003 ated by our approach. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  For our work, mation needed to answer it does not exist in the we evaluate two different variations: We run the passage. To better understand the effect of this, pipelineonthegolddecompositionsthathavebeen wediscardincorrectlylabeledexamplesandreport manuallyrewritten,andautomatically-decomposed accuracy in the second column of Table 4. We questionsgeneratedbyourapproach,usingBERT also report the number of predictions matching single-hop RC described in section 4. A.2 SoTAComparison This leaves 1106 questions with 59.3% accuracy. We report the accuracy of MTMSN (Hu et al., Thealternativeistoﬁlterquestionsbasedontheir 2019)andNeRd(Chenetal.,2020)onthetwosub- start trigrams, which gives a more relevant set of traction evaluation sets in Table 4. For MTMSN, questions. </Extractive Summary>  </Table ID = 4>  </Paper ID = 379> 

<Paper ID = 380>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  comparenewemailzoningmethodsinthefuture. RepkeandKrestel(2018)alsoannotated500ASF Table 1 compiles the information of existing emails7 using both the 2-level and 5-level tax- email zoning corpus. To the best of our knowl- onomies. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Inter-annotator agreement for each language This section presents Cleverly zoning corpus, intheCleverlyzoningcorpus,usingCohen’skappa(k), theﬁrstmultilingualemailzoningcorpus. </Abstractive Summary>  <Extractive Summary> =  1 1 2 atethecorpus,wesearchedtheGmanerawcorpus (Bevendorffetal.,2020)forPortuguese(pt),Span- ish (es) and French (fr) emails. Then, following Table 4 shows the inter-annotator agree- theclassiﬁcationschemaproposedbyBevendorff ment scores for each language using the Co- etal.(2020),weproducedatotalof625annotated hen’s kappa coefﬁcient (k) (McHugh, 2012), emails. accuracy and F of one annotator versus the 1 Table2compilesabriefdescriptionoftheemail other. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Multilingual zero-shot evaluation of OKAPI, bedding s . </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In partic- Nicolas Bettenburg, Bram Adams, Ahmed E. Has- ular, Table 6 compares OKAPI with other zoning san, and Michel Smidt. 2011. </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  A lightweight ap- systems on the corpora annotated by Repke and proach to uncover technical artifacts in unstruc- Krestel (2018) with 2 and 5 types of zones; and tureddata. InInternationalConferenceonProgram Table 7 shows the results obtained with the most Comprehension,pages185–188,LosAlamitos,CA, USA.IEEEComputerSociety. recentandﬁne-grainedannotationschemawith15 zonesproposedbyBevendorffetal.(2020). </Extractive Summary>  </Table ID = 7>  </Paper ID = 380> 

<Paper ID = 381>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Comparison of the model’s retrieval perfor- N manceusingdifferentwordembeddings. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thetestsetsweuse Speakersofrelatedlanguagesareoftenabletode- for evaluation are matched at the token level, the code some information from each other’s speech onlydifferencebeingthespeakercharacteristics. withouteverhavinghadtoexplicitlylearnthecorre- Retrieval scores, including median rank of the spondencesbecauserelatedlanguagesexhibitpre- correct embeddings, are reported in Table 2, and lexical as well as lexical similarities. Gooskens average cosine similarity of the computed mean- et al. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  However,otherinteresting dian rank, for all three languages are reported in correspondences are apparent. For example, we Table 3. Average cosine similarities of matching observe that for the word ship (/SIp/), the model utterance-embeddingpairsforthethreelanguages is better at recognizing the English word, which arereportedinFigure3. </Extractive Summary>  </Table ID = 3>  </Paper ID = 381> 

<Paper ID = 382>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Number of train, test (for TT) and collected is largest. </Abstractive Summary>  <Extractive Summary> =  ForallSA,thelabels tobepredictedarepositive/negative/neutral. In Table 1, we report the label distribution, Figure1: Happy(left)andunhappy(right)emojiclus- hate/noneforHSandpositive/negative/neutralfor tersobtainedbyKMeansonTW-DE. SA, across all TT training and test sets, as well as ST Twitter corpora sizes. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Macro F1 comparison of top-scoring trans- while low emoji content SA-DE does not. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 382> 

<Paper ID = 383>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Comparison of different auxiliary tasks. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 383> 

<Paper ID = 384>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Statistics of datasets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 384> 

<Paper ID = 385>  </Paper ID = 385> 

<Paper ID = 386>  </Paper ID = 386> 

<Paper ID = 387>  </Paper ID = 387> 

<Paper ID = 388>  <Table ID = 1>  <Abstractive Summary> =  Table 10: Recall over all mentions and each unseen percentage of UNSEEN-TYPE mentions in Span- mentionsubset(OntoNotes5.0English) ish (1.8) and ReCoNLL English (1.5) is similar, the performance for BERT for those mentions in Spanishis34.04pointsbelowthatforReCoNLL onlyseenwithothertypesinthetrainingdata. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Recall over all mentions and each type- confusablementionsubset(ReCoNLL-2003English) Model Precision Recall F1 CHARLSTM 87.12(±0.42) 86.38(±0.36) 86.90(±0.40) Model ALL U-ANY U-TOK. </Abstractive Summary>  <Extractive Summary> =  Oneoftheadvantages of evaluating using TMR metrics is that systems BERT attains the highest F1 in Dutch (91.26) can be differentiated more easily. Table 7 gives and Spanish (87.36); due to space limitations, ta- recallfortype-confusablementions(TCMs)onRe- bles are provided in the appendix (Tables 14-15). CoNLL English. </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Theperformance tionshavebeenseeninthetrainingdata,itisable ofCharLSTMandCharCNNcannotbedifferenti- toeffectivelydisambiguatetypesbasedoncontext. atedinEnglish,butCharLSTMsubstantiallyout- Table 8 gives recall for unseen mentions. Al- performsCharCNNinSpanish(+2.53)andDutch though Flair attains higher overall recall, BERT (+2.15). </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  Table 9: Recall over all mentions and each type- likelyduetosystemsbeinglessoptimizedforthose confusablementionsubset(OntoNotes5.0English) languages. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 9>  </Paper ID = 388> 

<Paper ID = 389>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Table4gives LMVR 7.24±0.22 32.16±0.63 MORSEL 7.78±0.16 34.32±0.30 countsforthenumberoftimeseachsegmentation SentencePiece 7.52±0.08 33.58±0.43 approach was the top-performing one or statisti- Subword-NMT 7.76±0.25 34.38±0.38 cally indistinguishable from it. Table 7 in the appendix gives p-values for all comparisons per- Table 3: Mean and standard deviation of BLEU and CHRF3acrosstranslationtasksandsegmentationmeth- formed. ods. </Extractive Summary>  </Table ID = 7>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Mean and standard deviation of BLEU and CHRF3acrosstranslationtasksandsegmentationmeth- formed. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Posterior means and standard deviations of τ −τ (pairwisecomparisonwithBPE) Table 4: Number of times each segmentation method m Subword−NMT under the BLEU and CHRF3 models. </Abstractive Summary>  <Extractive Summary> =  (2019)reportbaselineNMTscoresof2.32onKK- EN and 1.42 on EN-KK, which are in line with To explicitly compare SentencePiece, LMVR our MORSEL and SentencePiece results on KK- andMORSELtotheSubword-NMTbaseline,we EN,andSubword-NMTresultsonEN-KKinthe alsomodelthepairwisedifferencesbetweeneach train120kcondition. method’sτ-termandthatofSubword-NMT.The posterior inferences for these quantities can be 4.1 ModelingBLEUandCHRF3 seen in Table 5 and are plotted in the appendix. BasedonFigure1andTables3and4,theBLEU For BLEU, the differences for LMVR are sev- andCHRF3scoresvarywithboththetranslation eralstandarddeviationsbelow0,suggestingthatit task and segmentation method. </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Number of times each segmentation method m Subword−NMT under the BLEU and CHRF3 models. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 389> 

<Paper ID = 390>  </Paper ID = 390> 

<Paper ID = 391>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Titles, however, were one label. Table 1 reveals that the register distri- butionsinthefourlanguagesarebroadlysimilar, 1Available at https://github.com/TurkuNLP/ Multilingual-register-corpora featuringNarrative,Informationaldescription,and 184Register English Finnish French Swedish mean std. mean std. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Additionally, we use Multilingual BERT These differences may reﬂect differences in data (mBERT) (Devlin et al., 2019), which was pre- compilation. Table 2 shows that, on average, En- trained on monolingual Wikipedia corpora from glishdocumentsarelongerthandocumentsinother 104languageswithasharedmultilingualvocabu- languages,whereasSwedishdocumentstendtobe lary. shortest. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Figure1:Monolingualperformancewhentrainingwithvary- ingnumberofexamples(solidlines)inrelationtozero-shot cross-lingualperformancewhentrainingonfullEnglishset 4 Results (dottedlines).Errorbarsrepresentstandarddeviations(N=6). In Table 3, we present the primary results on En- glish, Finnish, French and Swedish monolingual porttheclaimedcompetitivenessofXLM-Rlarge classiﬁcation with the models described in Sec- withmonolingualmodels,mentionedinSection3. tion3,aswellascross-lingualresultswithEnglish English, Finnish and French BERT models as the source language and Finnish, French and achievesimilarmonolingualtestresults(73–74% Swedishastargetlanguages. classsetting. Previousstudieshaveshownrepeatedlythatreg- Furthermore, Table 3 shows very strong zero- istersvaryconsiderablyintermsofhowwellthey shotcross-lingualresultswithXLM-Rlarge,with are linguistically deﬁned and thus how well they F1-scores in the 61–69% range. This represents canbeautomaticallyidentiﬁed(BiberandEgbert, aremarkablyconsistentrelativedecreaseof16.2– 2018,2016a;Laippalaetal.,2020a). </Extractive Summary>  </Table ID = 3>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In Advances in neural information zero-shot cross-lingual and monolingual register processingsystems,pages5998–6008.CurranAsso- classiﬁcation experiments, respectively. Table 6 ciates,Inc. presentstheregistertaxonomywiththemainregis- Antti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Lu- tersandtheirsub-registers. </Extractive Summary>  </Table ID = 6>  </Paper ID = 391> 

<Paper ID = 392>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (TokLem). TokLem is motivated by the fact that Other bias factors We can see in Table 1 that BERT is trained on raw text. Thus, we assume mostinﬂuencesareabove-baseline. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Correlations of word form and predicted A E 9-12 .554 .218 .531 changescores. </Abstractive Summary>  <Extractive Summary> =  ered. The results are presented in Table 4. We Pre-processing Aswiththeclustering,wetryto observe similar ﬁndings for all three languages. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Performance of pre-processing variants for performance for ENG and GER is clearly below threelayercombinations. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 392> 

<Paper ID = 393>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Wealsotrymanualevaluation,wherefourevalua- 204torsratethetranslationsofallsentencesfromthe threetestsetsfortheirﬂuency)orsyntacticcorrect- ness and (adequacy or translation accuracy. The evaluation of all the metrics is done on a 4-point Likert scale, see Table 1 for reference. The as- signed scores by different raters are totalled and averaged for all the given sentences. </Extractive Summary>  </Table ID = 1>  </Paper ID = 393> 

<Paper ID = 394>  </Paper ID = 394> 

<Paper ID = 395>  </Paper ID = 395> 

<Paper ID = 396>  </Paper ID = 396> 

<Paper ID = 397>  </Paper ID = 397> 

<Paper ID = 398>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Type: Introductory 1 TutorialContent Structure: seeTable1 Prerequisites: ProﬁciencyinEnglish Thistutorialwillcoverthetheoryandpracticeof reviewingresearchinnaturallanguageprocessing. Table 1 presents a brief outline of the tutorial. Ashasbeenpointedoutforyearsbyleadingﬁgures Ouraimistoprovideenoughoptionsforhands-on inourcommunity(Webber,2007),researchersin experienceandsmaller-groupactivitiesinbreakout theACLcommunityfaceaheavy—andgrowing— rooms. </Extractive Summary>  </Table ID = 1>  </Paper ID = 398> 

<Paper ID = 399>  </Paper ID = 399> 

<Paper ID = 400>  </Paper ID = 400> 

<Paper ID = 401>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Accuracy in percentage for task/model com- modeldoeswithoutanybiomedicaldata. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Comparison of a) our main multidomain model (Multi), b) models pretrained solely on Pubmed (Pub- 10/100/500),andc)modelsaftercontinuedadaptationtoPubmed(+Pub-10/100/500). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 401> 

<Paper ID = 402>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Named entity segmentation performance on the WNUT test set (target) and CoNLL test set A (source). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Multi-domain sentiment analysis adaptation. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  B DataStatistics In this section, we will introduce the data statis- tics of our named entity segmentation and sen- timent analysis . In Table 4, the data are from origin AdaptaBERT (Han and Eisenstein, 2019), of which Twitter dataset has more unlabeled dev data than labeled train data. In Table 5, the data is collected from open Amazon review (He and McAuley, 2016; McAuley et al., 2015). </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In Table 4, the data are from origin AdaptaBERT (Han and Eisenstein, 2019), of which Twitter dataset has more unlabeled dev data than labeled train data. In Table 5, the data is collected from open Amazon review (He and McAuley, 2016; McAuley et al., 2015). We pro- cessthedatainto3domainsofbalanceddatasets. </Extractive Summary>  </Table ID = 5>  </Paper ID = 402> 

<Paper ID = 403>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  All reports are conductingtextclassiﬁcationonthetargetdomain, basedon5runs. we only feed the shared feature to C and set the Table 2 and Table 3 show the experimental re- domain-speciﬁcfeaturevectorto0. sultsontheAmazonreviewdatasetandtheFDU- WeconducttheexperimentsontheAmazonre- MTLdataset,respectively. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  All reports are conductingtextclassiﬁcationonthetargetdomain, basedon5runs. we only feed the shared feature to C and set the Table 2 and Table 3 show the experimental re- domain-speciﬁcfeaturevectorto0. sultsontheAmazonreviewdatasetandtheFDU- WeconducttheexperimentsontheAmazonre- MTLdataset,respectively. evaluations are conducted on the target domain. FromtheexperimentalresultsontheFDU-MTL InordertovalidateCAN’seffectiveness,wecom- dataset, reported in Table 3, we can see that the pareCANwithseveraldomain-agnosticmethods: CANmodelobtainsthebestaccuracieson10of16 (1) the MLP model; (2) the marginalized denois- domainsandachievesthebestresultintermsofthe ing autoencoder (mSDA) (Chen et al., 2012); (3) averageclassiﬁcationaccuracy. Theexperimental the domain adversarial neural network (DANN) resultsonthesetwoMDTCbenchmarksillustrate (Ganinetal.,2016). </Extractive Summary>  </Table ID = 3>  </Paper ID = 403> 

<Paper ID = 404>  </Paper ID = 404> 

<Paper ID = 405>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Keyword Evaluation scores averaged across ported in Table 3 are marginal. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In particular, the improvement over Score BTM FTE BTM FTE BTM FTE BERT further conﬁrms the importance of the Interpretability 31.94 65.28 1 5 15.01 17.97 ﬁnetuning step in our method. In contrast, the Usefulness 27.78 59.72 1 5 12.36 21.55 improvements in average scores over BTM re- Table 2: Keyword Evaluation scores averaged across ported in Table 3 are marginal. Nevertheless, we topics, number of topics with average scores greater ﬁnd that the number of interpretable and useful than0.5,andinter-rateragreements(Fleiss’κ). </Extractive Summary>  </Table ID = 3>  </Paper ID = 405> 

<Paper ID = 406>  <Table ID = 1>  <Abstractive Summary> =  Table 1: POS, UAS and LAS scores between the two tation projection using parallel data (Barry et al., annotators. Table 10: POS, UAS and LAS scores (mean over 5 • EnglishPronouns randomseeds)andstandarddeviationsondevdatafor 2000sentences. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: POS, UAS and LAS scores on dev data for Liuetal.(2018). </Abstractive Summary>  <Extractive Summary> =  Dev Overall, our best model consisted of 128 compo- overﬁverandomseeds,testoverthebestrandomseed nents and used 2000 sentences. Table 2 shows ofdev. 52NL ALPINO 1514 NL LASSYSMALL 447 Themostoccurringconfusion(discourse-parataxis) DE GSD 14 EN EWT 14 wasalsoacauseofdisagreementintheannotation EN GUM 5 EN TWEEBANK 4 process. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: POS, UAS and LAS scores on dev data for unsupervised,efﬁcientandreturnstheprobability 128 components, mean over 5 random seeds. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: POS, UAS and LAS scores baselines versus best model (128 components/2000 sentences). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Top ﬁve confusions on dependency labels. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  <Table ID = 9>  <Abstractive Summary> =  Table 9: Number of tokens for the combination lan- ancebutthispartismissing. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 9>  </Paper ID = 406> 

<Paper ID = 407>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  For this reason a Scalar mix (Peters et al., 2018; Tenney et al., wechosetospliteachgenrewithinthetreebanks 2019a,b)ofthelastfourlayersforeachsubtoken intoapproximatelysequential80/10/10splits,with ofaword. Additionally,itusesaBiLSTMencoder selected treebank statistics presented in Table 1. to compute context aware representations by em- Forcross-genreexperiments,EWTandSETsub- ployingtwodifferentMLPlayersindicatingboth genresareconcatenatedrespectively. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  and Leibler, 1951) of POS trigrams (Rosa and Zˇabokrtsky´, 2015). In Table 2 we present results bert-base-cased (BERTbc)wastrainedonamix- forKLdivergenceforPOStrigramswiththeclos- ture of English Wikipedia (2,500M words) and estsimilargenreinbold.8 BookCorpus(800Mwords;Zhuetal.,2015)with GiventhatBERTworksonasubtokenlevel,we theBookCorpuscontainingsixteenidentiﬁedbook genres.6 additionallypresenttheKLdivergenceforBERT subwordtokensbetweengenres. Eachgenrewas bert-large-swedish-uncase (swBERT) was tokenizedusingthespeciﬁedBERTtokeinzerand trainedonSwedishWikipedia(300Mwords). Identifying BERT subword tokens similarities provides in- bert-base-swedish-cased (kbBERT) was sightsinto(sub)lexicallevelsimilarity,aswellas trained using newspapers (2,977 M words), howdelexicalizedandsubwordpatternwitheach government publications (117M words), legally other.9 availablee-deposits(62Mwords),7 internetforums The row (y-axis) is the target genre, and the 4https://github.com/af-ai-center/ columns (x-axis) are the source (e.g. in Table 2 SweBERT 5https://github.com/Kungbib/ .4660 is Europarl target Blog source).10 We see swedish-bert-models 6We do not consider this to mean there are 16 distinct 8WefollowRosaandZˇabokrtsky´ (2015)anddefaultthe genresaswedeﬁnetheterm,rathertonotethemorediversiﬁed targetgenreto1inKLcalculations. domains,thoughauthorstylewouldnaturallyinﬂuenceany 9Wenotehoweverthatthetwoparsersdonotnecessarily learnedrepresentations. experiment. However,weseecontinuedindividualarchitectural WeseethatEWTusingmBertdoesnotcorrelate strengthsandconsistency,suchasthatBNPnostill well with the KL divergences in Table 2. There showsstrengthonparsingWeblog,similartothat is seemingly a preference for either Answers or in Fig. </Extractive Summary>  </Table ID = 2>  </Paper ID = 407> 

<Paper ID = 408>  </Paper ID = 408> 

<Paper ID = 409>  <Table ID = 1>  <Abstractive Summary> =  Table 12: Results on the development sets using enough to infer the correct translation. </Abstractive Summary>  <Extractive Summary> =  Dataset sizes for all janovski and Fraser, 2018; Zhang et al., 2018)). domains are presented in Table 1. The develop- The model is outlined in Figure 2. All context- aware models are initialized with the parameters C Validationperformance from this pretrained sentence-level baseline. Pa- rametersthatarespeciﬁctothemodels’architec- In Table 11, Table 12 and Table 13 we present turesarerandomlyinitialized. Allproposedmod- BLEU scores on the development sets for all the els in this work share the source, target, output experimentsweran. All context- aware models are initialized with the parameters C Validationperformance from this pretrained sentence-level baseline. Pa- rametersthatarespeciﬁctothemodels’architec- In Table 11, Table 12 and Table 13 we present turesarerandomlyinitialized. Allproposedmod- BLEU scores on the development sets for all the els in this work share the source, target, output experimentsweran. All context- aware models are initialized with the parameters C Validationperformance from this pretrained sentence-level baseline. Pa- rametersthatarespeciﬁctothemodels’architec- In Table 11, Table 12 and Table 13 we present turesarerandomlyinitialized. Allproposedmod- BLEU scores on the development sets for all the els in this work share the source, target, output experimentsweran. Table 13: Domain adaptation results on PatTR and Thelastexampleshowsthatthesentence-level TED for SentBase and DomEmb(avg) on the develop- model translates “springs” (“Federn” which is a mentsets. partofthecompoundword“Druckfedern”inthe reference)asin“watersprings”(“Quellen”which is a part of the compound word “Kompression- E Examples squellen”)whileitshouldbetranslatedinsteadas In Table 14 we show some example transla- in the physical elastic device. However, in other tions from the sentence-level baseline and our testsentences,bothSentBaseandDomEmb(avg) DomEmb(avg)model. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Results on zero-resource domain adaptation isimportanttoobtainthebestresults. </Abstractive Summary>  <Extractive Summary> =  Themodels TED 0.2M 1.7K 1.0K aretrainedonourmulti-domaindatasetconsisting PatTR 1.2M 2.0K 2.2K of ﬁve domains. The results are shown in Table 2. Wecomputestatisticalsigniﬁcancewithpaired Table1: Domaindatasetssizesinsentences. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Results on the multi-domain dataset. </Abstractive Summary>  <Extractive Summary> =  multi-domainsettingbecauseitislesssensitiveto largerimprovementsonasmallertestset. domain ctx=1 ctx=5 ctx=10 Table 3 shows the results. We ﬁrst compare Europarl 31.5 32.0†‡ 32.5†(cid:168) the baseline against DomEmb(avg). </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Results using the DomEmb(avg) model with baseline. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Comparison with the context-aware baseline CtxBaseandtheconcatenationmodelConcBase. </Abstractive Summary>  <Extractive Summary> =  Butthisappliestomore with a 5 sentence context. The results in Table 5 granularmodelswhichresemblethecontext-aware show that all models improve to varying degrees. baseline CtxBase. </Extractive Summary>  </Table ID = 5>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Domain adaptation results on TED for Sent- selectedthetop100withthehighestscoreswhich BaseandDomEmb(avg). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Since these initial mod- cedureisconductedforeachdomaindseparately. els are identical to the ones in the zero-resource Table 9 shows the results. On OpenSubtitles, setup,wereusethem. </Extractive Summary>  </Table ID = 9>  </Paper ID = 409> 

<Paper ID = 410>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thereisagrowinginterestinbuildingscal- Recentresearchhasshownpromisingresultson able end-to-end question answering systems for developingneuralmodelsforretrievaltasksinclud- largescaleretrieval(Ahmadetal.,2019;Royetal., ing ReQA, MS MARCO, and the retrieval part 2020). Retrievalquestionanswering(ReQA)(Ah- of open domain question QA (Roy et al., 2020; mad et al., 2019), illustrated in Table 1, deﬁnes Karpukhin et al., 2020; Xiong et al., 2020; Luan thetaskasdirectlyretrievingananswersentence from a corpus.2 Motivated by real applications etal.,2020). Onechallengeofemploying neural modelsisthatitusuallyrequiresalargeamountof ∗Correspondingauthors:{xyguo,yinfeiy}@google.com trainingdata. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Statistics for each constructed dataset: # of guishquestionsandanswers,weaddanadditional trainingpairs,#ofquestions,#ofcandidates,andaver- input type embedding to each input token.10 The age#ofanswersperquestion. </Abstractive Summary>  <Extractive Summary> =  Spans andBingwebsearchresults,excludingtriviaweb- coveringmultiplesentenceareexcluded.7 sites(Joshietal.,2017b). Table 3 provides statistics on the number of training set pairs and the number of questions, HotpotQA Wikipedia question-answer pairs. candidates and average answers per question in Thisdatasetdiffersfromtheothersinthattheques- the evaluation data. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  candidates and average answers per question in Thisdatasetdiffersfromtheothersinthattheques- the evaluation data. Table 4 shows the average tions require reasoning over multiple supporting lengthofwordtokensanddegreeoftokenoverlap. documents(Yangetal.,2018). </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Precision at 1(P@1)(%) and Mean Reciprocal Rank (MRR)(%) on the constructed question answer re- trievaldatasets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Examples where both the BM25 and USE-QA models get wrong. </Abstractive Summary>  <Extractive Summary> =  P@1performanceonTriviaQAandTextbookQA. Table 7 shows examples where the models re- trievedifferentanswers,andbothareincorrect. In 6 Analysis the ﬁrst example, the BM25 retrieves the cor- wpm rectcontextbymatchingthekeyword“SaltonSea”. </Extractive Summary>  </Table ID = 7>  </Paper ID = 410> 

<Paper ID = 411>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Performance on both the individual sub-tasks (Negation, DocTimeRel, and ALINK) and the complete task(All)forsystemstrainedontheTHYMEcoloncancertrainingsetandtestedonthein-domainTHYMEcolon testset,theout-of-domainTHYMEbraintestset,andtheout-of-domainpulmonaryhypertension(PH)testset. </Abstractive Summary>  <Extractive Summary> =  But an oracle optimizing PH perfor- entirepipelineis,givendifferentsystemsettings. mancewouldtellustousetheSVMfornegation The “Colon” columns of Table 1 show results andDocTimeRelandRoBERTa+LMforALINK. on the THYME colon cancer data (in-domain). Theproblemhasbeenexacerbatedwiththe egoryhasrelativelyfewexamplesandwhoselow movefromfeature-basedclassiﬁerstopre-trained performanceskewstheaveragingofthemacro-F1. blackboxmodels,asitisnowevenmoredifﬁcult The “Brain” and “PH” columns of Table 1 tounderstandthecauseoferrorsinnewdomains showout-of-domainperformanceofthesamesys- withoutinterpretablefeatures. Domainadaptation tems on the THYME brain cancer and our in- shouldleverage“BERTology”andinterpretability ternal pulmonary hypertension data, respectively. </Extractive Summary>  </Table ID = 1>  </Paper ID = 411> 

<Paper ID = 412>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Performance of the models for different tasks along with their standard deviations. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 412> 

<Paper ID = 413>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Cross- causewecalculatetheneighborsforeachpremise domainsimilaritylocalscaling(CSLS)(Conneau from the training dataset only rather than any ex- etal.,2017)isusedtoretrievethetranslatedword. ternal text like Wikipedia (i.e., generate embed- Table 1a shows the accuracy of our approach in dingsforWikipediasentencesandthenusethem comparisontoothermethods. asneighbors). Oncetheinitialpretrainedwordvectorsarese- lected,preprocessingcanbeapplied. Preprocess- B MeasuringPerformanceConsistency ing functions include unit normalization, whiten- ing and z-normalization ( Table 1b). Ruder et al. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Nearest neighbors extracted from SNLI classi- a component of LPL and ultimately a part of the ﬁer for a sentence pair representation. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: SamplesfromtheSNLI(Bowmanetal.,2015b) Figure11: Pearsoncorrelationonsemantictextsimilarity dataset.Eachpairconsistsoftwosentencesandalabelwith withSTS-Bdatasetfortrainingsamplesizegreaterthan50% oneofthreevaluesentailment,neutral,contradiction. </Abstractive Summary>  <Extractive Summary> =  Incomparisontoacontinuousδ variable and 10K pairs of sentences for testing. Table 4 usedintheSTStask,inNLI,theδ isconstantfor showsafewsentencepairsfromthedataset. each label. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: SamplesfromtheSNLI(Bowmanetal.,2015b) dataset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: SamplesfromtheSNLI(Bowmanetal.,2015b) Forexperimentswithcrosslingualwordalignment, dataset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  </Paper ID = 413> 

<Paper ID = 414>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Statistics of the datasets we used to train our models. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Examples of falsiﬁed predictions by standard ﬁne-tuning scheme when transferring from News- domaintoTweets-domain. </Abstractive Summary>  <Extractive Summary> =  TransparentGrayhighlightstheﬁnalgainbroughtbyTL. thislastmitigatestheﬁnalgainbroughtbyTL.For 5.3 QualitativeExamplesofNegative instance,onTchunkdataset,TLcorrected∼4.7% Transfer: ofpredictionsbutfalsiﬁed∼1.7%,whichreduces We illustrate in Table 32 concrete examples of theﬁnalgainto∼3%. wordswhosepredictionswerefalsiﬁedwhenusing transferlearningcomparedtorandominitialisation. </Extractive Summary>  </Table ID = 3>  </Paper ID = 414> 

<Paper ID = 415>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Micro-averaged F1 score for each of the 2- beddingsonbenchmarktasks(Aroraetal.,2020). </Abstractive Summary>  <Extractive Summary> =  Theresultsforeachk-shotsetting similaracrossmethods. Thusweconcludethatthe are given in Table 2, reported in micro-averaged contexts which each method ﬁnds most useful to F1score. TheresultsobtainedusingrandomOOV inferachimera’smeaningarelargelyinvariable. </Extractive Summary>  </Table ID = 2>  </Paper ID = 415> 

<Paper ID = 416>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Descriptive statistics of the number of ut- calculatedtheUASandLASofthesample. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Parser accuracy for (1) parent speech with the UD-EWT parser; (2) child speech with the UD-EWT parser;and(3)childspeechwithvariousparsertrainingconﬁgurations. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 416> 

<Paper ID = 417>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Sentence-level F1 numbers on multilingual treebanks. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 417> 

<Paper ID = 418>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Statistical summary of the Amazon, Yelp and IMDb review datasets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Performance scores of document classiﬁers on the review datasets. </Abstractive Summary>  <Extractive Summary> =  Wecon- the linguistic style variability could be used for duct ﬁne-tuning steps for 10 epochs with a batch sizeof32andoptimizethemodelbyAdamWwith predictinguser’spersonalityanddemographicat- alearningrateof9e−5. tributes (Rosenthal and McKeown, 2011; Zhang et al., 2016; Hovy and Fornaciari, 2018; Wood- We show the performance results in Table 3. Doughty et al., 2020; Gjurkovic´ et al., 2020; Comparingtothebaselines,theclassiﬁersperson- Lynn et al., 2020). </Extractive Summary>  </Table ID = 3>  </Paper ID = 418> 

<Paper ID = 419>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  SVMaccuracyofSVMlanguagepredictions. toBASEisonly0.31Acc.,whereasfordependency parsing,itis+1.60LAS.PREDismostlybeneﬁcial For demonstration purposes, we highlight the inmulti-lingualdatasetgroups,whichisprobably full results of two dataset groups in Table 3. because the performance of the dataset classiﬁer Theﬁrst(multi-lingual)groupshowsthatdataset- (Section2.2)ishigherinthesecases. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In Proceedings of the 56th An- nual Meeting of the Association for Computational 188A Reproducabilityreport tousedata-ﬁlters,andreportaveragescoresover speciﬁcsubsetsofthedata. Theresultsareshown The UUParser was run on two E5-2660 v3’s (40 in Table 4. The ﬁlters show aggregates over a) threadstotal,weusedonly30),andtookonaver- whetherthetrainingportionofthedatasetissmall ageapproximately20hoursonasinglethreadper (< 30,000 words) or large b) whether the dataset model. </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Joint morphological tagging and lemmatization results for all datasets. </Abstractive Summary>  <Extractive Summary> =  Excludingexternalembeddings D Fullresultsformorphologicaltagging helpedusreducetherunningtimeandmemoryus- andlemmatization age even further. For the UUParser, a maximum of8GBramisenoughfortrainingasinglemodel Table 6 shows the results of the Multi-Team tag- (4GBonaverage),andtheMulti-Teamtaggerre- ger(U¨stu¨netal.,2019)onthedevelopmentdatafor quiresaminimumof8GBofGPURAM. eachdataset. </Extractive Summary>  </Table ID = 6>  <Table ID = 5>  <Abstractive Summary> =  Table 5: LAS scores on all development splits of the dependency parser. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 419> 

<Paper ID = 420>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Foreventsconsistingofmultiple get discourse connectives, and so in practice we i j sub-wordtokens,wetrackonlytheﬁrsttokenpo- only rarely need to invoke the parser. Table 9 in sition. Weusetheconventionofpassingeventsin the Appendix shows collected data samples, and textorder,where1 ≤ i < j ≤ n. </Extractive Summary>  </Table ID = 9>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Transfer comparison for data with and with- 1k Test 50.8 58.7 54.5 outmasking. </Abstractive Summary>  <Extractive Summary> =  Our results presented in provetransferoverrawUDS-T.Thisisinspiteof Table1showthattheDistantTimexdataworkssub- very different event distributions between UDST stantiallybetterthanBeforeAfter: withtheDistant- andMATRESandacompletelackofexamplesof Timexdata,thereisacorrelationbetweenadding VAGUErelationsduringtraining. moredistantly-labeledexamplesandincreasedper- Effect of Masking In Table 3, we test the ef- formance. While both of these rules target par- fectofmaskingonmodelgeneralization. </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  of existing datasets. Table 2 shows results from Understanding the data distribution We re- 3Onepossiblereasonisthatexplicitindicatorslikebefore port the most frequent events from each dataset andafter maybeusedexplicitlytocommunicatetemporal inTable4. MATRESishighlyfocusedonreport- informationwhereitcannotbeotherwiseinferred,buttimexes ing verbs, but the distant data has a much ﬂatter are often used to communicate more speciﬁc details about eventswheretherelationmayalreadybeclear. </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Training data label distribution. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  creasesacrosstheboardunderthisevaluation. Fullimplementationcanbefoundathttps:// D MostCommonEvent-LabelTuples github.com/xyz-zy/distant-temprel Table 8 presents a comparison of the most com- B LabelCompositionAcrossDatasets mon(event1,event2,label)tuplesacrossdatasets. Table5comparesthelabeldistributionofourdis- InMATRES,themostcommontuplesarelargely tant data against the human-annotated MATRES (event,“said”, BEFORE)events. </Extractive Summary>  </Table ID = 8>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Comparison of majority-vote ensembled per- formance on {BEFORE, AFTER, EQUALS} examples (“−Vague”) versus performance on the entire test set. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  </Paper ID = 420> 

<Paper ID = 421>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Vectorsinthe whichissufﬁcientforconvergence. collection of a target type are clustered to derive 5 Discussion 3www.paracrawl.eu 5.1 WhyContextualEmbeddingMapping? 4https://lindat.mff.cuni.cz/ repository/xmlui/handle/11234/1-3226 Compare with Previous Methods: Overall re- 5We do not use the composition of subword vectors to sults are shown in Table 1. In the ﬁrst place, our approximatelyrepresentOOVtokens,becauseourpreliminary resultsshowthishurtsthemapping. ping. In Table 1, the performance of sense-level Peter F. Brown, Stephen A. </Extractive Summary>  </Table ID = 1>  </Paper ID = 421> 

<Paper ID = 422>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Data statistics for ﬁve domains from Multi- one-stageﬁne-tuning. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  for the Arabic portion of the ACE 2005 dataset, (2019),whichhasseenthefulltrainingset.5 thetrain/dev/testsplitsforArabicarerandomlyse- lected. Table 3 shows statistics of the ACE 2005 ResultandAnalysis Themainresultsareshown events dataset for our experiments. The gradual inTable2. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Identiﬁcation and classiﬁcation F1 scores for trainmodelsonmonolingualandmixedbilingual triggersandargumentsonArabicACE2005. </Abstractive Summary>  <Extractive Summary> =  #Roletypes 22 21 #Events/Arguments Train 4202/4859 1743/2506 Restaurant Hotel Dev 450/605 117/174 Slot/Joint Slot/Joint Test 403/576 198/287 NoFT(singledomain) 90.70/52.16 90.79/46.30 NoFT∗(alldomains) 92.19/58.63 91.48/50.26 Table3: StatisticsforEnglishandArabicACE2005. One-stageFT 93.47/61.15 91.43/46.30 GradualFT 94.30/67.27 92.49/52.12 Baselines The ﬁrst baseline in Table 4 is the Table 2: Slot and joint accuracy in the restaurant and modelfromWaddenetal.(2019)(withanXLM-R hotel domains under various training methods. * indi- encoder) trained only on Arabic data, the second cates that the model uses the full training set. </Extractive Summary>  </Table ID = 4>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Slot and joint accuracy in the restaurant and modelfromWaddenetal.(2019)(withanXLM-R hotel domains under various training methods. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 422> 

<Paper ID = 423>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We evaluate from one pair of domains (standard–biomedical theclustersusingNormalizedMutualInformation / standard–twitter) separately through the trans- (c.f. Table 1). Clustering quality is higher for up- formers,obtainingtwosetsofrepresentations. </Extractive Summary>  </Table ID = 1>  </Paper ID = 423> 

<Paper ID = 424>  <Table ID = 1>  <Abstractive Summary> =  Table 1: The top section shows AMI ASR tran- ferent threads may be intermixed in the post se- scripts and the bottom section shows human-written quence; see a meeting transcript from the AMI summaries. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: We use lowercase italics for variables, up- smallnumberofPubMedabstractsroughlyresem- percase italics for sets and sequences, math symbols blesinterleavedtexts,and,correspondingly,inter- for mathematical operations and uppercase words for leavingoftitlesresemblesitsmulti-sentencesum- methods. </Abstractive Summary>  <Extractive Summary> =  Theavail- thetitlesoftheselectedabstractsarereturned. ableonesareeitherextractiveVerberneetal.(2018) or too small (Barker et al., 2016; Anguera et al., We ﬁrst refer to Table 2 for terms and no- 2012)totrainaneuralmodelandthoroughlyver- tations used in Algorithm. 1. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Synthetic interleaved text summarization T,ofthethreadisaddedtothemulti-sentencesum- marysequence,Mˆ,ifitdidn’texistpreviously. </Abstractive Summary>  <Extractive Summary> =  interleaved). The ﬁrst row in Table 4 shows per- ...toassesstheeffectofaprogramofsupervisedﬁtness walkingandpatienteducationonfunctionalstatus... formance of the hier2hier summarization model. Whilehier2hieristrainedend-to-end,andtherefore, generatesmulti-sentencesummariesforagivenin- Table 5: An example of hier2hier generated summary terleaved text. Table 4 shows Shang et al. (2018) sentencesofathreethreadinterleavedtext. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We the experiments. Table 3 shows an example of a padshortsequenceswithaspecialtoken,(cid:104)PAD(cid:105). data instance from a Interleaved PubMed corpus WeuseAdam(KingmaandBa,2014)withanini- compiledusinga=2,b=5,m=2andn=5. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: An example of hier2hier generated summary terleaved text. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  different domains, weuse the byte pair encoding OuranalysisoftheAMIcorpusshowthat90%of (BPE)(Sennrichetal.,2016)basedsubworddic- meetingshave≤12summarysentenceswhile60% tionary. As shown in Table 6, hier2hier readily of meetings have ≥ 8 summary sentences, so we transfers its disentangling knowledge, and there- used8and12asthemin(a)andmax(b)number fore, obtains a boost in recall while maintaining of threads respectively in the algorithm and cre- its precision. The Li et al. We use only AMI data for 1)theprojectmanageropenedthemeetingandwentover thedenovotrainingandﬁne-tuningpurpose,and theminutesoftheprevious... the bottom two rows in Table 6 show the results ... fromthesemodels.1 Althoughourhier2hier t-learn 3)theindustrialdesignerdiscussedtheinteriorworkingsof aremoteandtheteam... </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  Table 7: The top and bottom sections show our hier- itativeevaluationofoursystemusinghumanjudg- archicalandtheShangetal.(2018)systemsummaries ments. </Abstractive Summary>  <Extractive Summary> =  a)referto performedacomparativeevaluation,wherewepro- theathsentenceinamulti-sentencesummary. videdsixhumanjudges(graduatestudentsﬂuentin English)withmeetings(≈ 6000words)andsum- asinTable1andourmodelandShangetal.(2018) maries from three sources, i.e., human reference, summaries as in Table 7. The judges were not two-stepbaselineandhier2hier t-learn(hereafter shown the source of the summaries. </Extractive Summary>  </Table ID = 7>  </Paper ID = 424> 

<Paper ID = 425>  </Paper ID = 425> 

<Paper ID = 426>  <Table ID = 2>  <Abstractive Summary> =  Table 2: In-domain Dataset description for three do- 5: Mt(cid:48) ←TranslateMs usingNMTs→t mainsMedical,LawandITwhereMonolingualrefers 6: FM(cid:48) ←Filtering(M(cid:48)) s s to in-domain sentences both in source and target lan- 7: FM(cid:48) ←Filtering(M(cid:48)) t t guage.DevandTestsetconsistsofin-domainbilingual 8: Sp ←synthetic Parallel Data(FMs(cid:48),Mt) sentences. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Translation of a German (de) sentence to En- class and in-domain as another. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Wetokenizetheout-of-domainsentencepairsas Here, we compare and discuss the results of our wellasin-domainsentencesusingmoses(Koehn proposed approach along with other baseline et al., 2007) and apply byte-pair-encoding (Sen- methods. As shown in Table 4, we compare nrichetal.,2016b)with37K mergeoperations. the BLEU scores in two different scenarios, 266Domain→ Medical Law IT LP→ de-en en-de de-en en-de de-en en-de HighResource BASE 33.61 24.98 33.07 23.33 21.93 16.27 BT 41.05 36.32 38.27 28.32 35.31 24.80 sent-LM 47.44 37.85 40.82 30.35 39.24 30.11 IBT 47.71 38.01 39.46 29.04 38.93 29.37 DDSWIBT 45.46 36.45 39.11 29.04 - - CFIBT 47.59 37.61 40.99 30.38 40.06 29.93 LowResource BASE 10.05 6.53 8.52 6.84 5.05 3.70 BT 22.64 14.02 18.47 10.53 13.07 10.79 sent-LM 36.21 29.35 25.36 17.28 28.80 24.32 IBT 33.14 24.31 22.96 14.31 28.06 24.16 DDSWIBT 31.22 28.12 22.06 13.28 - - CFIBT 37.61 30.63 27.18 18.88 29.56 25.82 Table4:WeusetheBLEU(Papinenietal.,2002)scoretocompareCFIBTwithexistingbaselines,BASE(Vaswani et al., 2017), BT (Sennrich et al., 2016a), sent-LM (Imankulova et al., 2017), IBT (Hoang et al., 2018) and DDSWIBT (Douetal.,2020). </Extractive Summary>  </Table ID = 4>  </Paper ID = 426> 

<Paper ID = 427>  </Paper ID = 427> 

<Paper ID = 428>  <Table ID = 1>  <Abstractive Summary> =  Table 1: A comparison of sizes of existing and our corpus. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Inter-annotator agreement with respect to negation cue and scope. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 428> 

<Paper ID = 429>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Speciﬁcally,Turkcorpuscontains 8 reference simpliﬁcations, and ASSET contains 3.3 TaggingModel 10referencespersourcesentence. Table 1 provides other statistics on these We use the GECToR sequence tagging model datasets. withapre-trainedRoBERTa Transformer(Liu BASE etal.,2019)astheencoder,stackedwithtwocon- 4.2 Datapre-processing current feed-forward layers, followed by corre- WikiAlldatacontainsspecialtokenstorepresent spondingSoftmaxlayers. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  competitiveperformanceonthetask. Ontheother The results in Table 3 show that the inference hand,theTST-FINALmakessigniﬁcantimprove- speeds10 of TST are at least 6 times faster than ments over TST-BASE. On TurkCorpus and AS- ACCESS and11.75timesfasterthanpure BART SET,itcomeswithin1SARIpointofthecurrent whichisthecrucialcomponentofthecurrentstate- state-of-the-art(Martinetal.,2020b),outperform- of-the-art (Martin et al., 2020b). </Extractive Summary>  </Table ID = 3>  </Paper ID = 429> 

<Paper ID = 430>  </Paper ID = 430> 

<Paper ID = 431>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Outputs of a tagged Seq2Edits corruption ERRANT(Feliceetal.,2016;Bryantetal.,2017). </Abstractive Summary>  <Extractive Summary> =  1c) allows other tags both before and after awell-studiedproblemcalledmaximumweighted SPELL. bipartite matching (Schrijver, 2003) and can be Table 1 lists example outputs of a tagged solved efﬁciently with a standard minimum-cost Seq2Edits corruption model for all 25 ERRANT ﬂowsolver. tags and demonstrates the model’s capability to Ofﬂine-Probabilistic The intuition behind the generateabroadvarietyofrealisticerrors. </Extractive Summary>  </Table ID = 1>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Adapting GEC to non-native or native En- using either a full sequence Transformer (row c) or a glish. </Abstractive Summary>  <Extractive Summary> =  The 2- 200M stageﬁne-tuningsetupisdescribedinSec.4.1. AdaptingGECtoEnglishproﬁciencylevels A the real parallel data (42.1 F Table 5b). This 0.5 potential use case for tagged corruption models demonstratesthepotentialoftag-basedcorruption is to adapt a GEC model to the proﬁciency level forimprovingGECofnativeEnglish. Asbeforeweﬁne-tunetheseed that follows the BEA-dev tag distribution works modelontheBEA-traintargetsentenceswithsyn- wellinacontrolledsetup(corrupting∼60Kclean theticsourcesentencesthatfollowoneofthesetag targetsentencesfromBEA+FCE).Wenowapply distributions, but evaluate the ﬁne-tuned models thesamecorruptionmodeltoamuchlarger,clean on the A2, B2, C2, and N2 splits. Table 5 shows data set (C4 ) consisting of 200M sentences 200M thatthetagdistributionsfromA1,B1,andC1yield andusetheresultingsyntheticdatasetasanaddi- similar performance (rows c-e in Table 5) across tionalpre-trainingsetforourGECmodels. Fig.4 mosttestsets. Asbeforeweﬁne-tunetheseed that follows the BEA-dev tag distribution works modelontheBEA-traintargetsentenceswithsyn- wellinacontrolledsetup(corrupting∼60Kclean theticsourcesentencesthatfollowoneofthesetag targetsentencesfromBEA+FCE).Wenowapply distributions, but evaluate the ﬁne-tuned models thesamecorruptionmodeltoamuchlarger,clean on the A2, B2, C2, and N2 splits. Table 5 shows data set (C4 ) consisting of 200M sentences 200M thatthetagdistributionsfromA1,B1,andC1yield andusetheresultingsyntheticdatasetasanaddi- similar performance (rows c-e in Table 5) across tionalpre-trainingsetforourGECmodels. Fig.4 mosttestsets. </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ofthesyntheticdata.9 ComparingTable4withTable3weobservethat Data generation without tags Fine-tuning the controllingthetagdistributionofthesyntheticdata seedmodelontherealparalleldataimprovesthe from a Seq2Edits model outperforms traditional F0.5-score on BEA-dev by 16.7 points (33.7 → back-translationwithouttags. Ourbestmodel(On- 50.4 in rows a and b of Table 4). Our goal is line column in Table 3c) achieves an F -score 0.5 to close the gap relative to the F0.5 of 50.4 using of48.0whichismuchbetterthanourbestsystem syntheticsourcesentences. Our goal is line column in Table 3c) achieves an F -score 0.5 to close the gap relative to the F0.5 of 50.4 using of48.0whichismuchbetterthanourbestsystem syntheticsourcesentences. Thecorruptionmodels without tags (42.4 F in Table 4c) and remark- 0.5 in rows c and d of Table 4 do not use any error ably close to the oracle score of 50.4 F (Table 0.5 tags,whichissimilartopreviousattemptstoapply 4b)obtainedusingamodeltrainedonrealparallel back-translationtoGEC(Kasewaetal.,2018). data. Our goal is line column in Table 3c) achieves an F -score 0.5 to close the gap relative to the F0.5 of 50.4 using of48.0whichismuchbetterthanourbestsystem syntheticsourcesentences. Thecorruptionmodels without tags (42.4 F in Table 4c) and remark- 0.5 in rows c and d of Table 4 do not use any error ably close to the oracle score of 50.4 F (Table 0.5 tags,whichissimilartopreviousattemptstoapply 4b)obtainedusingamodeltrainedonrealparallel back-translationtoGEC(Kasewaetal.,2018). data. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Ourbestmodel(On- 50.4 in rows a and b of Table 4). Our goal is line column in Table 3c) achieves an F -score 0.5 to close the gap relative to the F0.5 of 50.4 using of48.0whichismuchbetterthanourbestsystem syntheticsourcesentences. Thecorruptionmodels without tags (42.4 F in Table 4c) and remark- 0.5 in rows c and d of Table 4 do not use any error ably close to the oracle score of 50.4 F (Table 0.5 tags,whichissimilartopreviousattemptstoapply 4b)obtainedusingamodeltrainedonrealparallel back-translationtoGEC(Kasewaetal.,2018). data. Forallexperimentsintheremainderofthispa- Data generation with tags Table 3 reports re- per we used the unconstrained tagged Seq2Edits sultsfromthetag-basedcorruptionmethodsintro- corruptionmodels(Table3e)becauseityieldsrea- ducedinthiswork. Seq2Edits(rowsb-e)ismore sonablegainsacrossallmethods(Ofﬂine-Optimal, amenabletotag-basedcorruptionthanastandard Ofﬂine-Probabilistic,andOnline)andiseasiestand fullsequenceTransformermodel(rowa)because most practical to run on a large scale.10 Further- tag prediction is a component of the Seq2Edits more,wewillonlyusetheOnlinemethodtoavoid model. </Extractive Summary>  </Table ID = 3>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The BEA-dev tag distribution reﬂects whichcoversabroaderrangeoftexttypes. Inthe awiderangeofgrammaticalerrorsacrossvarious ablation experiment in Table 7, rather than using proﬁciencylevelscomparedtoothercorporasuch C4 ,wecorruptedtheWikiRevisiontargetsen- asCoNLL-14(mostlybeginner)orFCE(School) 200M tences with various corruption methods. Tagged (Bryant et al., 2019). </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Tagged (Bryant et al., 2019). Table 8 situates this single corruption(rowc)outperformsbothuntaggedcor- modelandanensembleofﬁveanalogouslytrained ruption(rowa)andround-triptranslation(rowb) modelsinthecontextofrelatedwork. Forourﬁnal whenthetargetsentencesarekeptconstant. </Extractive Summary>  </Table ID = 8>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  modelsinTable8wefollowLichtargeetal.(2019, A crucial practical question is whether our ap- 2020);StahlbergandKumar(2020)andmultiply proachissensitivetotheparticulartargettagdistri- themodelscoreoftheidentitymappingwithafac- butionP∗(t),andifthesyntheticC4 training tor(tunedonthedevelopmentset)tobalancepre- 200M datacanhelpgeneralizationtootherdevelopment cisionandrecall.12 Oursinglemodeloutperforms sets. Rows b-e in Table 6 show the performance other single models on CoNLL-14 and JFLEG- afterﬁne-tuningforfourdifferenttagdistributions: BEA-dev, CoNLL-13, JFLEG-dev, and Uniform. 11Formoreinsightintothedifferencebetweenuntagged Eachrowreportstheperformanceofamodelpre- andtaggedcorruptionsseeAppendixB. </Extractive Summary>  </Table ID = 6>  </Paper ID = 431> 

<Paper ID = 432>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Theheterogeneityofthetextscould lowertheoverallresultswhenincludedintheset. explainthelowclassiﬁcationaccuracyforthelexi- Table 2 summarises the best results from the calfeatures(56.15%),andthegrammarcomponent experimentsdiscussedabove. Forallthreecorpora, of the curriculum the slightly higher results for thebestperformingclassiﬁersarethosetrainedon morphology (58.26%) and syntax (65%). </Extractive Summary>  </Table ID = 2>  </Paper ID = 432> 

<Paper ID = 433>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Human scores on character sets constructed guishthesimilaranddissimilarpairs. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 433> 

<Paper ID = 434>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Structural negative language transfer error countsfromthetestdataset We envision a similar application of our results. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Length statistics of part-of-speech tagged Table4: Numberofsequencesbelongingtothetrain- trainingsequences ingandevaluationsplitsthatwereusedinhyperparam- etertuning 3.3 Datapreprocessing theycomputeitslikelihoodbasedonthetraining As previously mentioned, we used Chinese and probabilitydistribution. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  N-gramshallowsyntacticlanguage varying n-gram lengths (2 to 6). See Table 4 for modelsrepresentalanguage’sstructureasthedis- the number of training and evaluation sequences tribution of POS tag sequences in the language. used in this tuning process. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  These distinc- as“error+unigram”and“error+bigram”spans. tive aspects prompted the design of experiments Table 5 illustrates the POS tag spans used in the thatevaluateddifferentspansoferroneousPOStag experiments. sequences. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Theprecedingandsubsequent Compared to the POS tag sequences used for tagswereonlyincludediftherewerewordsbefore trainingthen-gramandRNNmodels,thetestPOS and after the error, e.g., when the error occurred tag sequences extracted from learner errors are on the ﬁrst word of a sentence, the preceding tag short. Table 6 presents a summary of the test se- was not included in the input test sequence as it quences’ lengths. The median values for these didnotexist. </Extractive Summary>  </Table ID = 6>  </Paper ID = 434> 

<Paper ID = 435>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Performance of MultiEnc-dec on BEA- FCE-test dev after each training stage and ablation tests. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  suggeststhatitiscrucialtohavebothpre-training andﬁne-tuningstagesasperformancedropswhen 3.4 Results removingeitherofthem. In Table 1, we can see that simply concatenating Contextlength Figure2showshowtheperfor- precedingsentences(SingleEnc)doesnotyielda mance changes in relation to an increasing num- consistentimprovementinF (recallimprovesat 0.5 ber of context sentences. The best performance thecostofprecision). </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  VERB:SVA+10.40F0.5,PRON isusefulinGECbutverylongcontextisnotnec- +8.32, and VERB:TENSE +5.95 - see Example essary for improved performance. Experiments (a) in Table 3). It is not surprising that our sys- on three test sets demonstrated the effectiveness temisgoodathandlingerrorsthatcrosssentence ofourdocument-levelGECmodels. </Extractive Summary>  </Table ID = 3>  </Paper ID = 435> 

<Paper ID = 436>  <Table ID = 1>  <Abstractive Summary> =  Table 10: Pearson’s r comparing feature values computed using each TCs extraction method with human (gold- standard) Evidence essay scores. </Abstractive Summary>  <Extractive Summary> =  Thisdimensionisalsodirectly man scores with automated essay quality signals related to the ﬁnal output of this work, Topical for training, while still achieving state-of-the-art Components. Table 1 shows the distribution of TCextraction. Evidence scores for both forms of the RTA. methods. We consider that good automated TCs Table 10 shows Pearson’s r comparing feature shouldcovertopicsinTC asmanyaspossi- manual valuescomputedusingeachTCextractionmethod ble. Therefore,wemanuallylabelatopicforeach withhuman(gold-standard)Evidenceessayscores, ofthemanualtopicwords. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  claims. Table 2 shows a source article excerpt, To support the needs of the AWE system for thecorrespondingprompt,andastudentessayof the RTA (Zhang et al., 2019), the feature values RTA . The bolded phrases in the source ex- andpredictedevidencescoresfromAES are MVP rubric cerptareevidenceexamplesmanuallylabeledby passedtotheAWEsystemtoselectformativefeed- 87SourceExcerpt:Today,YalaSub-DistrictHospitalhasmedicine,freeofcharge,forallofthemostcommondiseases.Water isconnectedtothehospital,whichalsohasageneratorforelectricity.BednetsareusedineverysleepingsiteinSauri... </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ﬁrstrequiredtomanuallyextractTCs(TCmanual) in the form of two lists related to evidence in the source text: 1) a topic words list of important keywordsthatindicatethemainsetofarticletop- ics, and 2) a speciﬁc examples list that includes backmessagessuchas“Usemoreevidencefrom phrasesrepresentingspeciﬁcexamplesforarticle the article” (based on NPE values) or “Provide topics. Table 3 shows a partial topic words list more details for each piece of evidence you use” for RTA , where the four topics (“hospital”, MVP (basedonNPEandSPCvalues). “malaria”,“farming”,and“school”)andtheassoci- atedkeywordsweremanuallycreatedbyahuman Although AES thus provides useful expert. </Extractive Summary>  </Table ID = 3>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  SinceTDSistopicdistributionsimilarity 40%fortesting. Weusethesamehyper-parameters between essays and the source article, the score fromZhangandLitman(2018)fortheco-attention range is [0, 1], so we do not scale it in the exper- neural network training as shown in Table 6. Ta- iments. theQuadraticWeightedKappa,wescaledthepre- 8 ResultsandDiscussion dictedscorebacktotheoriginalscorerange,which is[1,4]. Allhyper-parametersfortheAES neural Table8,whichaddressesH1,showstheQuadratic training are shown in Table 6. Please note that WeightedKappabetweenhumanevidencescores we used essay scores for the development set to and predicted scores by AES using differ- neural determine early stopping. </Extractive Summary>  </Table ID = 6>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  manual es mated(TC ,TC )methods;H2b)thecorrela- AlthoughbothWCandTDSunderperformthe wc tds tion between the human evidence score and the human-generatedEvidenceScores,TDSconstantly TC-dependent feature values will be comparable outperforms WC, despite the fact that WC has when extracting features using either TC , highercorrelationswithEvidenceScorethanTDS manual TC ,TC ,andTC . Extrinsically,theexperi- (recall Table 5). One possible reason is that the es wc tds mentforH2aexaminestheimpactofusingourpro- human evidence score assesses if an essay men- posedTCextractionmethodsonthedownstream tionsandelaboratesevidencefromthesourcearti- AES task. </Extractive Summary>  </Table ID = 5>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  andareextractedbasedonTCs. Table 9, which addresses H2a, shows the Fortheseexperiments,westratifyessaycorpora Quadratic Weighted Kappa between human evi- followingZhangandLitman(2020): 40%fortrain- dence scores and predicted scores by AES rubric ingwordembeddingsandextractingTCs,20%for when using different TCs. On RTA , TC MVP wc 91Prompt TC (1) TC (2) TC (3) TC (4) manual es wc tds RTA 0.643 0.648(1) 0.645 0.652(1,2,3) MVP RTA 0.609(4) 0.622(1,4) 0.622(1,4) 0.599 Space Table9: Theperformance(QWK)ofAES usingdifferentTCsextractionmethodsforfeaturecreation. </Extractive Summary>  </Table ID = 9>  </Paper ID = 436> 

<Paper ID = 438>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We report or K12SEG),withearlystoppingbasedontheloss theresultsofthein-domainWikipediaevaluation on the respective development set. We train in on WIKI50, CITIES, and ELEMENTS in Table 2. batches of 32 instances (i.e., 32 sequences of N OurHITSmodelvariants,whichwestarttraining sentences)andhavefound(viacross-validationon with the pretrained RoBERTa as the token-level respectivedevelopmentsets)theoptimalsequence transformer, outperform the hierarchical neural length to be N = 16 sentences and the optimal modelsfrom(Koshoreketal.,2018)and(Glavasˇ 113Model Fine-tuning WIKI50 CITIES ELEM. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Segmentation performance in domain trans- fer. </Abstractive Summary>  <Extractive Summary> =  Domain Transfer Results. Table 3 shows the performance of both in-domain and transferred 4 Conclusion HITS model variants on the K12SEG test set. Interestingly, with Full ﬁne-tuning, we observe Supervisedtextsegmentationhasbeenlimitedtoa the same performance regardless of whether we singlelarge-scalesegmentationdataset,WIKI727, train the model on the out-of-domain (but much builtautomaticallyfromWikipedia. </Extractive Summary>  </Table ID = 3>  </Paper ID = 438> 

<Paper ID = 439>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Even though 2https://meta.wikimedia.org/wiki/List_ thesetypesofrule-basedcorruptionmethodsdonot of_Wikipedias 118ﬁnetuning on gold data. For our experiments, cs de ru es weusethedatagenerationscriptsfromLichtarge Artiﬁcial etal.(2019)togathertrainingexamplesfromthe Unimorph 71.08 60.87 32.91 44.68 Wikipedia edit history (see Table 1); we refer to Aspell 71.53 63.49 32.86 48.22 thisdatasourceasWIKIEDITS. Aspell+Unimorph 71.90 62.55 35.95 48.20 Lang8 Lang8isasociallanguagelearningweb- WikiEdits site,whereuserscanposttextsinalanguagethey WikiEdits 55.14 58.00 23.92 47.35 Artiﬁcial→WikiEdits 74.64 66.74 40.68 52.56 are learning, which are then corrected by other Artiﬁcial+WikiEdits 72.91 66.66 42.80 51.55 userswhoarenativeorproﬁcientspeakersofthe Summary language. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  +Unimorph). 3https://github.com/tensorflow/ Table 2 shows that using only Unimorph per- tensor2tensor 4Weusedthe“transformer clean big tpu”setting formstheworst. Thisisexpectedsincethesystem 119wouldonlylearntocorrectmorphologicalsubsti- 80 tutionerrors. </Extractive Summary>  </Table ID = 2>  </Paper ID = 439> 

<Paper ID = 440>  </Paper ID = 440> 

<Paper ID = 441>  </Paper ID = 441> 

<Paper ID = 442>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Most frequent grammatical errors and sets extracted from a spellchecker was used by alternative-correct (AC) answers in the annotated one of the top-scoring systems at the Restricted dataset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Results of evaluation of ﬁne-tuned BERT models on assessing grammatical correctness: CE—cross- entropy loss, wCE—weighted cross-entropy; the numbers denote the number of layers; s—sentence training in- stance,p—paragraphtraininginstance,t—scoresaftermovingdecisionthresholds. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  TousethemodelinRe- notonlyinﬂectionerrors. vita,wecancomputetheentropyofpredictedscores BERT, ﬁne-tuned on synthetic data, performs anddisregardthepredictionswhentheentropyis comparably with the baseline (see Table 7). It high. </Extractive Summary>  </Table ID = 7>  </Paper ID = 442> 

<Paper ID = 443>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Acontains BeforecomparingtheTransformermodelswiththe 7,323,502interactions,involving34,696students stateoftheart,weshowheretheperformanceof and13,603uniquequestions;onaverage,eachitem thedifferentconﬁgurations,bothwithandwithout isansweredby304studentsandeachstudentan- the additional pre-training on MLM. Table 1 dis- swersto115differentitems. Theoverallcorrect- playstheMeanAbsoluteError(MAE)andtheRoot nessis66%. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Theresultsshownhereareobtained 2012-13-school-data-with-affect 4https://sites.google.com/ onQTEST forCloudAcademy;indeed,forASSIST- site/assistmentsdata/home/ mentsthetextofthepossiblechoicesisnotavail- assistments-problems 5https://cloudacademy.com/ 7https://huggingface.co/transformers 6https://pypi.org/project/pyirt/ 8https://www.tensorflow.org/ 151Figure2: Experimentalsetup. able and therefore the only possible input conﬁg- Table 2 shows that also R2DE and ELMo are uration is Q only. Even though there is not one able to leverage the text of the possible choices modelconﬁgurationwhichclearlyoutperformsall toimprovetheaccuracyoftheestimation;indeed, the others, it can be seen that both the additional the best performing input conﬁguration is Q+all pre-trainingandthetextualinformationofthepos- for R2DE and Q+correct for ELMo. Results are similar for the RMSE: isdifferentthantheoneintheoriginalpaper). the increase is 1.19 for the best model, between Table 2 and Table 3 show the results of the ex- 1.19and1.30fortheotherTransformers,1.23for perimentsonQDEforCloudAcademyandASSIST- ELMo(Q+correct)and1.37forR2DE(Q+all). ments,bydisplayingtheMAEandtheRMSEob- tained on Q by the Transformer models and TEST thechosenbaselines(allthepossibleinputconﬁg- Table3showsresultssimilartoTable2: BERT urationsareconsideredforthebaselines). </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Results are similar for the RMSE: isdifferentthantheoneintheoriginalpaper). the increase is 1.19 for the best model, between Table 2 and Table 3 show the results of the ex- 1.19and1.30fortheotherTransformers,1.23for perimentsonQDEforCloudAcademyandASSIST- ELMo(Q+correct)and1.37forR2DE(Q+all). ments,bydisplayingtheMAEandtheRMSEob- tained on Q by the Transformer models and TEST thechosenbaselines(allthepossibleinputconﬁg- Table3showsresultssimilartoTable2: BERT urationsareconsideredforthebaselines). </Extractive Summary>  </Table ID = 3>  </Paper ID = 443> 

<Paper ID = 444>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Example passage (taken from the TQA tions.Weevaluateviabothautomaticandman- dataset), extracted cause/effect, and generated ques- ualmetricsandﬁndperformanceimprovesfor tions. Table 11: Average cause/effect presence recall in the C-I.1.c 2 3 2 3 TQA and SQuAD datasets, categorized by typology. Table 10: Number of extracted relations that were la- questionstotheSyn-QGquestions,theﬁne-tuned beledascausalbycrowdworkersfororiginalCaoetal. </Abstractive Summary>  <Extractive Summary> =  However,theoriginalpatternassumes sage, intended answer, and generated question, from thatthecauseisalwaysbefore“as.” Inreality,“as” TQAdataset. canbeincludedbeforeboththecauseandtheeffect, suchasinthefollowingexample: Table 10 contains the crowdworker ratings for theCaoetal.(2016)causalextractionsystem,strat- Somerenewableresourcesaretooexpen- iﬁedbytypology. Eachmaintypologycategoryis sivetobewidelyused. approximate runtime to ﬁne tune this model on an auxiliary dataset on a p3.2xlarge AWS ec26 E Cause/EffectPresentResults machineis0.5hours. Forourﬁne-tuningprocess, Table 11 shows the results for the automatic wetrainfor3epochswithalearningrateof1e-6 cause/effectpresentmetricsegmentedbytypology withabatchsizeof1. Therestofparametersare categories. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ofcausalrelationsextractedusingeachpartofthe Weuseasliding3-sentencewindowtoexamineall typology, using the TQA dataset, can be seen in possible sentence combinations. Multiple causal Table 2. By incorporating these patterns in our relationshipsmaybefoundinasinglepassage;the evaluation framework, we can examine potential slidingwindowcapturesthemultiplerelationships gaps in model performance that might be tied to acrossdifferentpassages. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Percent of extractions labeled as causal by token,whichhelpstopreventoverﬁttingonstrong crowdworkers,forsamplesof100eachfromTQAand local correlations. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Average cause and effect recall in questions Table7:F1scoresofbestQG-QApair(bothﬁne-tuned generated by ProphetNet Base and ﬁne-tuned on Syn- onSyn-QG)brokendownbytypology. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  whetheraquestionwascausal,thereisnotacon- Results stratiﬁed by typology for the best- sistentwinnerbetweenthesetofcauseandeffect performing QG and QA pair (both ﬁne-tuned on questions. ThisindicatesProphetNetisconsistently Syn-QG) can be seen in Table 7. The lowest- betteratgeneratingquestionsforbothdirectionsof performing category for both TQA and SQuAD thecause-and-effectrelationship. </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Percentage of Base ProphetNet generated effect questions was to generate questions which questions(n=324)classiﬁedbycrowdworkersascausal fallhigheronBloom’staxonomy. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  Table 9: Percentage of crowdworking ratings indicat- this(StasaskiandHearst,2017). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 9>  </Paper ID = 444> 

<Paper ID = 445>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Top positive features for ‘Apoyo whileforexperimentsofthesecondgroupweuse fonoaudio´logo(a)’interventioninexperimentusingall denseneuralrepresentationsoftheobservations. </Abstractive Summary>  <Extractive Summary> =  Weanalyzetheweightstheclassiﬁer representationsasdifferentfeaturesasinputforour assigntoeachfeatureandobservethatforcertain classiﬁer. interventionsthereareinterventionsusedasbinary Forthisstudyweusethefollowingmetrics: ac- attributesshowingahighpositiveweightandinthe curacy,Cohen’skappascore(Vieiraetal.,2010), same way, for other interventions some interven- F1scoreaverageofpositiveandnegativeclassand tionsusedasabinaryattributewithahighnegative AreaUndertheReceiverOperatingCharacteristic weight as well, as shown in Table 4 and Table 5, Curve(ROCAUC)(Flach,2016)scoreofeachex- showingthetoppositiveandnegativefeaturesus- periment for each intervention. In this work we ing‘SpecialNeedsteachersupport’interventionas reportthemacro-averageofalltheinterventions. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Top negative features for ‘General medical attributesformakingbetterpredictionswhenusing support’ intervention in experiment using all observa- theprofessionalsobservations. </Abstractive Summary>  <Extractive Summary> =  Weanalyzetheweightstheclassiﬁer representationsasdifferentfeaturesasinputforour assigntoeachfeatureandobservethatforcertain classiﬁer. interventionsthereareinterventionsusedasbinary Forthisstudyweusethefollowingmetrics: ac- attributesshowingahighpositiveweightandinthe curacy,Cohen’skappascore(Vieiraetal.,2010), same way, for other interventions some interven- F1scoreaverageofpositiveandnegativeclassand tionsusedasabinaryattributewithahighnegative AreaUndertheReceiverOperatingCharacteristic weight as well, as shown in Table 4 and Table 5, Curve(ROCAUC)(Flach,2016)scoreofeachex- showingthetoppositiveandnegativefeaturesus- periment for each intervention. In this work we ing‘SpecialNeedsteachersupport’interventionas reportthemacro-averageofalltheinterventions. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Macro-averaged sparse n-grams experiments results of all interventions. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Macro-averaged dense neural representations experiments results of all interventions. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  </Paper ID = 445> 

<Paper ID = 446>  </Paper ID = 446> 

<Paper ID = 447>  </Paper ID = 447> 

<Paper ID = 448>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Distribution of texts by CEFR proﬁciency levelandtextlengthstatistics(inwords) theREPROLANG2020sharedtaskon‘Language proﬁciency scoring’5. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Confusion matrix of the contour-based RNN L1 writing development. </Abstractive Summary>  <Extractive Summary> =  Thesensitivity Si1,t,...,Sim,t aremlargest of feature fi on the nth fold at step t is obtained amongallSi,t(i F)in 2 from: descendingorder; list list.append([f ,...,f ]); 8 i1 im S = g(X ) g(Xi ) F F f ,...,f ; i,t,n t,n   t,n 9  { i1 im} Theﬁnalsensitivityforafeaturefi atsteptis: 10 t t+1; 11 returnlist k 1 S = S i,t i,t,n k nX=1 for the C1 level and 0.42 for the C2 level. The Themostimportantfeatureatsteptcanbefound confusionmatrixofthecontour-basedRNNmodel by: is presented in Table 3. As is evident in this ta- f :ˆi = argmax(S ) ˆi i,t ble,mostclassiﬁcationerrorsappearedinadjacent Thenwesettherankfoir:ffie2aFtutref : categories,withfewclassiﬁcationerrorsoccurring ˆi Rank = t betweendistantcategories. </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  As is evident in this ta- f :ˆi = argmax(S ) ˆi i,t ble,mostclassiﬁcationerrorsappearedinadjacent Thenwesettherankfoir:ffie2aFtutref : categories,withfewclassiﬁcationerrorsoccurring ˆi Rank = t betweendistantcategories. ˆi In the end, feature f is dropped from F and the The top 20 features that contributed most to ˆi t correspondingcolumnsintrainingandtestdataset the classiﬁcation accuracy of the contour-based arealsodroppedsimultaneously: RNN model are shown in Table 2 (see the col- F = F f umn ‘Acc after Del’). The results of the feature t+1 t { ˆi} ablationexperimentsrevealedthatclassiﬁcationac- Thisprocedureisrepeated,until F = 1. </Extractive Summary>  </Table ID = 2>  </Paper ID = 448> 

<Paper ID = 449>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Token counts of each category in the training, dev, and test sets of ARG2020 makeidentifyingargumentclaimsandpremises Eger et al., 2017)) and others at the sentence from ARG2020 more challenging. </Abstractive Summary>  <Extractive Summary> =  We apply a ﬁve-way token- we have a modest agreement of 0.71 that is classiﬁcation (or sequence tagging) task while comparable to (Stab and Gurevych, 2014) and using the standard BIO notation for the claim for premise, we have a high agreement of 0.90. and premise tokens (See Table 1). Any token Out of the 145 essays from ARG2020 we thatisnot“B-Claim”,“I-Claim”,“B-Premise”, randomly assign 100 essays for training, 10 or “I-Premise” is denoted as “O-Arg”. As ex- essays for dev, and the remaining 35 essays for pected, the number of “O-Arg” tokens is much test. Table 1 represents the data statistics in larger than the other categories (see Table 1). the standard BIO format. As ex- essays for dev, and the remaining 35 essays for pected, the number of “O-Arg” tokens is much test. Table 1 represents the data statistics in larger than the other categories (see Table 1). the standard BIO format. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: F1 scores for Claim and Premise Token Detection on the test set. </Abstractive Summary>  <Extractive Summary> =  bedding layer and projected to a single-layer Finally,weextractlexico-syntacticfeatures(de- BiLSTM of 256D. BiLSTMs provide the con- noted as lexSyn in Table 2) that include the text to the token’s left and right, which proved dependency relation governing the token in to be useful for sequence tagging tasks. We the dependency parse tree and the token it- train this model with and without a CRF de- self, plus its governing dependency relation as coder to see its e↵ect on this task. caseofpremisetokens(SeeTable5inAppendix We denote this model as BERT . Next, A.3) and therefore, we only report the results IMHO we train on a task and domain relevant of all discrete features (Discrete* in Table 2) corpus of around 10K essays that we obtained and individually only the performance of the originally (See section 3) from the Writing lexSyn features. Stab and Gurevych (2017) Mentor App, excluding the annotated set of noticed that structural features are e↵ective ARG2020 essays. Accuracy improves by 5.3% in the case conventions. Table 2 displays that the lexSyn of BiLSTM against the Embeddings+lexSyn feature independently performs better by al- features. However,resultsdonotimprovewhen most 8% accuracy than the combination of we augment the CRF classiﬁer on top of the the other discourse features. tomersseevalueinWalmartthatisabsentfrom In summary, we have two main observations other retailers] ”. Here, the premise tokens claim from Table 2 and Table 3. First, the best are wrongly classiﬁed as O-Arg tokens. First, the best are wrongly classiﬁed as O-Arg tokens. This is model in Table 3 reports only about 3% im- probably because the premise appears before provement over the result from Table 2 which the claim, which is uncommon in our training shows that the ﬁve-way token-level classiﬁca- set. We notice some of the other sources of tion is comparable against the standard task of errors, and we discuss them as follows: argument segmentation. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: F1 scores for Argument Token Detection argumentative and non-argumentative tokens. </Abstractive Summary>  <Extractive Summary> =  mt Accuracy, and Macro-F1 (abbrev. as “Acc.” Adaptive Pre-training Learning We and “F1”) scores for all the categories in Table adaptively pretrained BERT over two un- 2 and Table 3. labeled corpora. According to the confusion ma- I-Premise are merged into I-Arg, while the O- trix, therearethreemajorsourcesoferrors: (a) Arg class remains unchanged. The results of around 2500 “O-Arg” tokens are wrongly clas- all of our models on this task are shown in siﬁed as “I-Claim” (b) 2162 “I-Claim” tokens Table 3. We notice similar patterns (except are wrongly classiﬁed as “O-Arg”, and (c) 273 for BERT that performs better than mtIMHO “I-Premise” tokens are erroneously classiﬁed as BERT this time) in this three-way classiﬁ- mt “I-Claim”. tomersseevalueinWalmartthatisabsentfrom In summary, we have two main observations other retailers] ”. Here, the premise tokens claim from Table 2 and Table 3. First, the best are wrongly classiﬁed as O-Arg tokens. First, the best are wrongly classiﬁed as O-Arg tokens. This is model in Table 3 reports only about 3% im- probably because the premise appears before provement over the result from Table 2 which the claim, which is uncommon in our training shows that the ﬁve-way token-level classiﬁca- set. We notice some of the other sources of tion is comparable against the standard task of errors, and we discuss them as follows: argument segmentation. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Accuracy and F1 scores for Claim and A.1 Parameter Tuning Premise Token Detection on the test set for each group of the discrete features in the CRF model. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 449> 

<Paper ID = 450>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  All Toassignitemstoclasses,p-valueandmeanre- items were MCQs. An example practice item2 sponsetimearerescaledsuchthateachvariablehas is given in Table 1. The exam comprises several ameanof0andastandarddeviationof1. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  bilitythatresponseprocesscomplexityissimplya functionofitemlength. 5 Results Standard Item Features: This baseline com- Table 3 presents the classiﬁcation results for the prisesWordcount,Presenceofanimage,Content baselines, the full feature set, and the selected category,PhysiciantaskandYear. Thismodelre- features for both logistic regression and random ﬂects the standard item characteristics that most forests. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Confusion matrix for the results from the se- scribed using slightly different language. </Abstractive Summary>  <Extractive Summary> =  sight into this model’s performance. As could Embeddedmethods: TheﬁrstmethodisLASSO beexpected,themajorityclassoflow-complexity regularizedregressionwhereinthecoefﬁcientsof itemswaspredictedmoreaccuratelythanthehigh- variablesthathavelowcontributionstowardsthe complexity class, as shown by the confusion ma- classiﬁcation performance are shrunk to zero by trix in Table 4. An interesting observation was forcingthesumoftheabsolutevalueoftheregres- madeduringafollow-upclassiﬁcationexperiment, sioncoefﬁcientstobelessthanaﬁxedvalue. </Extractive Summary>  </Table ID = 4>  </Paper ID = 450> 

<Paper ID = 451>  </Paper ID = 451> 

<Paper ID = 452>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  As expected, 4.1 ExperimentalDesign theresultsontheKLEJBenchmarkarereallypoor Hyperparameters Unless stated otherwise, in withtheaveragescoreequalto58.15. all experiments we trained BERT model ini- Next,weevaluatedtheBERTmodelinitialized BASE 4with XLM-RoBERTa weights (see Table 2). It Corpus SSO BPE Score achieved much better average score than the ran- domlyinitializedmodel(83.15vs58.15),butitwas Small 1.0 Yes 88.73±0.08 still not as good as the original XLM-RoBERTa Large 1.0 Yes 89.10±0.19 model(88.82%). </Extractive Summary>  </Table ID = 2>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Part-of-speech tagging results on NKJP XLM-RoBERTa 95.38±0.02 93.66±0.07 dataset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  </Paper ID = 452> 

<Paper ID = 453>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  WetooktheresultsofmT5-smalland baseline provided by the organizers was used for gpt3 models. In Table 2 we present the scores TERRa. depending on the different range strategies. </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Human evaluation of paraphrasers perfor- ConferenceonArtiﬁcialIntelligence,volume32. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 453> 

<Paper ID = 454>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Uppercase and profane word distribution acrossthedataset 4 Experiments 4.1 DataPreprocessing Thestagesofpre-processingarethefollowing: 1. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 454> 

<Paper ID = 455>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  32Figure 2: Interface of appropriateness labeling task. Translation: upper line – text: “You should give up smok- ing and urgently consult cardiologist”, middle line – task: “Read the sentence and indicate whether this phrase generatedwithchatbotcanharmthereputationofthecompanywhichcreatedthischatbot?”, possibleanswers– “Yes/No” Table 2 shows the number of samples on each sensitive topic in both datasets. While we tried 17500 0.7 to keep the topic distribution in the topic dataset s15000 0.6 e balanced,sometopics(drugs,politics,healthsham- ple12500 0.5 tag ing) get considerably more samples in the appro- # sam170500000 00..34 Percen pthraiatttehneescsladsastiaﬁseert.peTrhfoisrmmaignhctebfeorrethlaetseedttooptihcsewfaacst 5000 0.2 good,soutterancesclassiﬁedwiththesetopicswith 2500 0.1 highconﬁdenceemergedoften. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Wenoticedthattheclassiﬁerperformance ples. Its performance is outlined in Table 3. The forindividualclassesiscorrelatedwiththenumber scores are quite high, and the results of training ofsamplesoftheseclassesinthedata(Spearman’s withtensplitsarequitestable. </Extractive Summary>  </Table ID = 3>  </Paper ID = 455> 

<Paper ID = 456>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Wealsoper- retain the model with minimum median distance formanidenticalhyperparameteroptimisationand ondevelopmentdata. TheresultsinTable4show experimentation and report the results in Table 3. thattheBERTi´cmodelimprovedtheresultsofthe Theresultsshowagainthatthetwobestperforming sharedtaskwinner–thecseBERTmodel. </Extractive Summary>  </Table ID = 3>  </Paper ID = 456> 

<Paper ID = 457>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Results of Logistic Regression classiﬁer for each encoder over the shared English and Russian tasks. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 457> 

<Paper ID = 458>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thetaskofsubgroupdiscoveryistoﬁndinterest- 4 Results ing subgroups in the population, i.e. subgroups thathaveasigniﬁcantlydifferentclassdistribution Table 2 presents the rules describing groups thantheentirepopulation(Klo¨sgen,1996;Wrobel, of documents with positive or negative senti- 1997). The result of subgroup discovery is a set ment. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Quality of the rules describing subgroup pre- sentedinTable2. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 458> 

<Paper ID = 459>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Characteristics of adapted and original texts. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Alignment statistics. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Simpliﬁcation evaluation – larger automati- callyalignedtestsets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Simpliﬁcation evaluation – small manually hassomemodiﬁcationsthathavefurtherimproved alignedtestset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  </Paper ID = 459> 

<Paper ID = 460>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Results for the Multilingual training bundle we sample K = 3 random chosen occurrences per language. </Abstractive Summary>  <Extractive Summary> =  among them. The bottom part of Table 1 and Ta- The most “complex” training bundle is the ble7containresultsofaM-Casedmodel,trained Slovene-Miscellaneousbundleistrainedtwo-fold: in a FF setting. We observe that the results of a ﬁrston4-categorydatasets,andthenusesanaddi- multi-lingualmodelaremuchlowerthanourinitial tionalmodeltosplittheMISCclassintoPROand results. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Ofﬁcial results for the Slovene part of BSNLP 2021 Shared Task data. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  </Paper ID = 460> 

<Paper ID = 461>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Difference in performance obtained for shown to outperform CRF approaches (Yu et al., the NORD_STREAM and RYANAIR topics of the 2020);(iii)ﬁnetuningthetoplayersofthecontex- SLAVNER2021whenmodifyingourapproach. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  tualembeddingmodelduringtraining,whichhasa positiveimpactoversimplefeatureextraction(Sun experiments have the following goals: (i) com- etal.,2019);and(iv)train/developmentsetsusing pareourapproach’sperformancewiththeofﬁcial allnon-testavailabletopics. SLAVNER2019 scores; and (ii) evaluate the im- Theﬁrsthypothesismatcheswhatwehaveob- pactoftheaddedlanguagesforthesametopicsin served in the results presented in Table 1. In par- SLAVNER2021. Itis mentsweusetheavailabledatawiththeaforemen- plausible to attribute this to the unique character- tionedtopicsplits. istics of the languages and entities of the task at The obtained results can be seen in Table 1. hand, where making part of the parameters train- The impact of further ﬁnetuning Multilingual able allows the model to learn better contextual BERT with the four SLAVNER2019 languages representationsfortheNERtask. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Table 4: Entity-level results for the test set of the 3rd With regard to the entity-level results, as ob- editionofthesharedtask. served in Table 4, our scores for PRO and EVT seem subpar. After analyzing the error-log ﬁles, experiments, and for all SLAVNER2019 experi- we noticed some common mistakes: (i) We miss ments excluding the one that used Multilingual somequotationmarks,e.g.,wepredictabcdinstead BERT.Secondly,removingthecharacter-levelem- of «abcd»; (ii) Covid-19 related tags are mostly beddings seems to have an almost negligible im- erroneously classiﬁed as PRO and not as EVT; pact. </Extractive Summary>  </Table ID = 4>  </Paper ID = 461> 

<Paper ID = 462>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Entity linking results evaluated on SlavNER (Relaxedpartialmatching) testdata(Documentlevel,systemTLD3) bg cs pl ru sl uk ingspeciﬁcentity,basedontheentitymentionand PER 89.3 95.7 93.9 87.6 95.5 96.5 context. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In Proceedings of the 7th Workshop on Balto- Evaluationresultsforthenormalizationtaskare Slavic Natural Language Processing, pages 89–93, Florence, Italy. Association for Computational Lin- summarized in Table 5. In almost all cases the guistics. </Extractive Summary>  </Table ID = 5>  </Paper ID = 462> 

<Paper ID = 463>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Strict micro F-scores obtained by each model for every language and topic. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 463> 

<Paper ID = 464>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Since data provided in this shared task are annotated on the document level a different type The summary statistics of the dataset can be of metric needs to be used. The task deﬁnition3 found in Table 1. The distribution of documents speciﬁestwoevaluationtypesthataretobeusedto showsthatBREXITisbyfarthelargesttopic,hav- evaluatethistask: relaxedandstrictevaluation. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Named Entity type disambiguation rules ap- once)weapplythedisambiguationrules6 outlined pliedinpostprocessing inTable2. </Abstractive Summary>  <Extractive Summary> =  This výhody Ryanair Choice patrˇí možnost..." (trans- factmightbeusedtoimprovetheperformanceof lated to English as "The advantages of Ryanair thespeciﬁclanguagemodelswithlongertraining Choice include"), our models are not able to dis- time and higher regularization as both languages ambiguatebetweenthetwoacceptabletagsandthe arelargelypopulatedinthedataset. rules mentioned in Table 2 then push the predic- tion to ORG, resulting in an incorrect prediction. Forinstance,"Boeing747"isanotherexampleof companynameincludedintheproducttag,which couldbeswayedtotheORGpredictionbythena- tureofthedisambiguationrules. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: F1 score results on the RYANAIR topic (dev set). </Abstractive Summary>  <Extractive Summary> =  6.1 ImpactofTransformer-basedpre-trained Slovene (SL) part of the development set, where models the performance increased by more than 10% in absoluteF1scores,from78.46to88.42incaseof As the results across the whole dataset suggest Polish and from 73.34 to 83.90 for Slovene. The (column All in Table 3), the Transformer-based same phenomenon was not observed on Trankit- pre-trainingusingvariantsofBERT,whichcanbe basedmodelswheretheinclusionoftheWikiANN foundinbothspaCyandTrankit,generallyyields dataset actually hampered the ﬁnal performance, betterresultsthanthepre-trainedLSTM-basedlan- comparedtothemodelthatonlyusedthetraining guagemodelusedinStanza. WhileaStanza-based dataprovidedwiththesharedtask. We hypothesize this may be due to the factthataBERT-basedmodelwasnotusedforthis 6.4 Erroranalysis language/toolkitpair,asdescribedinSection4.4. Stanza As the Table 3 shows, the worst perfor- 6.2 Impactoftokenizers manceofStanzasystemsisrecordedfortheEVT We can observe the impact of various tokenizers tagsandthebestscoreisachievedfortheLOCtags. introducedinSection4.1intheresultsreportedfor Figure1indicatesthattheEVT’serrordistribution theStanzaandTrankittoolkitsinTable3. (last row of the confusion matrix). The system In particular, the results in Table 3 show that the isincapableofdisambiguationbetweensaidtags, inclusion of the WikiANN dataset (marked with as the dataset itself can contain such ambiguities WikiANN) helps the Stanza model better perfor- as is also mentioned in the Task Guidelines. We mancethanthatreportedbyspaCy(88.55vs85.14). </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Results of the submitted systems on the test set. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 464> 

<Paper ID = 465>  <Table ID = 3>  <Abstractive Summary> =  Table 3: 2019 BSNLP Shared Task selected results (strict recognition evaluation, test set, F1 metric). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  CogComp-6 Ryanair 0.88 0.94 0.91 0.94 0.92 RIS-slav_lemma Ryanair 0.86 0.94 0.92 0.91 0.91 Lemmatization We analyzed the inﬂuence of Cog_Tech_Cent-4 Ryanair - - - 0.91 0.91 variouspartoflemmatizationontheperformance IIUWR.PL-4 Ryanair 0.76 0.87 0.84 0.79 0.82 TLR Ryanair 0.76 0.83 0.82 0.83 0.82 of our method. The results are shown in Table 4. Sberiboba Ryanair 0.65 0.84 0.81 0.72 0.77 Ourbaselineistheidentityfunction,inwhichwe JRC-TMA-CC-1 Ryanair 0.64 0.55 0.52 0.79 0.64 NLP_Cube Ryanair 0.15 0.13 0.19 0.18 0.16 assumeaphrasebeingitsownlemma. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: 2019 BSNLP Shared Task results (cross- algorithm on the 2019 BSNLP Shared Task training languagelinking,testdata). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: 2021 BSNLP Shared Task selected results COVID-19,whichiscommoninthetestdata. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  </Paper ID = 465> 

<Paper ID = 466>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  For recognition, we show only the relaxed pass those benchmarks by a substantial margin. evaluation, since the results obtained on the three The best results for each team, averaged across evaluation schemes are correlated, as can be seen two corpora, are shown in Table 3. These results fromFigure3. </Extractive Summary>  </Table ID = 3>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thisvariationshould undertakefurtherreﬁnementoftheunderlyingan- andwillbeinvestigatedingreaterdepth. notation guidelines—a highly complex task in a In Table 6 we present the results of the evalua- real-worldsetting. Morecomplexphenomenaalso tion by entity type. </Extractive Summary>  </Table ID = 6>  </Paper ID = 466> 

<Paper ID = 467>  </Paper ID = 467> 

<Paper ID = 468>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Statistics of the Tamil lyrics dataset and angry. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Examples from the Tamil lyrics corpus. </Abstractive Summary>  <Extractive Summary> =  of unique Tamil words in the corpus is close to Figure1depictsthenumberofsongswritten 130, 000. Table 2 presents examples from the by a lyricist across different years. Evidently, corpus. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Number of songs written by a lyricist and the most frequent 10 words by them are presented. </Abstractive Summary>  <Extractive Summary> =  The top 10 lyricists with the highest word. WeattributethistothenatureofTamil number of songs and their most frequent 10 movie songs which usually circles around the words are presented in Table 3. Vaali ranks emotion, romance or love (காதல்). </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Number of songs written in a decade and the most frequent 10 words per decade is given. </Abstractive Summary>  <Extractive Summary> =  first with 873 songs followed by Kannadasan (797 songs), and Vairamuthu (390 songs). Table 4 presents the number of songs per decade and the top 10 most frequent words We noticed that the top 10 most fre- per decade. For example, there are 312 songs quent words across the entire corpus (Table:3, includedinthecorpusbelongingtothedecade 1st row), and the top 10 most frequent words (1970),thatisfromtheyear1961to1970. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Words based similarity between pairs of the top 5 lyricists from the corpus. </Abstractive Summary>  <Extractive Summary> =  Our further investigation revealed that The top 10 most frequent words are extracted the top 100 words mostly included pronouns, after removing the stop words. interrogativewords,prepositions,andconjunc- tions which are considered as stop words in In Table 5, we present similarity scores be- English. Thus we decided to treat the top 100 tween pairs of lyrics among the top 5 artists. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Results of binary classifiers to classify between Vaali and Kannadasan given a song Tamil lyric is presented. </Abstractive Summary>  <Extractive Summary> =  the songs released after the year 2000 would include more English words (code-switching) 6.2 Results than the songs from the 1960s. For instance, We present results in Table 6. The results ob- the song Why this kolaveri? has more than tained using multilingual BERT is lower than 50% English words. </Extractive Summary>  </Table ID = 6>  </Paper ID = 468> 

<Paper ID = 469>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Annotators and their characteristics. </Abstractive Summary>  <Extractive Summary> =  Also,twooftheannotatorsacted thefollowinginstructions: as annotation veriﬁers. Table 1 shows details of theannotatorswitheducationalqualiﬁcation,gen- • Extracttheoffensivewordsequences(spans) derdiversity,Mediumofinstructioninschooling, of the comment by highlighting each such miscellaneous qualities, including knowledge of spanandlabelingthemasCAUSEasshown multipleaccentsofKannada/Tamil. EachYouTube inFigure1. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  3.5 CorpusStatistics Language-Pair Tamil-English Kannada-English NumberofSentences 4786 1097 Numberofuniquetokens 22096 7781 Numberofannotatedspans 6202 1641 Averagesizeofspans(#ofcharacaters) 21 20 Figure 3: Histogram of annotated Span size in Tamil- Minsizeofspans(#ofcharacaters) 4 2 Maxsizeofspans(#ofcharacaters) 82 160 Englishdataset. Numberofuniquetokensinspans 10737 3742 Table2: DOSAcorpusstatistics 4 ExperimentalSettings Corpus statistics is given in the Table 2. Com- To establish a baseline performance, we applied pared to Tamil-English, we can see Kannada- multiple state-of-the-art multilingual language English has a signiﬁcantly lesser number of sam- models to determine the span of offensive com- ples. </Extractive Summary>  </Table ID = 2>  </Paper ID = 469> 

<Paper ID = 470>  </Paper ID = 470> 

<Paper ID = 471>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Accuracy metrics of the Language Identiﬁca- wasusedinsteadofthecontinuous-bag-of-words tionmodel. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 471> 

<Paper ID = 472>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  As can be work was built as shown in Figure 1. The de- seen in Table 1, there are three sets of data, out tailed working of the CNN and attention-based ofwhichintheﬁrstsets,Malayalamcode-mixed Bi-LSTM network for text classiﬁcation can be andTamilcode-mixed,thepostswerewrittenina seenin(Jangetal.,2020;Xuetal.,2020;Saumya singlescriptEnglish, butinthelastset, theposts et al., 2019). To CNN, character embedding was were written in two different scripts (Malayalam theinput,whereastoBi-LSTM,wordembedding script-mixed). </Extractive Summary>  </Table ID = 1>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  1 allthreeaforementionedmodelsexplainedinSec- Finally,theexperimentalresultsoftransfermod- tion4. Theresultsarepresentedintermsofpreci- els are shown in Table 5. The table shows the sion,recall,andF1-scoreofclassoffensiveandnot- results of three transfer models BERT, BERT- offensive. </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  However, for the 1 Malayalamscript-mixedtextarelistedinTable4. MalayalamScriptmixed,VanillaNeuralNetwork As can be seen from the Table 4, the vanilla neu- (VNN) reported best results having precision, re- ralnetwork(VNN)modeloutperformedattention- call, and F -score of 0.95, 0.95, and 0.95 respec- 1 based Bi-LSTM-CNN for all three datasets. For tively. </Extractive Summary>  </Table ID = 4>  </Paper ID = 472> 

<Paper ID = 473>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  idating and 1,348 for testing the model. Table 1 (2016). After generating the sub-word level in- gives the distribution of each sentiment in each formation of the input data, they used one Bidi- dataset. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  AstheLinearSVC modelreceivedpoorerresultswithaweightedF1- 6.1 TamilTask scoreof0.61,ithasnotbeenrepresentedinanyof 6.1.1 Sub-wordlevelmodel thetables. Therehasbeenanimprovementinthe From Table 2, one can see that the F1-score of weightedF1-scoresofthenegative,non-Tamiland thepositivecommentsisthehighestwithavalue unknownstateclasses. Aninterestingimprovement of 0.80. Table6representsthe distribution of misclassiﬁed comments across all 6.2 MalayalamTask ofthetestedclasses. 6.2.1 Sub-wordlevelmodel TheclassiﬁcationreportoftheMalayalamtaskcan 6.2.3 MachineLearningmodel be seen in Table 2. The F1-score of the positive Theclassiﬁcationreportofthismodelcanbeseen comments is the highest at 0.73. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  For future work, a sarcasm detection fea- dataset. The result can be seen in Table 3. Com- turecouldbeincludedtoavoidmisclassiﬁcationof paredtothesub-wordlevelmodel,thismodelman- commentsfromthepositive,negativeandmixed- agedtogainahigherF1-scoreinalloftheclasses feelingsclasses. </Extractive Summary>  </Table ID = 3>  </Paper ID = 473> 

<Paper ID = 474>  <Table ID = 1>  <Abstractive Summary> =  Table 1: The number of sentences in the monolingual whereasforourbilingualsupervisedmodelbase- andparalleldatasetsfordifferentlanguages.*Notethat lines we have 5 layers and 2 heads following the theEnglish-Kannadaparalleldataisonlyusedtotrain sameconﬁgurationasinGuzma´netal.(2019). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thisﬁndingisalsoinlinewiththeclose relationbetweenKannadaandTelugufromthelin- 5.5 ImpactofReferenceLanguages guisticliterature(Datta,1988;Bright,1996). This Table 4 provides a clear view of which reference evidenceshowsthattransliterationbridgesacross language is more beneﬁcial to a purely unsuper- differentwritingsystemsandenablessimilarrep- vised system. In line with previous observations, resentations for related languages. </Extractive Summary>  </Table ID = 4>  </Paper ID = 474> 

<Paper ID = 475>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Classiﬁcation Metrics of our GCN with multi-headed approach when compared to baselines on code- mixedMalayalam-EnglishDataset Model Pos Neg not-Tamil Mixed Unknown Macro Weight Acc TF-IDF+SVM 0.80 0.19 0.93 0.06 0.12 0.42 0.56 0.68 CNN 0.81 0.04 0.82 0.01 0.00 0.34 0.56 0.68 LSTM 0.81 0.16 0.85 0.01 0.15 0.40 0.59 0.68 ELMO+SVM 0.81 0.17 0.88 0.04 0.15 0.43 0.58 0.69 mBERT 0.77 0.53 0.99 0.26 0.69 0.65 0.72 0.72 Text-GCN 0.81 0.30 0.98 0.05 0.02 0.43 0.61 0.70 Ourapproach 0.81 0.43 0.98 0.16 0.15 0.45 0.64 0.71 Table 3: Classiﬁcation Metrics of our GCN with multi-headed approach when compared to baselines on code- mixedTamil-EnglishDataset tenconsecutiveepochs. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Classiﬁcation Metrics of our GCN with multi-headed approach when compared to baselines on code- mixedTamil-EnglishDataset tenconsecutiveepochs. </Abstractive Summary>  <Extractive Summary> =  modelifthevalidationlossdoesnotdecreasefor 68Dataset #Docs #Train #Dev #Test #classes Tamil-EnglishCM 15744 11335 1260 3149 5 Malayalam-EnglishCM 6739 4851 540 1348 5 Table1: DataStatistics Model Pos Neg not-mal Mixed Unknown Macro Weight Acc TF-IDF+SVM 0.72 0.40 0.98 0.27 0.58 0.59 0.68 0.67 CNN 0.76 0.39 0.93 0.23 0.63 0.59 0.68 0.69 LSTM 0.74 0.44 0.93 0.26 0.63 0.60 0.68 0.69 ELMO+SVM 0.75 0.45 0.96 0.26 0.63 0.61 0.69 0.69 mBERT 0.77 0.53 0.99 0.26 0.69 0.65 0.72 0.72 Text-GCN 0.79 0.55 0.98 0.30 0.67 0.66 0.74 0.73 Ourapproach 0.80 0.55 0.99 0.30 0.69 0.66 0.75 0.73 Table 2: Classiﬁcation Metrics of our GCN with multi-headed approach when compared to baselines on code- mixedMalayalam-EnglishDataset Model Pos Neg not-Tamil Mixed Unknown Macro Weight Acc TF-IDF+SVM 0.80 0.19 0.93 0.06 0.12 0.42 0.56 0.68 CNN 0.81 0.04 0.82 0.01 0.00 0.34 0.56 0.68 LSTM 0.81 0.16 0.85 0.01 0.15 0.40 0.59 0.68 ELMO+SVM 0.81 0.17 0.88 0.04 0.15 0.43 0.58 0.69 mBERT 0.77 0.53 0.99 0.26 0.69 0.65 0.72 0.72 Text-GCN 0.81 0.30 0.98 0.05 0.02 0.43 0.61 0.70 Ourapproach 0.81 0.43 0.98 0.16 0.15 0.45 0.64 0.71 Table 3: Classiﬁcation Metrics of our GCN with multi-headed approach when compared to baselines on code- mixedTamil-EnglishDataset tenconsecutiveepochs. Weuseddefaultparameter 4.3 ResultsandAnalysis settingsforbaselinemodelsasgivenintheirorigi- Table 3 and 2 presents F1-scores and accuracy nalpapersorimplementationsandusedthesame of each model on Tamil-English and Malayalam- pre-trainedembeddingsobtainedfromthefastText Englishdatasetsrespectively. FortheMalayalam- model. </Extractive Summary>  </Table ID = 3>  </Paper ID = 475> 

<Paper ID = 476>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Dataset statistics for Tamil-English, speciﬁc pre-training on English BERT mod- Malayalam-English and Hinglish (Sentimix) dataset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Monolingual Results for Tamil-English. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Zero-shot prediction results for different models trained on the Train Language. </Abstractive Summary>  <Extractive Summary> =  XLM-RoBERTa and Hinglish datasets (refering to the entire ﬁrst consistentlyperformsbetterthanmBERT. row of Table 4). This is same as looking at the The pre-trained mBERT and XLM-RoBERTa zero-shot performance of the TweetEval model models are trained on multiple languages and in sinceitwastrainedonanEnglishcorpus. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Comparison between performance of XLM- data. </Abstractive Summary>  <Extractive Summary> =  Theresultsforthisexperimentareshown sourcesassociation. in Table 5 and Table 6. Though the combined modelsshowimprovementsforbothdatasets,the Bharathi Raja Chakravarthi, Ruba Priyadharshini, Shubhanker Banerjee, Richard Saldhana, improvementsarenotstatisticallysigniﬁcant. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Comparison between performance of XLM- Eval(Barbierietal.,2020)sentimentclassiﬁcation RoBERTa,TweetEvalandTweetEvalmodelpretrained modelforcode-switcheddata. </Abstractive Summary>  <Extractive Summary> =  Theresultsforthisexperimentareshown sourcesassociation. in Table 5 and Table 6. Though the combined modelsshowimprovementsforbothdatasets,the Bharathi Raja Chakravarthi, Ruba Priyadharshini, Shubhanker Banerjee, Richard Saldhana, improvementsarenotstatisticallysigniﬁcant. </Extractive Summary>  </Table ID = 6>  </Paper ID = 476> 

<Paper ID = 477>  <Table ID = 1>  <Abstractive Summary> =  Table 1: English Transliteration of Tamil usage distribution in the Tamil film songs Uvama Urubugal data set comprising of 4215 songs. </Abstractive Summary>  <Extractive Summary> =  These There are 12 uvama urubugal in Tamil. The transformations have also reflected the beauty Tamil uvama urubugal and their respective of the language followed during every time English Transliteration is given in Table 1. period through the word usages. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Fig 1 shows the experimental set up for detecting the uvama urubugal. Table 2 shows the distribution of the uvama 82urubu in Sangam literature and Tamil film the experiment are correct as per the human songs. expert judgements. Uvama urubu in Tamil is used to express the similes. Tamil language has seen many transformations in Table 2:Uvama Urubugal Count Comparison terms of it’s word usage both in text and Fig 2 shows the graph visualization of the speech forms since 2nd century BC. There comparison of uvama urubugal count in are 12 uvama urupugal in Tamil language and sangam literature and Tamil film songs. </Extractive Summary>  </Table ID = 2>  </Paper ID = 477> 

<Paper ID = 478>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Example of an utterance along with its amounts of training data to effectively slot in BIO (Beginning, Inside, Outside) notation understand the user intent and its cor- and intent label. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Some qualitative examples from our dataset (With word-by-word eng translations) and predic- tions by our mBERT model. </Abstractive Summary>  <Extractive Summary> =  Figure 2 shows Results the architecture of our system. Since mBERT is pre-trained across many languages, we fol- 6.1 Qualitative examples lowthesamearchitectureforourzero-shotand Table 2 shows some qualitative examples from few-shot experiments. our dataset and the predictions made by our model. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Performance of different slot alignment methods tam-5K kan 59.47 37.24 kan-5K, tam-5K kan 93.46 79.75 tam-5K, kan-5K tam 92.76 78.21 6.3 Effect of number of training kan-300, tam-5K tam 93.71 78.82 samples Table 4: Zero and Few shot transfer performance We conduct experiments with varying amounts of data to show the effect of training data size in Table 4. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Zero and Few shot transfer performance We conduct experiments with varying amounts of data to show the effect of training data size in Table 4. </Abstractive Summary>  <Extractive Summary> =  On 1 kan-300 kan 86.27 62.24 our hand-annotated set of 300 examples, we eng-30K, kan-300 kan 81.04 65.82 show that the slot F obtained by our tagging 1 method (§3) outperforms simple word match eng-30K tam 30.50 27.81 andexistingwordalignmentbaselinesinTable tam-300 tam 82.07 59.67 3. tam-5K tam 93.08 78.68 Method Lang Slot F1 eng-30K, tam-300 tam 79.55 65.01 Word Alignment tam 31.17 kan-5K kan 93.13 79.87 Translate+Overlap tam 46.82 kan-10K kan 91.80 79.95 Ours tam 80.70 kan-20K kan 93.79 81.51 Ours kan 80.76 kan-5K tam 62.22 46.16 Table 3: Performance of different slot alignment methods tam-5K kan 59.47 37.24 kan-5K, tam-5K kan 93.46 79.75 tam-5K, kan-5K tam 92.76 78.21 6.3 Effect of number of training kan-300, tam-5K tam 93.71 78.82 samples Table 4: Zero and Few shot transfer performance We conduct experiments with varying amounts of data to show the effect of training data size in Table 4. We can see that even 6.4 Effect of the MT system with 300 examples from our auto-tagged dataset, the model is able to achieve sig- Given the key role of the MT system in the nificant boost as compared to the zero shot dataset construction, we compare the per- settingoftrainingonengandtestingonkan. </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Performance difference due to the inclu- utterances accurately (We examined frequent sion of mBERT wordsinourtrainingcorpus-Remind, Alarm, etc and observed that these words didn’t exist or were very infrequent in the EnTam corpus) lies which also exhibit suﬀix-based morphol- Furthermore, owing to the differences in style ogy. </Abstractive Summary>  <Extractive Summary> =  examples lead to similar performance as com- We can see from Table 7 that our auto- pared to 25K noisy training examples (Table created dataset of 5K examples performs only 5) 12 F1 worse than training on the entire hand- annotated es dataset (Consisting of 3.6K ex- MT System Lang Intent Slot amples) further showcasing the quality of training examples produced by our method. tam-300-Google MT tam 82.07 59.67 tam-5K-Google MT tam 93.08 78.68 Train Intent Slot F1 tam-5K-Seq2Seq tam 80.50 51.32 tam-25K-Seq2Seq tam 81.76 56.56 es-5K-auto 97.63 71.85 es-5k-mBERT 97.13 77.66 Table 5: Performance of different MT systems es-20K-mBERT 97.46 79.47 es-FB 98.78 89.39 6.5 Effect of Semantic Matching Table 7: Performance on a Romance language es We now present results when mBERT based semantic matching is included in the aligner in Table 6. We noticed a small increase in F1 7 Conclusion & Future Work score for kan but a small drop for tam. </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Performance on a Romance language es We now present results when mBERT based semantic matching is included in the aligner in Table 6. </Abstractive Summary>  <Extractive Summary> =  We ing on the hand-annotated es data provided hence observe that 300 high quality training in the Facebook dataset. examples lead to similar performance as com- We can see from Table 7 that our auto- pared to 25K noisy training examples (Table created dataset of 5K examples performs only 5) 12 F1 worse than training on the entire hand- annotated es dataset (Consisting of 3.6K ex- MT System Lang Intent Slot amples) further showcasing the quality of training examples produced by our method. tam-300-Google MT tam 82.07 59.67 tam-5K-Google MT tam 93.08 78.68 Train Intent Slot F1 tam-5K-Seq2Seq tam 80.50 51.32 tam-25K-Seq2Seq tam 81.76 56.56 es-5K-auto 97.63 71.85 es-5k-mBERT 97.13 77.66 Table 5: Performance of different MT systems es-20K-mBERT 97.46 79.47 es-FB 98.78 89.39 6.5 Effect of Semantic Matching Table 7: Performance on a Romance language es We now present results when mBERT based semantic matching is included in the aligner in Table 6. </Extractive Summary>  </Table ID = 7>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Performance of different MT systems es-20K-mBERT 97.46 79.47 es-FB 98.78 89.39 6.5 Effect of Semantic Matching Table 7: Performance on a Romance language es We now present results when mBERT based semantic matching is included in the aligner in Table 6. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 478> 

<Paper ID = 479>  </Paper ID = 479> 

<Paper ID = 480>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Schema errors in Malayalam lexical- Input: A CSV file contains gloss and semantic resource seed term of synset for languages English and Malayalam Output: A CSV file contains id and semantic similarity score shows the steps followed in computing the se- between English and mantic similarity between a word sense in the Malayalam synset and the gloss of MWN. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 480> 

<Paper ID = 481>  </Paper ID = 481> 

<Paper ID = 482>  </Paper ID = 482> 

<Paper ID = 483>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Example 1isatrollmemetargetedtowardsthepotatochip Figure1: Examplesforatrollandnot-trollmemefrom brand called “Lays”. In this example, an image theTamilMemesdataset isharmlesswithjustapictureofthepotatochips 127Rank Team Precision Recall F1-score 1 Codewithzichao(Li,2021) 0.57 0.60 0.55 2 IIITK(Ghanghoretal.,2021) 0.56 0.59 0.54 3 NLP@CUET(Hossainetal.,2021) 0.55 0.58 0.52 4 SSNCSE NLP(SilviaAandB,2021) 0.58 0.60 0.50 5 Simon work(Queetal.,2021) 0.53 0.58 0.49 6 TrollMeta(JandHS,2021) 0.45 0.41 0.48 7 UVCE-IIITT(Hegdeetal.,2021) 0.60 0.60 0.46 8 cean 0.53 0.57 0.43 9 HUB(HuangandBai,2021) 0.50 0.54 0.40 10 iiit dwd(MishraandSaumya,2021) 0.52 0.59 0.30 Table2: TheranklistwithdetailedreportonPrecision(P),Recall(R),F1-score(F1)ofsubmissionsforthe“Troll MemeClassiﬁcationinTamil”task 3 Evaluation comes from the implementation of multi- modal attention which considers the whole Table 1 shows the class distribution in the train- text caption in the context of the image by ing and test set for the TamilMemes dataset. We mappingbothimageandtextfeaturesinthe provided a training set of 2,500 memes (with the samesemanticspace. </Extractive Summary>  </Table ID = 1>  </Paper ID = 483> 

<Paper ID = 484>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  poyichavinedey... and average number of words per sentences) are giveninTable1andclass-wisedistributionisgiven English: “Thosewhodislikeanytrailerswill in Table 2. From the Table 1 we can see that vo- probablybeassholes. Thistexthaveinadmissiblelanguagewithout phology of these languages. Table 2 shows that targetinganyone. all languages have the not-offensive class in the majority. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  and average number of words per sentences) are giveninTable1andclass-wisedistributionisgiven English: “Thosewhodislikeanytrailerswill in Table 2. From the Table 1 we can see that vo- probablybeassholes. Gotohell...” cabularysizeisverybigforTamilandMalayalam ThisisanexampleofOffensiveUntargetedcom- this is due to the code mixing and complex mor- ment. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Rank list based on F1-score along with other evaluation metrics (Precision and Recall) for Malayalam language 138Team-Name Precision Recall F1Score Rank SJ-AJ(JayanthiandGupta,2021) 0.73 0.78 0.75 1 hate-alert(Sahaetal.,2021) 0.76 0.76 0.74 2 indicnlp@kgp(KediaandNandy,2021) 0.71 0.74 0.72 3 Codewithzichao(Li,2021) 0.7 0.75 0.72 3 IIITK(Ghanghoretal.,2021) 0.7 0.75 0.72 3 e8ijs 0.7 0.74 0.71 4 NLP@CUET(Sharifetal.,2021) 0.7 0.74 0.71 4 ALI-B2B-AI 0.7 0.73 0.71 4 Zeus 0.66 0.75 0.7 5 hypers(VasantharajanandThayasivam,2021) 0.69 0.72 0.7 5 bitions(Tulaetal.,2021) 0.69 0.72 0.7 5 SSNCSE-NLP(BandSilviaA,2021) 0.71 0.74 0.7 5 MUCS(Balouchzahietal.,2021) 0.68 0.72 0.69 6 ZYJ123(Zhao,2021) 0.65 0.74 0.69 6 MUM 0.69 0.71 0.69 6 JUNLP(Garainetal.,2021) 0.62 0.71 0.66 7 OFFLangOne(DowlagarandMamidi,2021) 0.66 0.65 0.65 8 cs(ChenandKong,2021) 0.64 0.67 0.64 9 hub(HuangandBai,2021) 0.65 0.69 0.64 9 Nooffense(Awatramani,2021) 0.62 0.7 0.64 9 IRNLP-DAIICT(Daveetal.,2021) 0.69 0.69 0.64 9 JudithJeyafreeda(Andrew,2021) 0.66 0.67 0.63 10 CUSATNLP(RenjitandIdicula,2021) 0.62 0.63 0.62 11 Agilna 0.58 0.65 0.59 12 Amrita-CEN-NLP(Ketal.,2021) 0.65 0.54 0.58 13 snehan-coursera 0.56 0.61 0.54 14 KBCNMUJAL 0.72 0.48 0.52 15 IIITT(Yasaswinietal.,2021) 0.46 0.48 0.47 16 Simon(Queetal.,2021) 0.6 0.3 0.33 17 Table 5: Rank list based on F1-score along with other evaluation metrics (Precision and Recall) for Kannada language 139resentationfromTransformers)forsentence positioninKannada,andtheﬁrstpositionin embedding and a Softmax classiﬁer. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Rank list based on F1-score along with other evaluation metrics (Precision and Recall) for Kannada language 139resentationfromTransformers)forsentence positioninKannada,andtheﬁrstpositionin embedding and a Softmax classiﬁer. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 484> 

<Paper ID = 485>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In this section, we will describe our translation 3 DataPreparation systemwiththeadaptedmethodsindetailfordif- ferentcategoriesoftranslationdirection,including ThedataofDravidianlanguagesprovidedbythe betweenEnglishandDravidianlanguagesandbe- organizersarescarce,withonlytensorhundreds tweenDravidianLanguages. of thousands of parallel sentences per language pair, which is shown in Table 1. In general, it 2http://data.statmt.org/wikititles/v2/ is difﬁcult to train a well-performing translation 3http://data.statmt.org/pmindia/ modelwithsuchasmallamountofdata,becauseit 4http://www.statmt.org/moses/ 147Figure 1: The whole training process of the proposed system. The green data represents pseudo-parallel data which is generated by applying back-translation on monolingual data. 4.1 BetweenEnglishandDravidian data of Ofﬁcial data in Table 1 and Parallel data Languages in Tabel 2), we train a four-to-four multilingual neural machine translation model with eight di- Inthispart,therearethreelanguagepairs: English- rectionsofEnglish↔Tamil,English↔Malayalam, Tamil, English-Malayalam, and English-Telugu. English↔Telugu and Tamil↔Telugu. Fi- AsshowninFigure1,weﬁrsttrainaThree-to- nally,wetrainamultilingualtranslationmodelof Enmultilingualneuralmachinetranslationmodel theseeightdirectionsbasedonthesumofalllan- by the sum of the data of the organizer and we guagesdataasourﬁnalmodel. collected (that is, the sum data of Ofﬁcial data in Table 1 and Parallel data in Tabel 2). With 5 Experiments thiswell-trainedtranslationmodel,weadaptback- translation method on the monolingual data of Weﬁrstintroducethetrainingdetailsandempiri- Tamil, Malayalam, and Telugu (that is, Monolin- callyevaluatethesystemsintwoscenarios. </Extractive Summary>  </Table ID = 1>  </Paper ID = 485> 

<Paper ID = 486>  </Paper ID = 486> 

<Paper ID = 487>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Label distribution of Kannada language sub- acters task • RemovingtheURL sive language, many tasks related to it have been • Removingtheemoticons completedbyscholars. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 487> 

<Paper ID = 488>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Ofﬁcial results and ablations of our model for Tamil, Malayalam and Kannada languages on the test datasets. </Abstractive Summary>  <Extractive Summary> =  In Proceedings of the 1st Joint Workshop on Spoken Language Technolo- 4.2 ResultsandAblations gies for Under-resourced languages (SLTU) and CollaborationandComputingforUnder-Resourced All teams were ranked by the weighted average Languages (CCURL), pages 177–184, Marseille, F1 score. Table 4 shows results of our ensem- France. ble model on all three of languages. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We use mixed preci- References sion training based on Apex library2. We list all hyper-parameters in Table 5. We conduct the ex- Bharathi Raja Chakravarthi, Navya Jose, Shardul perimentsonNVIDIATeslaT4GPUs. </Extractive Summary>  </Table ID = 5>  </Paper ID = 488> 

<Paper ID = 489>  </Paper ID = 489> 

<Paper ID = 490>  <Table ID = 1>  <Abstractive Summary> =  Table 1: The detailed architecture of our model used forthemulticlassclassiﬁcationofembeddings. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: The model descriptions and the results ar- ranged in descending order of test F1 score. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 490> 

<Paper ID = 491>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ThisispassedthroughtheReLuac- We achieve an overall F1-score of 0.96 when we tivationfunctionandadropouttoobtainoneﬁnal use images for classiﬁcation using ViT as shown neuronwhichdeterminestheclassasTrollorNon- in 2. It is to be noted that using mBERT to clas- 183sifymemessolelybasedonthecaptionsachieves FourteenthWorkshoponSemanticEvaluation,pages 0.93 as F1-score as shown in Table 3. While we 1190–1194, Barcelona (online). </Extractive Summary>  </Table ID = 3>  </Paper ID = 491> 

<Paper ID = 492>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Class distribution for Training set in Tamil, siﬁertoidentifyoffensivelanguagefromManglish MalayalamandKannada. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 492> 

<Paper ID = 493>  </Paper ID = 493> 

<Paper ID = 494>  <Table ID = 1>  <Abstractive Summary> =  Table 1: The results of our model and method on the validation set. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: The results of our model and method on the testset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: The results of the Top1 team in the test set. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 494> 

<Paper ID = 495>  </Paper ID = 495> 

<Paper ID = 496>  </Paper ID = 496> 

<Paper ID = 497>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Theotherlossfunctionwe STMs (Schuster and Paliwal, 1997) as their key have tried is the Sadice Loss. We have reported componentstothecurrentmodelshavingstateof the results on various GLUE benchmarks based 6https://huggingface.co/transformers/ 224ourresultsonthedevelopmentsetinTable3and 4.4 ImageModality the results on the test set are reported at Table 4. Imageclassiﬁcationwasthetaskwhichcanbeat- Theresultsarebeingreportedintermsofweighted tributedtorecoilingtheinterestinneuralnetworks averageF1scores. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ﬁne-tunedthemoverourclassiﬁcationtask. According to researchers, both deﬁnitions are Wehavereportedourresultsonthedevelopment equallyacceptableandsettherequiredknowledge and test set in Table 5. The results are being re- base to fully understand a meme. </Extractive Summary>  </Table ID = 5>  </Paper ID = 497> 

<Paper ID = 498>  <Table ID = 1>  <Abstractive Summary> =  Table 1: The parameter settings of our system on getedInsultGroup: 5.29%,OffensiveUntargetede: thetrainingsetsofthreedifferentlanguages: Kannada, 3.41%, Offensive Targeted Insult Other: 1.98%. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Our model and the Top1 team on each lan- layer uses softmax for n classiﬁcation. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 498> 

<Paper ID = 499>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Data statistics for Kannada, Malayalam and tiﬁcation(OLID)datasetbasedtweetsbasedonthe Tamil deﬁnedclassesusingSVM,CNN,BiLSTM,outof whichCNNperformedbetterforthethreelevelsof Thedatasetstatisticsforthethreelanguagesare classiﬁcation. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  guagesinsocialmediacommentsorposts. These Thearchitectureoflanguage-agnosticBERTisin dataset details are in Table 2, and it has 16010 Figure1. tweetsastrainingdatasetand1999tweetsasdevel- opmentdataset. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Weobservethatthesystemisgoodonly Offensive Targeted 0.50 0.07 0.12 14 atclassifyingnot-offensiveclassesandcomments Insult Other thatdonotbelongtotheKannadalanguage. Offensive 0.00 0.00 0.00 33 Themisclassiﬁedinstancescanbeclearlyseen Untargetede through the confusion matrix shown in Table 4. not-Kannada 0.66 0.56 0.61 185 Samples from Offensive Targeted Insult Group, accuracy 0.63 778 Offensive Targeted Insult Other, Offen- macroavg 0.45 0.36 0.38 778 sive Untargetedclassesarethemostmisclassiﬁed weightedavg 0.62 0.63 0.62 778 instances. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thismodel’sbetterclassiﬁcationF1-scoreforthe MalayalamdatasetimpliesthatLaBSEbasedsen- tence representations are suitable for Malayalam codemixedtexts. The confusion matrix for Malayalam dataset classiﬁcation is shown in Table 5. Here we ob- servethatmajorityofsamplesfromeachclassare classiﬁed correctly which lead to an F1-score of Figure2: Systemdesign 0.95. </Extractive Summary>  </Table ID = 5>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Here we ob- servethatmajorityofsamplesfromeachclassare classiﬁed correctly which lead to an F1-score of Figure2: Systemdesign 0.95. 5.3 Tamil Classiﬁcation module : The embedded repre- For the Tamil language, the classiﬁcation report sentation of sentences are passed through a per- for offensive content classiﬁcation is in Table 8, ceptronbasedclassiﬁer. Itisatwo-layerclassiﬁer andweobserveaboveaverageresultsonlyforthe withDense(100)asaninputlayerwithReLUac- not-offensiveclassandanaveragevaluefortweets tivation and Dense(5,6) as an output layer with a thatdonotbelongtotheTamillanguage. </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thesameclassiﬁcationmodule weightedmeanperlabel1 andareusedwhenthe is used. LaBSE based results are compared with themultilingualBERTbasedclassiﬁcationresults 1https://scikit-learn.org/stable/modules/generated /sklearn.metrics.classiﬁcation report.html in Table 9 in terms of its weighted F1 score. We 239PredictedClasses NO OTIG OTII OTIO OU NK NO 333 11 18 0 20 45 s se OTIG 23 10 4 1 3 3 s la OTII 23 4 39 0 5 4 C l OTIO 6 6 0 1 1 0 a u t OU 25 2 4 0 0 2 c A NK 72 0 5 0 4 104 Table4: ConfusionMatrixforKannadadataset PredictedClasses NO OTIG OTII OU NM s NO 1726 3 9 8 19 e s s OTIG 10 12 1 0 0 a l C OTII 12 0 14 0 1 l a OU 11 0 0 18 0 u t c NM 29 0 0 0 128 A Table5: ConfusionMatrixforMalayalamdataset PredictedClasses NO OTIG OTII OTIO OU NT NO 2823 116 79 8 117 47 s se OTIG 172 60 23 1 29 3 s la OTII 177 28 60 7 26 17 C l OTIO 46 8 6 1 10 0 a u t OU 203 37 23 4 93 8 c A NT 65 6 4 0 4 81 Table6: ConfusionmatrixforTamildataset ﬁnd that the performance of both the models are References almostsimilarwithrespecttothistask. </Extractive Summary>  </Table ID = 9>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Results for Malayalam language in terms of Arivazhagan, and Wei Wang. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  </Paper ID = 499> 

<Paper ID = 500>  </Paper ID = 500> 

<Paper ID = 501>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Set of hyperparameters used in building the iest sentence in the dataset. </Abstractive Summary>  <Extractive Summary> =  Mal-Eng 0.90 0.82 0.85 Tam-Eng 0.64 0.62 0.62 6 Conclusion Kan-Eng 0.65 0.54 0.58 This paper presents the submission of Am- Table7: Performanceofthemodeloverthetestdata. rita CEN NLP to the shared task at EACL 2021 on Offensive Language Identiﬁcation from threeDravidianLanguages,namelyTamil-English The set of optimal hyperparameters for this (Tam-Eng), Malayalam-English (Mal-Eng), and model are shown in Table 3. We used the same Kannada-English (Kan-Eng). </Extractive Summary>  </Table ID = 3>  </Paper ID = 501> 

<Paper ID = 502>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In recent years, transformer- (NK), and Offensive-Untargetede (OU). Table 1 based model such as BERT (Sharif et al., 2021), shows the number of instances for each class in XLM-R gained more attention to identify and train,validationandtestsets. Datasetsarehighly classify offensive texts. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Evaluation results of ML, DL and transformer-based models on the test set. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 502> 

<Paper ID = 503>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (2017) toolkit. Table 4 describes the modelconﬁgurationusedinthisexperiment. Afterallthisprepossessingtheﬁnaldatastatis- Table 5 describes the training parameters used tics are explained pairwise in Tables 2 and 3 for by us to model data. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Table 4 describes the modelconﬁgurationusedinthisexperiment. Afterallthisprepossessingtheﬁnaldatastatis- Table 5 describes the training parameters used tics are explained pairwise in Tables 2 and 3 for by us to model data. We validate the model for trainingandvalidation. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Wemadeseveralmodelswithdifferentparameters KishorePapineni,SalimRoukos,ToddWard,andWei- JingZhu.2002. Bleu: amethodforautomaticeval- and vocabulary sizes, Table 6 and Table 7 shows uation of machine translation. In Proceedings of theresultsproducedbythebestmodelsineachlan- the40thAnnualMeetingoftheAssociationforCom- guagepairforvalidationandtestdatarespectively. </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Wemadeseveralmodelswithdifferentparameters KishorePapineni,SalimRoukos,ToddWard,andWei- JingZhu.2002. Bleu: amethodforautomaticeval- and vocabulary sizes, Table 6 and Table 7 shows uation of machine translation. In Proceedings of theresultsproducedbythebestmodelsineachlan- the40thAnnualMeetingoftheAssociationforCom- guagepairforvalidationandtestdatarespectively. </Extractive Summary>  </Table ID = 7>  </Paper ID = 503> 

<Paper ID = 504>  </Paper ID = 504> 

<Paper ID = 505>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Final evaluation (Conneauetal.,2019),multilingual-BERT(Devlin score was calculated using weighted F1-score etal.,2018),MuRIL7 andIndic-BERT(Conneau metriconaheld-outtestdataset. etal.,2019)havebeenintroducedtofacilitateNLP We present the dataset statistics in Table 1. 7https://tfhub.dev/google/MuRIL/1 PleasenotethattheMalayalamsplitofthedataset 271contained no instances of ’Offensive-targeted- other’ label, so classiﬁcation is done using 5 la- Data bels only, instead of the original six labels. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Weighted-F1 score comparison for GA- themodelsandthenpassallthemodelstotheGA weighted ensemble for transformers category, Fu- pipeline. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 505> 

<Paper ID = 506>  </Paper ID = 506> 

<Paper ID = 507>  </Paper ID = 507> 

<Paper ID = 508>  <Table ID = 2>  <Abstractive Summary> =  Table 2: The hyper-parameters settings used to train theXLM-RobertaontheTamilMemesdataset References Languages(HASOC1) competition, many contes- Michal Bilewicz, Miko?Aj Winiewski, Miros?Aw tants using the BERT model rank high. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 508> 

<Paper ID = 509>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Data distribution of the three datasets. </Abstractive Summary>  <Extractive Summary> =  The distribution of the data is described 2020. It was conducted for 5 languages (mul- in Table 1 for all three classes. In total there tilingual) language, namely English, Arabic, are 5936 samples for Kannada, 11695 samples Danish, Greek, and Turkish. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Most frequent words imbalance by assigning more weights to hard 5.2 ULMFiT examples and down-weighting easy examples. </Abstractive Summary>  <Extractive Summary> =  et al., 2020). Table 2 shows a list of the most frequent There have been attempts at developing words for each language for each class. We of- models for hate speech detection in English, ten see the same native word written in differ- Hindi-German (Mandl et al., 2019) and Ital- ent ways in English. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: weighted Precision, Recall, f1-scores for the 3 languages using our best model on the test set imbalance issue, we use class weighting where FiTareusedforasoftvotedensemblestrategy. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  6 Results For DistilmBERT and ULMFiT, we imple- All the results for all experiments are re- ment pseudo labelling. The trained model is ported in Table 3. Similar experiments were used to make predictions on the unseen test carried out in all the languages. </Extractive Summary>  </Table ID = 3>  </Paper ID = 509> 

<Paper ID = 510>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  However, other evaluation metrics like this model’s true positive rate is comparatively 1 precision and recall are also considered in some low than the true negative rate as it identiﬁed cases. Table 2 shows the evaluation results of only 72 not-troll memes correctly and wrongly differentapproachesonthetestset. Theoutcome classiﬁed 200 memes. </Extractive Summary>  </Table ID = 2>  </Paper ID = 510> 

<Paper ID = 511>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Theclass-wisetrainingdatasetstatisticsare largestreamofdatainvolved. Code-Switching2 is a phenomenon common shown in Table 2. Hande et al. </Extractive Summary>  </Table ID = 2>  </Paper ID = 511> 

<Paper ID = 512>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  andresultsaretabulatedinTable2. From Table 5, it has been noted that the per- FromtheTable2,countvectorizerfeaturewith formance of the proposed system is better for 315Features Classiﬁer Precision Recall F1-score Tﬁdf k-nearest 0.63 0.65 0.63 Tﬁdf MLP 0.68 0.70 0.68 Tﬁdf SVM 0.68 0.73 0.69 Countvectorizer k-nearest 0.62 0.65 0.62 Countvectorizer MLP 0.69 0.71 0.69 Countvectorizer SVM 0.67 0.69 0.68 SentenceTransformer MLPclassiﬁer 0.67 0.65 0.64 Table4: PerformanceoftheproposedapproachofKannada-Englishcodemixedtextusingdevdata Dataset Precision Recall F1-score Rank Tam-Eng 0.74 0.73 0.73 6 Mal-Eng 0.95 0.96 0.95 3 Kan-Eng 0.71 0.74 0.70 5 Table5: Performanceoftheproposedapproachusingtestdata Malayalam-Englishcodemixedtext. BharathiRajaChakravarthi,MihaelArcan,andJohnP. </Extractive Summary>  </Table ID = 5>  </Paper ID = 512> 

<Paper ID = 513>  </Paper ID = 513> 

<Paper ID = 514>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The distribution of train, develop- tag Ma-En Ka-En Ta-En ment(Dev.) andtestsetsisgiveninTable2. NO 14153 3544 25425 OU 1287 1522 1454 Statistics of the datasets shown in Table 2 il- OTI 140 329 2557 lustrates that Ka-En texts are less than other two OTG 239 487 2343 languagepairsanditcanaffecttheperformanceof OTO 191 212 2906 NIL - 123 454 theproposedmodels. Further,alldatasetsareim- Dev.Set balanced. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  2020. Leveraging ortho- graphicinformationtoimprovemachinetranslation models are shown in Table 3 and the results il- of under-resourced languages. Ph.D. </Extractive Summary>  </Table ID = 3>  </Paper ID = 514> 

<Paper ID = 515>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Weighted-F1 scores for RNN models on batchsizewas32. </Abstractive Summary>  <Extractive Summary> =  Trans- datasets. The results in Table 2 summarize our formernetworks’performancealsoimprovedwhen RNNapproacheswhereULMFiTismarkedlysu- alltheDravidianlanguagedatasetswerecombined. perior. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Final weighted-F1 scores using average en- submittedﬁnally,andwealsoreportitsscoreson sembling on Tamil (T), Malayalam (M) and Kannada thevalidationandtestsetusingthescoresweare (K)validation(V)andtest(T)datasets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 515> 

<Paper ID = 516>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Data distribution for meme classiﬁcation in Differentcharactern-gramareconstructedwith Tamil n-gram range varying from 1 to 7. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Performance of the meme classiﬁcation task explainedin(ReimersandGurevych,2020). </Abstractive Summary>  <Extractive Summary> =  BERT From the given text transcription, n-gram and multilingualmodeltrainedonalargecorpusfrom TFIDFandBERTembeddingsareextractedasfea- 104differentlanguages(Devlinetal.,2018),which tures. As the content of the memes is a mix of Dravidian language grammar in Roman lexicons 1https://scikit-learn.org/stable/ 337includesMalayalam,Tamil,andEnglishcouldbe Performance of the test set is given in Table 3. considered. </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In 2018 Third International Con- ferenceonInformaticsandComputing(ICIC),pages 1–5. From the Table 2, it has been noted that Tﬁdf andn-gramfeaturesgivingalmostthesameperfor- S.K.Bharti,B.Vachha,R.K.Pradhan,K.S.Babu,and S.K. Jena. </Extractive Summary>  </Table ID = 2>  </Paper ID = 516> 

<Paper ID = 517>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Fur- in the datasets were duplicated both in train and ther,thismodelisevaluatedusingBLEUaswellas testsets,itbecomesessentialtoclean,analyseand humanevaluationagainstgolddatasetwhichispro- correct the dataset before using it for the transla- videdbytheorganizersandperformancemeasure tion experiments. It was also observed that few of the models are as shown in Table 3. Though sentencesarerepeatedmorethan5timesandthis therearemanychallengeswiththetestdataset,con- confuses the model to learn and recognise vari- siderableresultsareobtainedforallthecorpora. </Extractive Summary>  </Table ID = 3>  </Paper ID = 517> 

<Paper ID = 518>  </Paper ID = 518> 

<Paper ID = 519>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In abilityofourmodel. Welistallhyper-parameters Proceedingsofthe58thAnnualMeetingoftheAsso- of our model in Table 4. we conduct the experi- ciation for Computational Linguistics, pages 8440– 8451,Online. </Extractive Summary>  </Table ID = 4>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In2009IEEEConferenceonCom- puter Vision and Pattern Recognition, pages 248– The top ﬁve results in this task have been shown 255. in Table 2. Our model achieved 0.55 weighted Ian J. </Extractive Summary>  </Table ID = 2>  </Paper ID = 519> 

<Paper ID = 520>  </Paper ID = 520> 

<Paper ID = 521>  </Paper ID = 521> 

<Paper ID = 522>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  to OMWN and BabelNet. We choose all four in- The three most frequently colexiﬁed concept stances that are colexiﬁed in more than two lan- pairs in each resource are shown in Table 1. For guages,plustenmoreinstancesthatareselectedat example, the concepts LEG and FOOT are both random. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: The concept pairs with the ratio of 1 represent possible exceptions to our hypothesis. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 522> 

<Paper ID = 523>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The resulting semantic network, that is, UZ- semanticstructureofPWNandchecksifit—node WORDNET, contains 28140 synsets, 64389 sense S in the ﬁgure—exists in the semantic structure B and 20683 words, positioning UZWORDNET at thateventuallybuildstheUzbekwordnet. the 18th place in the list of wordnets ranked by For every synset s from PWN, the algorithm number of synsets, see Table 1 below (also cf. halts when run on s if either it ﬁnds a synset s(cid:48) (Batsurenetal.,2019,Table2).11 fromPWNthatisanindirectparentofsands(cid:48)has an equivalent Uzbek synset according to the dic- Afterthehumanevaluationoversampleentries tionary, namely, s(cid:48) exists in the Uzbek wordnet— as described in the previous section, with a total like S and S , respectively, in Figure 5—or tra- D B number of instances processed be 17425 nouns, versed the whole semantic network of PWN until 5792adjectives,673adverbs,and4250verbs,the it reached the synset at the root without ﬁnding estimatedaccuracyoftheautomatictranslationby a parental synset of s whose semantically equiva- CRAresultedin71.79%(Table2). </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  re-validation resulted in 75.98%. Table 3 reports A question is how polysemy and topological indetailstheresultsofindividualvalidations. data we mentioned on average distance of nodes 13For corpora to test our work on UZWORDNET upon to UZWORDNET’s root correlate. </Extractive Summary>  </Table ID = 3>  </Paper ID = 523> 

<Paper ID = 524>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The OMW format is as fol- from2.0to3.0. lows: Table 1 shows a sample format of sense map- ping that are later used to convert the WordNets offset-poslangcode:lemmawordform from2.0to3.0. 8https://www.sadilar.org/ 7https://wordnet.princeton.edu/documentation/senseidx5wn 9https://pypi.org/project/beautifulsoup4/whereoffsetistheuniqueID(linkingtothePWN), langcode is the universal language code10, word- form is the written word, and pos is the part-of- speech. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The ﬁles have been named ac- NLTK. OMW consists of 29 languages in NLTK cordingtothefollowingformat: as shown in Table 3. There are 8 languages • Sepedi: wn-data-nso.tab in OMW that contains lemmas less than that of Setswana, isiXhosa, and Sepedi. </Extractive Summary>  </Table ID = 3>  </Paper ID = 524> 

<Paper ID = 525>  </Paper ID = 525> 

<Paper ID = 526>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Usage and Domain tags in the Taboo clude 変態オヤジ hentai oyaji “pervert old man” Wordnet and 性格わるい seikaku warui “personality bad”. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The • Helpupstreamwordnetsintegratethisdata resourcesaresummarizedinTable4. • AddmissingsensesforexistingsynsetsinEn- The comparison in Table 5 shows a surprisingly glish(aswehavedoneforJapanese): thecov- small overlap. The original wordnet does very erageofcolloquialexpressionsisstilllow badly. </Extractive Summary>  </Table ID = 5>  </Paper ID = 526> 

<Paper ID = 527>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  system is unsupervised, we use the whole dataset fortesting. 5.1 Approachcomparison Table 2 shows the Top-1, Top-3 and Top-5 accu- 5 EvaluationandResults racyofeachsystemwhenusingthesameobjective This section presents a quantitative and qualita- pattern. Tounderstandbetterthebehaviourofthe tive evaluation. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  them A2T and A2T respectively. FT-small FT-xlingual Table 3 shows the results obtained by testing The ﬁrst one achieve a x425 faster inference (5 different textual patterns. Very short patterns ob- times smaller and 85 times less inferences) while tainlowresults. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Micro-averaged precision, recall and F1 for ”thegamelastedtwohours”whichislabelledby eachofthesystems.Distributional(Camacho-Collados et al., 2016) and BabelDomains (Camacho-Collados our system as ”Sport and recreation”. </Abstractive Summary>  <Extractive Summary> =  5.5 Results In order to know how good is our ﬁnal approach 5.3 Labeldescriptors/Mapping we compare our new systems with the previous Asimportantastheinputpatternsisthesetofdo- ones. The results are reported on the Table 4 in main labels used. Actually, BabelDomains uses terms of Precision, Recall and F1 for comparison labels that refers to one or several speciﬁc do- purposes. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Finally, wealsoplantoexplorethe 5.7 Qualitativeanalysis utilityoftheseﬁndingsintheWordSenseDisam- biguationtask. Table 5 shows some of the top predictions ob- tainedbyaMaskedLanguageModel(MLM)and Acknowledgments the real label for 4 different synsets. In this case, the system is guessing its best predicted domain. </Extractive Summary>  </Table ID = 5>  </Paper ID = 527> 

<Paper ID = 528>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  4.2 for details. Table 1 presents standardised l enmPomlya, ferealtLuEreNs PWN tsheem manettiacl adnegsucarigpetio onf statistics of final data sets. As could be seen the ● gloss levels of datasetwasunbalanced. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Confusion matrices for English test set, Table 4: Confusion matrices for Polish test set. </Abstractive Summary>  <Extractive Summary> =  Intuitively,wecoulddefineclosepolysemy .67∗ .78∗ H 2 89 asapairofsenseswhichare(atleastinonedictio- ∗ cP 295 0 .44 1 ∗ narygraph): mBL dPH 368 0 – 0 • either adjacent nodes in the chain of ordered cP 147.5 147.5 .44 .50 rBL senses, dPH 184 184 .56 .50 • oramainsenseanditssubsense. Table5: ThesubsetofLRandSVMconfusionma- trices presented in Table 2 limited to Lexico and Moreformallywewouldsaythattwosensess and i Merriam-Webster data. Three grades of seman- s ofthesamewordrepresentstherelationofclose j tic similarity/dissimilarity represent: close poly- polysemy(cP)ifthefollowingconditionholds: semy(‘cP’)–distantpolysemy(‘dP’)–homonymy cP := {(s ,s ) ∈ S ×S : (‘H’), as cross-tabulated with binary logistic pre- i j dictions (‘P’, ‘H’). </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Confusion matrices for Polish test set. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Confusion matrices for Spanish test set. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  complementofthe‘cP’settothe‘P’class,i.e. dP := {(s ,s ) ∈ S ×S : (s ,s ) ∈/ H∧ 4.3 Manualevaluation i j i j dist (s ,s ) > 1 ∧ dist (s ,s ) > 2}, LEX i j MW i j Table 6 presents results of the independent man- (23) ual evaluation by the first (#1) and the second (#2) author of this paper. #2 annotated 300 sense whereH isthesetofhomonymycases. </Extractive Summary>  </Table ID = 6>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  #2 annotated 300 sense whereH isthesetofhomonymycases. pairs (100 for each language), randomly selected Figure 3 and Table 5 illustrate how well the lo- from the outcome ‘P’ class of the English lo- gistic classifier and the SVM model deal with the gistic regression model. #1 evaluated a subset two different types of polysemy: close, ‘cP’, and of 100 of those pairs. </Extractive Summary>  </Table ID = 5>  </Paper ID = 528> 

<Paper ID = 529>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  tative”and“Final”states)unlessotherwisenoted. As indicated by Table 3, the fact that the pro- The review state indicates if the mapping has cessedsynsetsarenotoverlydominatedbythose been ﬁnalized by the lexicographers. To ensure with“Incomplete”statusisatestamenttothesuc- consistency and limit individual subjectivity, we cess of our cluster screening and the coverage of adopted a measure-twice-and-cut-once protocol, thecombinedASLlexicaldatabase. </Extractive Summary>  </Table ID = 3>  </Paper ID = 529> 

<Paper ID = 530>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  relation that two senses may possibly have. To Once the senses extracted, we create data in- this end, we use CONCEPTNET (Speer et al., stances using the features in Table 1. Features 2 2016), an openly-available and multilingual se- and 3 concern the length of senses and how they mantic network with relational knowledge from aredifferent. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Basic statistics of the datasets and the best classiﬁcation results with and without an RBM. </Abstractive Summary>  <Extractive Summary> =  Finally,features12and the part-of-speech p. Table 2 provides the basic 13 provide the semantic similarity of each sense statisticsofthesensesandtheirsemanticrelation- pair using word embeddings. For this purpose, shipsinvariouslanguages. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  For in- jorityofcases. Ouroptimalmodelswheretrained stance,therelationshipbetweentwosensesofEN- with50iterations,alearningratewithin[0.05-0.2] TIRE in Table 3, “constituting the undiminished andahiddenunitnumberwithintherangeof400 entirety” and “complete in all parts; undivided; and600. undiminished;whole”isannotatedasnarrower andexactbytwodifferentannotators4. </Extractive Summary>  </Table ID = 3>  </Paper ID = 530> 

<Paper ID = 531>  </Paper ID = 531> 

<Paper ID = 532>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Then there are simple antonyms we propose to add some of them. They are listed (alsoknownascomplementaryorbinaryantonyms) in Table 1. All of these are used in the innovative where the negative of one entails the positive of plWordNetproject(Piaseckietal.,2009)andmany the other. </Extractive Summary>  </Table ID = 1>  </Paper ID = 532> 

<Paper ID = 533>  </Paper ID = 533> 

<Paper ID = 534>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Distribution of conversion and afﬁxal derivation in PWN after changes were performed. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Agent:verb.cognition–noun.person(168) {think:3} 71 22.26 Agent:verb.communication–noun.person(506) cluster’sshareasabsolutenumbersandaspercent- {act:1} 234 46.25 ageofthenumberoftheprimepairoccurrences. {express:2} 66 13.04 {think:3} 46 9.09 Figure 2 exempliﬁes the distribution of synsets Agent:verb.consumption–noun.person(69) belonging to the prime verb.motion which are in- {consume:2} 39 56.52 volved in the relation Agent (see Table 2). More Agent:verb.creation–noun.person(205) {make:3} 127 61.95 than half of the synsets (149 out of 286) are hy- Agent:verb.motion–noun.person(286) ponyms of the synset travel:1; go:1; move:1; lo- {go:1} 149 52.10 comote:1 (the embedded bubbles). Table2: Somesigniﬁcantclusterswithinthemor- As not all derivational relations are assigned a phosemanticrelationofAgent. morphosemanticl label (see above, this Section), Table 2 shows the overall number of occur- thequestionofthepredictabilityofthemorphose- rences for the most numerous combinations of mantic relations arises. Our analysis of several verbprimeswiththeprimenoun.personforthere- samples of the data shows that the relations are lation Agent. </Extractive Summary>  </Table ID = 2>  </Paper ID = 534> 

<Paper ID = 535>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  calUnits. FortheFrugalitycase,weintroduced3 subframes: Frugality Time, Frugality Waste and 3 Results Frugality Money (see Table 2 for frame statis- Inthisstudy,atotalnumberof139Framesin8do- tics). Thereasonbehindwasthemerepatterndis- mainswerecreated6. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: A comparison with initial versions of and create cutting edge NLP tools or train highly otherFrameNets accurate semantic annotators. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 535> 

<Paper ID = 536>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Each dataset consists of a (1) taxonomyandasetofnovelwordstobeaddedto APi = M1 (cid:80)ni preci×I[yi = 1], this resource. The statistics are provided in Table 1. where N and M are the number of predicted and groundtruthvalues,respectively,prec isthefrac- i Dataset Nouns Verbs tionofgroundtruthvaluesinthepredictionsfrom 1 to i, y is the label of the i-th answer in the WordNet1.6-WordNet3.0 17043 755 i ranked list of predictions, and I is the indicator WordNet1.7-WordNet3.0 6161 362 function. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: MAP scores for the taxonomy enrichment methods for the non-restricted English datasets of differentWordNetversions. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Prediction noun examples from the English v 1.6-3.0 dataset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Prediction verb examples from the English v 1.6-3.0 dataset. </Abstractive Summary>  <Extractive Summary> =  overlapswiththeoneoffastText-basedmodels. In other words, we would like to know if the graph We list the candidate synsets predicted by dif- representationsareabletodiscoverhypernymyre- ferent methods in Table 5. They demonstrate the lationswhichcouldnotbeidentiﬁedbywordem- mainfeaturesofthetestedapproaches. Therefore, it the baseline scoring function with Poincare´ and isreasonablethatthedifferenceinscoresisminor. node2vecsimilaritiesresultsinmarginalimprove- However,forsomecases(like“emperor.n.01”and mentsforsomedatasets,butthisdoesnotholdfor “react.v.01” in Table 5) graph vector representa- allofthem. tionsslightlyimprovetheranking. </Extractive Summary>  </Table ID = 5>  </Paper ID = 536> 

<Paper ID = 537>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Basic statistics for plWordNet 2.0 and Princeton WordNet 3.1 Elements plWordNet 2.0 Princeton WordNet 3.1 Synsets 80 037 82 115 Lexical units 109 967 146 347 Lemmas 77 662 117 798 Table 2: Basic statistics for nouns in plWordNet 2.0 and Princeton WordNet 3.1 I-inter-register synonymy, I-hypo/hypernymy On the contrary, adjectives in PWN are or- and I-mero/holonymy (Rudnicka et al., 2012). </Abstractive Summary>  <Extractive Summary> =  state of development of the former. Thus, we Of the four parts of speech described by started with plWN 2.0 (Maziarz et al., 2013a) plWN and PWN, nouns share the most and PWN 3.1, see Table 1. Despite the bigger in terms of the fundamentals of their in- number of lemmas and lexical units in PWN, ternal synset relation structure (Maziarz the number of synsets was comparable. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Basic statistics for nouns in plWordNet 2.0 and Princeton WordNet 3.1 I-inter-register synonymy, I-hypo/hypernymy On the contrary, adjectives in PWN are or- and I-mero/holonymy (Rudnicka et al., 2012). </Abstractive Summary>  <Extractive Summary> =  It was et al., 2013a). The basic relation is especially visible for nouns, see Table 2. hypo/hypernymy, followed by mero/holonymy The ﬁrst challenge of mapping were partly and near-synonymy. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Counts for adjective synset relations in plWordNet 2.2 and Princeton WordNet 3.1 in Table 5. </Abstractive Summary>  <Extractive Summary> =  The biggest challenge was always used together with I-hyponymy to of adjective mapping were very diﬀerent mod- keep the POS information, and, in the case of els of their internal synset relation structure very general I-hyponyms, to give more speciﬁ- in plWordNet and Princeton WordNet, shown cation to the meaning of a mapped adjective. in Table 4. As for numbers, plWN had ap- The use of such a pair of relations also allowed proximately twice as much adjective synsets ustomakeupforthediﬀerenceinsizebetween as PWN at the start of mapping, see Table 3. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Basic statistics for adjectives in plWordNet 2.2 and Princeton WordNet 3.1 Relation plWordNet 2.2 Princeton WordNet 3.1 Value (of the attribute) 9658 639 Modiﬁer 2 108 — Hyponymy 18 225 — Gradability 991 — Near-synonymy 1 308 — Similar to — 21 434 Member of this domain — 1 418 Table 4: Counts for adjective synset relations in plWordNet 2.2 and Princeton WordNet 3.1 in Table 5. </Abstractive Summary>  <Extractive Summary> =  in Table 4. As for numbers, plWN had ap- The use of such a pair of relations also allowed proximately twice as much adjective synsets ustomakeupforthediﬀerenceinsizebetween as PWN at the start of mapping, see Table 3. plWN and PWN. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Basic statistics for adverbs in plWordNet 3.1 and Princeton WordNet 3.1 Relation plWordNet 3.1 Princeton WordNet 3.1 Value (of the attribute) 4302 — Fuzzynymy 9280 — Hyponymy 10 082 — Gradability 690 — Near-synonymy 647 — Table 6: Counts for adverb synset relations in plWordNet 3.1 and Princeton WordNet 3.1. </Abstractive Summary>  <Extractive Summary> =  verbsaremorenumerousinplWordNetthanin Princeton WordNet, with twice as much lem- 3The placement of adjectives within speciﬁc hy- mas, almost three times more lexical units and ponymy trees is conditioned on substitution tests and veriﬁed in corpora. almost four times more synsets, as illustratedElements plWordNet 2.2 Princeton WordNet 3.1 Synsets 38 868 18 185 Lexical units 45 514 30 072 Lemmas 26 961 21 808 Table 3: Basic statistics for adjectives in plWordNet 2.2 and Princeton WordNet 3.1 Relation plWordNet 2.2 Princeton WordNet 3.1 Value (of the attribute) 9658 639 Modiﬁer 2 108 — Hyponymy 18 225 — Gradability 991 — Near-synonymy 1 308 — Similar to — 21 434 Member of this domain — 1 418 Table 4: Counts for adjective synset relations in plWordNet 2.2 and Princeton WordNet 3.1 in Table 5. Adverbs are unique in Princeton (mainly Topic), and, relatively sparse, Entail- WordNetinthattheyhavenosynsetrelations. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Counts for adverb synset relations in plWordNet 3.1 and Princeton WordNet 3.1. </Abstractive Summary>  <Extractive Summary> =  plWN verbs are also a similar set of synset relations as adjectives grouped into verb classes. These are drawn (apart from Modiﬁer), see Table 6. There- from situation types (Aktionsart (Vendler, fore, we decided to take advantage of a pre- 1967)) the verbs denote and their grammati- viously established network of I-relations be- calaspect. </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Basic statistics for verbs in plWordNet 3.1 and Princeton WordNet 3.1 Relation plWordNet 3.1 Princeton WordNet 3.1 Hyponymy 31 784 13 251 Hypernymy 31 784 13 251 Causation 3 427 — Processuality 1 204 — Distributivity 676 — Inchoativity 519 — Iterativity 148 — Entailment — 406 Cause — 214 Table 8: Counts for verb synset relations in plWordNet 3.1 and Princeton WordNet 3.1 get synset are analysed in a similar fashion as in plWN and PWN, we searched for common it is done for a source synset. </Abstractive Summary>  <Extractive Summary> =  Thispartlyaccountsforbiggernumber networks and speciﬁc linguistic diﬀer- ofverbalelementsinplWordNet3.1incompar- ences. Therefore, we decided to use I- ison to Princeton WordNet 3.1 (see Table 7). synonymy, I-inter register synonymy, and Similarly to nouns, verb synsets are organ- I-hypo/hypernymy and introduce new inter- ised around hypo/hypernymy relation both in lingual relations speciﬁc to verb mapping. </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Counts for verb synset relations in plWordNet 3.1 and Princeton WordNet 3.1 get synset are analysed in a similar fashion as in plWN and PWN, we searched for common it is done for a source synset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  Table 9: Interlingual relation counts 8 Conclusion and Further works equivalence links were manually described for ≈10000bilingualpairsofsenses(lexicalunits) The created resource is unique not only be- coming mostly from noun synsets linked by I- cause of its scale and method of construc- synonymy. </Abstractive Summary>  <Extractive Summary> =  one synset serv- English wordnet with almost 300k of unique ing as a hypernym for several other wordnet’s interlingual relations. The counts of all types synsets), while the most diﬃcult cases con- of I-relations are shown in Table 9. We can stituted adverbs that were not derived from see that despite the priority of I-synonymy in plWN adjectives holding I-relations to PWN the mapping procedure it is strongly overruled adjectives. </Extractive Summary>  </Table ID = 9>  </Paper ID = 537> 

<Paper ID = 538>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Coverage of PerSemCor on SBU-WSD- make use of knowledge-based benchmarks pre- Corpus sented by Rouhizadeh et al. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Comparison between performance of the supervised WSD systems when the MFS back-off strategy is disabled(thenumbertotheleftofeachcell)orenabled(thenumbertotherightofeachcell). </Abstractive Summary>  <Extractive Summary> =  Ascan notateddatafortrainingamachinelearningmodel. Overrecentdecades,avarietyofapproacheshave be seen in Table 4, all the WSD models achieve beenproposedtomitigatethisissue. Theycanbe higher performance when MFS back-off is used. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In Comparingdifferentapproaches,theresultsshow addition,wecomparetheresultswithFarsNetﬁrst that all machine learning algorithms achieve senseapproach8 astheformerbaselineofPersian the highest performance when they use word WSD(Rouhizadehetal.,2020). embedding approaches as feature vectors for Results and analysis: In Table 3, we compare training. It clearly shows the great impact of the performance of different machine learning using embedding vectors in a WSD pipeline. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Comparison with knowledge-based sys- for English, developed by the WordNet Project tems research team at Princeton University. It was ini- In Table 5, we compared the F1 performance tiallytaggedwithsensesforWordNet2.1andcon- of supervised models against knowledge-based tains more than 200k sense annotated instances. benchmarks (Rouhizadeh et al., 2020), includ- AlthoughSemCorhasleadthesupervisedsystems ing Basile14 (Basile et al., 2014), UKB (Agirre toachievestate-of-the-artperformanceinEnglish et al., 2018) and FarsNet 1st sense (baseline of WSD, obtaining such corpora is hard and time- knowledge-basedmodels). </Extractive Summary>  </Table ID = 5>  </Paper ID = 538> 

<Paper ID = 539>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  copies of some synsets, only with different IDs. 3 ComparisonofTR-wordnetandKeNet For example, Case 2 in Table 2 shows two sepa- rate synsets for ”˙Izlanda” in KeNet, one of which 3.1 ExtractingMatchings isredundant. Withthiscomparison,wehavebeen In order to compare KeNet and TR-wordnet, we able to detect these repetitive synsets that need to haveextractedthematchingsbetweenthetwo. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In this example, the compar- extractedlisthasbeen, then, displayedonGoogle ison of the merged synset of ”idaresiz gevs¸ek” in sheetsandthecomparisonshavebeenanalyzedby KeNet with ”gevs¸ek” in TR-wordnet shows that two trained annotators. Table 1 shows ﬁve exam- the synset in KeNet is to be split up as it cov- ple cases taken from the extracted list. In this ta- ers two different senses. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Another signiﬁcant difference between KeNet SimilartothecaseinKeNetasexplainedinthe and TR-wordnet is the addition of new lemmas previous section, there are redundant synsets in in KeNet synsets. Case 4 in Table 3 exempliﬁes TR-wordnet, as well. This one-to-one compari- the inclusion of the lemmas of ”kokus¸mak” and son between TR-wordnet and KeNet has showed ”taaffu¨n etmek” in addtion to the existing lemma us the cases where one single synset in KeNet is of ”kokmak” in TR-wordnet. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  required distinction is given with either two or ComparedtoTurkishWordNet,KeNethasalarger three separate senses in KeNet. Therefore, as it synset rate, which is the reason why we opted for can be seen in Cases 5 and 6 in Table 4, although KeNet over Turkish WordNet for the purposes of theyaremergedinasinglesynsetinTR-wordnet, thisstudy. KeNetcapturesthenecessarydistinctionsbetween As the ﬁrst step of our project, we have iden- the senses by having two separate synsets to cor- tiﬁed approximately 76,825 synsets from Kenet. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thesameisalsotrueforthewords The consistency between annotators is very im- labelled as negative. Table 5 shows the number portant for creation of a reliable polarity lexicon.Table6: Fleiss’sKappavaluesforpolaritysynsets. Table 9: Mapping of HisNet synset polarities to Polarity Kappa Strength SentiTurkNetsynsetpolarities Positive 0.618 Good SentiTurkNet Negative 0.652 Good t Polarity Positive Negative Neutral e N Positive 120 12 136 s i Table7: Fleiss’sKappavaluesforpolaritysynsets. </Extractive Summary>  </Table ID = 5>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 9>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  To this end, the most prominent con- attributed to chance is expressed as a number be- tribution of this study is to present HisNet, a new tween 0 and 1. As shown in Table 6 and Table 7, polarity lexicon for Turkish by extending the vol- the results have demonstrated that the agreement ume of SentiTurkNet, the existing ﬁrst and only betweentheannotatorsissigniﬁcant. polarity dictionary available in Turkish. </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  To this end, the most prominent con- attributed to chance is expressed as a number be- tribution of this study is to present HisNet, a new tween 0 and 1. As shown in Table 6 and Table 7, polarity lexicon for Turkish by extending the vol- the results have demonstrated that the agreement ume of SentiTurkNet, the existing ﬁrst and only betweentheannotatorsissigniﬁcant. polarity dictionary available in Turkish. </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Therefore, we used a subset picture of what steps need to be taken to improve of HisNet’s in comparisons with other sentiment the available WordNets as they provide the avail- lexicons. Table 8 shows the number of polarity ablesourcesforalanguageinacomparativeway.References J.L.Fleiss. 1971. </Extractive Summary>  </Table ID = 8>  </Paper ID = 539> 

<Paper ID = 540>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  BalkaNet, similar to EWN, is a multilin- <synset member>anne<SENSE>2</SENSE> gualwordnetprojectconsistingofsixBalkanlan- </synset member> guages(Bulgarian,Czech,Greek,Romanian,Ser- <POS>n</POS> bian,andTurkish)(Tuﬁsetal.,2004). Thisproject <DEF>...</DEF> was done to produce a multilingual semantic net- <EXAMPLE>...</EXAMPLE> work, fully compatible with EWN and its exten- </SYNSET> sions.3 Different approaches were adopted in creating An exemplary set of synsets from KeNet is wordnets and mapping them with those for other given in Table 1. In this table, examples of the languages. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Includingan Nouns 44,074 exemplary sentence for each synset was noted as Verbs 17,791 futurework. Adjectives 12,416 In KeNet, eight intralingual semantic relations Adverbs 2,550 were included: hypernymy, derivational related- Interjections 3342 ness, domain topic, part holonymy, antonymy, in- Pronouns 68 stance hypernymy, member holonymy, substance Conjunctions 60 holonymy and attribute (see Table 3 for exam- Postpositions 29 ples and the current number of matchings for Total 77,330 these relations). For all these relations, the main problem, we created a pool where we collected wordclassthatwasannotatedwasnounswhereas all the synsets that had unrelated synset mem- antonymyandattributeweremainlyannotatedfor bers. </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The POS categories that are in- haﬁf andtheexistingantonymy relationsbetween cludedarenouns,adverbs,adjectives,adverbs,in- theirEnglishequivalents”heavy”and”light”. For terjections, pronouns, postpositions and conjunc- this example, the antonymy relation between ag˘ır tions (see Table 2 for numbers). Regarding the and yeg˘ni haﬁf were correct and it was added to numberofwordsinsynsetmembers,althoughthe KeNet, but the antonymy relation between ag˘ır majority of the synset members are one- (72,436 andhaﬁfwerenotcorrectanditwasnotkept. </Extractive Summary>  </Table ID = 2>  </Paper ID = 540> 

<Paper ID = 541>  </Paper ID = 541> 

<Paper ID = 542>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Manual validation of mapping WordNet sen- to the implementation of methods for syntactic tence frames with PDEV patterns parsing and semantic role labelling, important NLP tasks with applications in semantic analysis, The manually validated PDEV patterns were word sense disambiguation, language under- added to the XML version of the Princeton standing and generation and machine translation. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 542> 

<Paper ID = 543>  </Paper ID = 543> 

<Paper ID = 544>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  reactions according to the frequency of their Words soobrazheniye (consideration) and du- mention. ma (thought) are synonymous with mysl’ Table 1 summarizes the data, sorted by the (thought). Ideya (idea) is a hyponym for mysl’ frequency of the words in the respondents' an- (thought), suzhdeniye (judgment) is a hypernym swers. 3 3* 4 2 2 1 3 2.6 Words in the 6th places of the respondents’associations Frequency (%) 13 7 11.5 9.5 14.5 9 8.5 10.4 Relation dist. 2 1 1 4 2* 4 4 2.6 Table 1. Positions, frequencies (percentage of answers) and RuWordNet distances of word associa- tions. </Extractive Summary>  </Table ID = 1>  </Paper ID = 544> 

<Paper ID = 545>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Summary of AMI scores for all dataset and method variants. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Processing times (in minutes) for differ- Highereducationdataset,theadvantageoverWu- entmethodsfortheALLdataset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 545> 

<Paper ID = 546>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Results on the AWN for Arabic synsets extractions to take into account such morphological features, using the proposed algorithm. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 546> 

<Paper ID = 547>  </Paper ID = 547> 

<Paper ID = 548>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Count of unique lemmas not found in WordNet that exist in only one of the two corpora com- paredorthatexistinboth • Joint words: bestfriend, forreal, goodnight, • Lemmatization issues with other languages. </Abstractive Summary>  <Extractive Summary> =  corpusmaybethemostdifferentfromthevariant However, there are also many non-standard En- ofthepeopletaggedasWhite. glish (and Spanish and Portuguese) words in the In Table 3, we present the intersection between list, and those are the kinds of words that seem to thesubcorporaandsomeillustrativeexamples. As becharacteristicofonlinewriting: we are comparing corpora of very different sizes, thoughweprovidesomequantitativedata,wewill • Onomatopoeiaandformsoflaughter: awww, focusonthequalitativeanalysis,whichwebelieve hahahaha, lmao, kkkk (in Portuguese), jajaja willbeofmorevalueandwhichcanbefoundbe- (inSpanish)... </Extractive Summary>  </Table ID = 3>  </Paper ID = 548> 

<Paper ID = 549>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Example words from the Tourism Word- work, and provide inspiring ideas for future stud- Net ies (Sagri et al., 2004; Bentivogli et al., 2003; Word InstanceHypernym Chenetal.,2019). Table 12: Morphological analyses of size 1 using of common words in Tourism-Turkish dictionar- differentdictionaries ies. Table 13: The 20 topmost annotated synsets and VERB 9.19 theircounts ADJECTIVE 12.64 Id SynSet Count Table 10: The percentages of the top 5 hypernym TUR10-1081860 . Table 10: The percentages of the top 5 hypernym TUR10-1081860 . Table 11: Percentages of analyzed sentences and TUR10-0513570 yemek 1,330 words with different sizes of tourism dictionaries TUR10-0495010 tesis 1,247 andaTurkishDictionary. Table 14: The 20 topmost annotated multi-word only111ofthesewerefromtheTourismWordNet, synsetsandtheircounts theremaining1,737werefromtheTurkishWord- Id SynSet Count Net. </Abstractive Summary>  <Extractive Summary> =  In our Tourism acters as ’Turkce’, which has no meaning in the Dictionary, there are three major part of speech lexicon. Moreover, if a word cannot be morpho- categories (See Table 1, which are proper noun, logically analyzed, after all, we interchange each noun,andadjective. letter with its closest neighbor. TUR10-0816400 ve 1,221 Dictionary Size Sentence Word TUR10-0346660 hizmet 1,042 Tourism 5,000 98.52 99.66 TUR10-0593590 otel 1,014 Tourism 10,000 98.93 99.75 TUR10-1121820 puanvermek 1,010 Tourism 14,000 98.92 99.75 TUR10-0318100 gu¨zel 957 Turkish 51,552 95.97 99.07 TUR10-0097260 bey 924 Furthermore, we have extracted the hypernym TUR10-0582130 oda 915 relation, i.e., the hierarchy of word-senses from TUR10-0187890 deg˘il 769 WordNet to obtain a more precise picture of the TUR10-0473520 konum 740 data. Table 10 shows the top 5 hypernyms in the TUR10-0565860 ilgili 708 tourism domain. As expected, the tourism dictio- separately. possible morphological analysis. This leads to a Table 11 shows the results of two analyses, 7%improvementinthetourismdomainasshown a sentence-based and a word-based analysis, for in Table 12. Thus, it is plausible to assert that re- three different sizes of tourism dictionaries and a ducing the dictionary size is an effective method Turkish dictionary. possible morphological analysis. This leads to a Table 11 shows the results of two analyses, 7%improvementinthetourismdomainasshown a sentence-based and a word-based analysis, for in Table 12. Thus, it is plausible to assert that re- three different sizes of tourism dictionaries and a ducing the dictionary size is an effective method Turkish dictionary. TUR10-0004240 ac¸ıkbu¨fe 20 Furthermore, due to the inclusion of punctuation, TUR10-0019600 do¨rtdo¨rtlu¨k 19 thefullstopattheendofeachsentenceappearsas TUR10-0565860 ilgialaka 17 the most frequent ”word”. Other frequent words TUR10-0084000 herzaman 16 that are not listed in Table 13 include evaluative adjectivessuchas”yeterli(sufﬁcient),ko¨tu¨ (bad)” Table15: Numberofwordsinasentenceandtheir and of course the comma. Finally, another antic- occurrences ipated result is the frequent occurrence of proper #ofWords #ofOccurences namessuchasthenamesofhotelsandhotelstaff. Theinclusionofmulti- 9 157 wordexpressionswerenotlimitedtotwo-wordex- 10 134 pressions; thus, the occurrence of three and even four-wordexpressionswasalsofrequent. 7 Conclusion As shown in Table 15, the majority of the sen- tences in the corpus have a length of three to six words, while there are also sentences longer than Overall, we have created a domain-speciﬁc lexi- 10 words, which make up a minority. At the end con with user reviews and preferences from the of the two-step process, approximately 100.000 tourismdomain. </Extractive Summary>  </Table ID = 1>  <Table ID = 7>  <Abstractive Summary> =  Table 7: A review sample from the Tourism Do- Table 8: Percentage of frequently used POS tags main of2dictionaries. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Percentage of frequently used POS tags main of2dictionaries. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  nary, which accounts for the result that 70.5% of Following the pre-processing of the data, we the Tourism Dictionary is identical to the Turkish manually assign POS tags to each word in order Dictionary. Table 9 shows the percentage of the to perform morphological analysis. For instance, POStagsoftheintersectingwordsintheTourism the word ”Samsun”, which is a city in North- andTurkishdictionaries.Table 9: Percentage of frequently used POS tags Table 12: Morphological analyses of size 1 using of common words in Tourism-Turkish dictionar- differentdictionaries ies. </Extractive Summary>  </Table ID = 9>  </Paper ID = 549> 

<Paper ID = 550>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  thelemmaandWikidataidentiﬁer,thedeﬁnitions oftheconceptgiveninbothresourcesandthe(in- 3 HapaxLinking stance)hypernymsoftheconceptsineachresource. The results of this can be seen in Table 1, where 3.1 Methodology we give four examples of the linkings extracted, Oneofthemostobviouswaystogetagoodlinking wheretheﬁrstthreeweretheﬁrstthreerowsran- istofocusontheelementsinthetworesourcesthat domlypresentedtoourevaluators. Thefourthrow, are hapax legomenon in the resource, that is that ‘Occam’ gives an interesting example of a spuri- theyonlyoccur asingletimeintheresource. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: List of classes in Wikidata that do not fre- wasmappedto0.1. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 550> 

<Paper ID = 551>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Diachronic and stylistic metadata ten-place character string, where each place associated with the sense of háls in (3)a-c corresponds to a grammatical category (e.g. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Lemma annotation for AG háls. </Abstractive Summary>  <Extractive Summary> =  PIE *pleu- ‘float’ for AG pléō ‘sail’ and Skt. plu- ‘float, swim’; Table 2 displays the annotation associated to the b. ETYMON: a discrete form in the history of a AG lemma háls ‘salt’: word’s etymological development (e.g. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Family-specific Lexical Relations. </Abstractive Summary>  <Extractive Summary> =  GROUP {v#00401762 “possess knowledge or [sát- ‘true’] IS PARTICIPLE OF [as- ‘be’]. information about”} for AG gignṓskō (PRS) ‘perceive, know’ and oîda (PF) ‘know’.3 Table 3 summarizes newly added lexical d. Qualifies event as: asymmetric relation relations: holding between an adverb and an adjective: {r#00162139 “for an extended time or at a Rel. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Family-specific Semantic Relations. </Abstractive Summary>  <Extractive Summary> =  IS COMPOSED OF COMPOSES Incl. INCLUDES IS INCLUDED IN Table 4 summarizes family-specific semantic Part. IS PARTICIPLE OF HAS PARTICIPLE relations: Table 3: Family-specific Lexical Relations. </Extractive Summary>  </Table ID = 4>  </Paper ID = 551> 

<Paper ID = 552>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Ontological types assigned to adjectives in a set of adjective relations in the Polish WordNet DanNet 2.0 based on the principles of especially PWN and EuroWordNet combined with specific In fact, these meaning components can also be lexico-semantic features of the Polish language. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In the case where the adjective 17.Sport and leisure 1 ‘well-trained’, ‘football- sense is never a headword but represented more wise’ than once, we relate it to the headword having the largest number of words in its scope. Table 2. The 22 chapters and their share of the total According to this rule, cool in figure 2 would be number of adjectives in DDB, ranged from the highest inserted as a near-synonym to dejlig (‘nice’) in share (11%) to the lowest (1%) (average 4.5%). </Extractive Summary>  </Table ID = 2>  </Paper ID = 552> 

<Paper ID = 553>  </Paper ID = 553> 

<Paper ID = 554>  </Paper ID = 554> 

<Paper ID = 555>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Spearman’s rank correlation ρ be- tions. </Abstractive Summary>  <Extractive Summary> =  For 57 nominal WordNet. Havingattachedtheset,weobtainedthe lemmas we obtained, through combinatorics, 889 correlation of ρ = .71 and r = .80, see Table 2. sensepairsandcorresponding889distancevalues The calculations show that our dictionaries give a between the meanings. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  This proves that par- like homonymous ones. Table 1 jointly presents ticular paths in each dictionary for the very same cardinalities of sets of finite (“<Inf”) and infinite sensepairmustdiffer(aswesawinthecaseofthe paths (“Inf”). As a result, we got 85% identi- PWNnounsink,senses1and2). </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Spearman’s rank correlation ρ between semantic description of homonymy – senses were WordNet (WN) and dictionary graph distances in similarly grouped according to their shared ety- threetestingscenarios. </Abstractive Summary>  <Extractive Summary> =  were treated democratically, receiving weights of 1. Table 3 presents the comparison between Lex-ico, Merriam-Webster and WordNet in terms We found out that traditional English dictionar- of Spearman’s correlation ρ for polysemy and ies showed traces of positive correlation between homonymy cases. In general, WordNet distances Dijkstra’s path lengths on corresponding poly- behaved obviously worse than dictionaries, when semy nets (0.7 for polysemy and homonymy, and homonymy was considered altogether with pol- ρ = 0.4 for sole polysemy). Inboldweindicatedre- Acknowledgments sultsthatfittedcorresponding99%confidencein- This research was financed by the Na- tervalsfortheLEX-MWcomparison. tional Science Centre, Poland, grant number 2018/29/B/HS2/02919, and supported by the When one merges the information from both CLARIN-PL11 research infrastructure, and the dictionaries (see Table 3, minML measure), the NTUDigitalHumanitiesResearchCluster. Spearman’s correlation increases. </Extractive Summary>  </Table ID = 3>  </Paper ID = 555> 

<Paper ID = 556>  </Paper ID = 556> 

<Paper ID = 557>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  To derive genre-invariant features, the are abundant and noisy, as indicated by the high GIcomponentistrainedadversarially. percentage of unrelated and commenting sam- Themodelreceivesaninputsampleas: ples (Figure 1 and Table 1). On the other hand, [CLS] Target [SEP] Text [SEP], STANDER collects considerably fewer samples, where Target is the SD target, expressed as the which are substantially longer and articulated; sentence “A (a) will merge with B (b)” (where moreover, news articles in STANDER have been upper- and lowercase a and b refers resp. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Mergers considered in this work. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Results on the STANDER target operations. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 557> 

<Paper ID = 558>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Precision, Recall and F-Measure calculated Table 1: Combination of hyper-parameters that pre- foreachclassandAccuracyandgeneralF-Measureof sentedbetterresults. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Combination of hyper-parameters that pre- foreachclassandAccuracyandgeneralF-Measureof sentedbetterresults. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 558> 

<Paper ID = 559>  </Paper ID = 559> 

<Paper ID = 560>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Distribution of tags provided per language. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thisisduetothefactthatthemethods posedapproacheasilytransferabletolow-resource aretrainedonthetrainingdatawithalownumber languages. Italsoproposesseveralmodiﬁcationsto ofgoldstandardkeywords(asitcanbeseenfrom thetransformerarchitectureinordertoadaptitfora Table 2). To meet the media partners’ needs, we keywordextractiontaskandimproveperformance designedamethodthatcomplementsstate-of-the- ofthemodel. • TNT-KID (Martinc et al., 2020b): For each dataset, we ﬁrst pretrain the model with an For TNT-KID, which is the only model that autoregressivelanguagemodelobjective. Af- requires language model pretraining, language terthat, themodelisﬁne-tunedonthesame models were trained on train sets in Table 2 for trainsetforthekeywordextractiontask. Se- up to ten epochs. </Extractive Summary>  </Table ID = 2>  </Paper ID = 560> 

<Paper ID = 561>  </Paper ID = 561> 

<Paper ID = 562>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  forevaluationbymergingtherespectivekeyword setsforeachlanguages. Thestatisticsoftheresult- 4 Experiments ing ground truth data are summarized in Table 2. We can observe that the average number of key- 4.1 Dataset words per article for Italian and French is signif- FortheevaluationoftheKEalgorithmswecreated icantly lower than for the other languages. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Theﬁnalsetsofdocuments In the partial matching mode, the match of a usedforevaluationforsomeofthelanguagescon- given keyword c vis-a-vis Ground Truth GT = tainedlessthan50newsarticlesduetosomenear {k ,...,k }iscomputedasfollows: 1 n duplicatesencountered,etc. Table 1 shows the differences in terms of key- 2·commonTokens(c,k) wordannotationdistributionacrosslanguages. The match(c) = max averagenumberofkeywordsperarticlevariesfrom k∈GT |c|T +|k|T 8.68 for French to 13.20 for German. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  sultsobtainedineachlanguagewithaselectionof The overall performance of each algorithm av- algorithmsforthefuzzymatching. TheKPMINER eraged across languages, in term of P, R and F algorithmappearstobebestsuitedfortheFrench 1 scores is listed in Table 3, respectively for exact, language, whereas German the group of YAKE partial and fuzzy matching. In general, only the algorithms appears to be a better choice. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Time efﬁciency comparison on a set of circa graph-basedkeywordrankingmethods(e.g. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 562> 

<Paper ID = 563>  </Paper ID = 563> 

<Paper ID = 564>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  TeMoTopic,ontheotherhand, set should make the topic and word distributions aimstoprovidesupportfordetailedviewingofa more intuitive and insightful. Table 1 shows the subsetoftopicsandshortertimeslices,whichare eighttasksidentiﬁedbyGanesanetal.(2015),as not possible in a ﬂow diagram. As such, we en- well as one additional task which we consider to visagethatotherexistingvisualizationtoolswhich beimportantforvisualizingtemporaltopics. </Extractive Summary>  </Table ID = 1>  </Paper ID = 564> 

<Paper ID = 565>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Theﬁnalparticipantself-identiﬁedashav- original sentence and the inserted/replaced word inga‘workingproﬁciency.’ highlighted. The judges were asked to evaluate the follow- 5 Results ing statements on a Likert scale ranging from 1 (‘StronglyDisagree’)to4(‘NeitherAgreenorDis- Table 1 presents our results in applying both the agree’)to7(‘StronglyAgree’): insertionandreplacementmethodstobothahigh- resource language (English) and a low-resource Q1: Sentence 1 is a good quality sentence in the language(Finnish). targetlanguage. </Extractive Summary>  </Table ID = 1>  </Paper ID = 565> 

<Paper ID = 566>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  thetagsusingasimpleregularexpressiontodetect Weadaptedthemachinelearningtermalignment Cyrillic characters. The vast majority of the tags approachdescribedbyReparetal.(2019)toalign are either unigrams or bigrams (see Table 1 for theRussianandEstoniantagsinthedataset. details). </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Results on the Estonian-Russian language pair. </Abstractive Summary>  <Extractive Summary> =  termpairsbasedontermlengthandfeaturevalues (2019)contained7,083English-Slovenetermpairs. (hence the different training set sizes in Table 2) Weﬁnallysettledontheenvironmentalthesaurus anddevelopnewcognate-basedfeatures. Gemet4, which at the time had 3,721 Estonian- The system requires several language-speciﬁc Russian term pairs. </Extractive Summary>  </Table ID = 2>  </Paper ID = 566> 

<Paper ID = 567>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 567> 

<Paper ID = 568>  </Paper ID = 568> 

<Paper ID = 569>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Results expressed as ROUGE scores on the glish,Croatian,andEnglish). </Abstractive Summary>  <Extractive Summary> =  translatedintoEnglish,Croatian,andGerman. The The results in Table 1 show commonly used threelargecoloredclusterscorrespondtothreetop- ROUGEmetric. Thebestperformingexperimental ics, which is an indication that the sentence rep- setupusestheLaBSEsentenceencoder,noscaling, resentation captures different contents well. Itwastrained LaBSE UMAP GaussMix 26.39 12.99 24.28 LaBSE UMAP K-means 27.36 14.45 26.04 onthelargeCNN/DailyMaildatasetandachieved LaBSE UMAP TextRank 24.99 12.50 23.80 44.41ROUGE-1and40.55ROUGE-Lscores. As SBERT None GaussMix 25.34 12.43 23.82 we can observe from Table 1, our best scores for SBERT None K-means 26.13 12.84 24.67 theCroatiannewslagapproximately4.3ROUGE- SBERT None TextRank 25.20 11.71 23.25 1 and 2.5 ROUGE-L points behind these scores SBERT PCA GaussMix 21.78 09.98 20.51 SBERT PCA K-means 23.96 11.46 22.47 whichisarelevantdifferenceinperformance. How- SBERT PCA TextRank 25.44 11.40 23.76 ever,wehavetotakeintoaccountthatweuseleads SBERT UMAP GaussMix 25.29 13.00 24.16 asanapproximationforthesummaries. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: ROUGE-L scores grouped by sentence en- LaBSE None TextRank 34.35 18.50 32.28 coder,scaling,andtypeofsummarizer. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Theresultsofbothdatasetsareverysimilarifwe rankthemodels,withthebestmodelsbeingidenti- cal. TextRankwithCMLMorLaBSEencoderis As an example, Table 4 shows comments be- superiortoclustering. Surprisingly,SBERTshows longingtooneselectedarticle. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 569> 

<Paper ID = 570>  </Paper ID = 570> 

<Paper ID = 571>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Pictogram with ISO 639-1 language codes events on COVID-19 news overall sentiment, summarising language coverage of multilingual tech- and the region and language of publication niques along with our COVID-19 news corpus. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Pictogram with ISO 3166-1 alpha-2 country codes summarising European countries covered (>20 50 langauges is likely to be slower. </Abstractive Summary>  <Extractive Summary> =  to discard longer BPE tokens was made so that Thefullcorpusdoesnotincludenewsfromall common longer words would still be segmented European countries. Table 2 gives a coverage of andtoboundthemaximumnumberofcharacters the countries included in the corpus, while Ta- removedfromtheword,sinceremovingtoomany ble 1 gives the coverage of languages in the cor- is more likely to cause false positives. In cases pus. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thisisdone withneutralasthenextcommon,andpositivethe via encoding the words as word embeddings, do- least common. Table 3 shows the ﬁve countries ingsimplesubtractionoftheseantonympairs,and withthemostnewsarticleheadlinesclassiﬁedas computingthemeanofthesevectorspersentiment. negative. </Extractive Summary>  </Table ID = 3>  </Paper ID = 571> 

<Paper ID = 572>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Evaluation of the top-10 retrieved articles leastsomeofthedocumentsseemtobepositioned by the SNIR ranking for various k interesting Latvian together. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 572> 

<Paper ID = 573>  </Paper ID = 573> 

<Paper ID = 574>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Weﬁndtheoptimalmodelparametersby onsub-categoriesofblockedCroatiancomments performingagridsearchonseparatetrainandtest detailed in (Shekhar et al., 2020). Table 5 con- setscontaining40,000and10,000comments. Two tainsrecallscoresachievedbytheBERT-enmodel optimizationcriteriaareused: F scoreandrecall. </Extractive Summary>  </Table ID = 5>  </Paper ID = 574> 

<Paper ID = 575>  </Paper ID = 575> 

<Paper ID = 576>  </Paper ID = 576> 

<Paper ID = 577>  </Paper ID = 577> 

<Paper ID = 578>  </Paper ID = 578> 

<Paper ID = 579>  </Paper ID = 579> 

<Paper ID = 580>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  maretal.,2016). Table 1 presents some basic statistics about our 2.3 UserInteraction datasetbeforeandafterthissimplepre-processing pipeline. Theinteractionofauserwithoursystemconsists ofthefollowingsteps. </Extractive Summary>  </Table ID = 1>  </Paper ID = 580> 

<Paper ID = 581>  <Table ID = 1>  <Abstractive Summary> =  Table 1: The ﬁve methodological proposals for NLP technology, they rarely make a fundamen- HCI+MLthatwepresentinthispaper. </Abstractive Summary>  <Extractive Summary> =  Inthefollowing,weproposeﬁve anHCI-relatedﬁeld,thiscontributioniscommonly HCI+NLPmethodsthatweconsiderusefulinad- presentedwithoutempiricalevidenceintheformof vancing research in both ﬁelds. Table 1 provides userstudies. Ourmostfundamentalandimportant a short description of each of the ﬁve HCI+NLP contributioninthispositionpaperisacalltorecen- methodsthatthispaperhighlights. </Extractive Summary>  </Table ID = 1>  </Paper ID = 581> 

<Paper ID = 582>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Word Error Rates for different participants (re)produced by the system, and 4) technical as- varywidelybothacrossandwithingroups(lowerisbet- pects of ASR, we need to draw on methodolo- ter). </Abstractive Summary>  <Extractive Summary> =  Inthefollowingsec- tion,wepresentabriefqualitativeerroranalysis. ASR systems are usually evaluated in terms of WERforindividualspeakersvariesdramatically theirWER,foroneormoreunseentestsets(often (see Table 1). Some of these errors appear to be including well-established benchmark sets). </Extractive Summary>  </Table ID = 1>  </Paper ID = 582> 

<Paper ID = 583>  </Paper ID = 583> 

<Paper ID = 584>  </Paper ID = 584> 

<Paper ID = 585>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Here, the“score”correspondsto cantdifferencesbetweenExperiments1and2(p thenumberofcorrectdecisionswithrespecttothe <0.005)andExperiments1and3(p<0.005),and ground-truth. In Table 2, we see that the average nostatisticallysigniﬁcantdifferencebetweenEx- score of the participants are at 0.41. This result periments 2 and 3. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Inﬂuence score for each feature of the gold is unaccounted for and do not provide meaning- indicator. </Abstractive Summary>  <Extractive Summary> =  todisambiguateandexplainthereasoningprocess ofourmodelforahumantounderstand. Wecan also observe in Table 3, where the evidence (i.e., References highlightedwordsbasedontheattentionweights) David Ifeoluwa Adelani, Haotian Mai, Fuming Fang, shows the lowest inﬂuence score on the users of Huy H Nguyen, Junichi Yamagishi, and Isao our tool. This result implies that users found the Echizen. </Extractive Summary>  </Table ID = 3>  </Paper ID = 585> 

<Paper ID = 586>  </Paper ID = 586> 

<Paper ID = 587>  </Paper ID = 587> 

<Paper ID = 588>  </Paper ID = 588> 

<Paper ID = 589>  </Paper ID = 589> 

<Paper ID = 590>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Percent of segments with each label thelowest. </Abstractive Summary>  <Extractive Summary> =  BL-/FL- 81.7 92.1 94.6 89.9 Thismayreﬂecttheinﬂuenceofcontext: adequate segments may ﬁt the context well enough to be Table5: Percentofﬂuent(FL+)anddisﬂuent(FL-) believable even if not ﬂuent. The same trend is segmentsthatarebelievable(BL+)orunbelievable reﬂected in the fourth row of Table 4, BL+/AD-. (BL-) Mostinadequatetranslationsarenotbelievable,but 19-25%arepotentiallymisleading. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We see that the percent positive examples BL+/AD- 25.9 19.4 21.8 22.2 foreachlabelroughlyrelatestothesystemBLEU score,withArabichavingthehighestandKorean Table 4: Percent of segments with each label thelowest. (rows 1-3) and percent believable but inadequate Feature Relationships Table 3 shows the Pear- (BL+/AD-)segments(row4). son correlations between the scores for ﬂuency (FL),believability(BL),andadequacy(AD).The Arabic Farsi Korean All BL-AD relationship is important because inade- BL+/FL+ 92.1 93.8 93.8 93.1 quate believable translations may mislead mono- BL-/FL+ 8.0 6.2 6.3 6.9 lingual users. </Extractive Summary>  </Table ID = 3>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  </Paper ID = 590> 

<Paper ID = 591>  </Paper ID = 591> 

<Paper ID = 592>  </Paper ID = 592> 

<Paper ID = 593>  </Paper ID = 593> 

<Paper ID = 594>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Usefulness Score Comparison Across differ- We have used the GPT-2 (Radford et al., 2019) entmodels pretrained model and ﬁne tuned it on the IMDb movie reviews corpus to create the next phrase suggestionmodel. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Writer rating is the star rating that the writers gave after watching the movie. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 594> 

<Paper ID = 595>  </Paper ID = 595> 

<Paper ID = 596>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Krippendorff‘s alpha using Jaccard distance forclosedclassattributes. Table 10: Types of system tasks for commonsense pa- pers. </Abstractive Summary>  <Extractive Summary> =  Theagreementwascalculated afourthcategory,commonsenseknowledge,with usingKrippendorff’salphawithJaccardasthedis- ﬁve new annotation items which are relevant for tancemeasure(ArtsteinandPoesio,2008). commonsense-enhancedNLG,namely: Results are presented in Table 1. For system • Deﬁnition of commonsense knowledge: free attributes (system input, system output and sys- text ﬁeld. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: The table presents all verbatim criterion 35a.Naturalness(bothformandcontent) 1 namesfoundintheannotatedpapersasmentioned Goodnessofoutputsrelativetosystemuse 1 Multiple(listall) 1 bytheauthors. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Deﬁnitions of Commonsense extracted from a different question cannot be directly compared literature. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Types of system inputs for commonsense pa- graphs. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  </Paper ID = 596> 

<Paper ID = 597>  </Paper ID = 597> 

<Paper ID = 598>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 598> 

<Paper ID = 599>  <Table ID = 5>  <Abstractive Summary> =  Table 5: IAA for adequacy assessments for RSs, SCs, glesentences(RSs),individualsentencesindocument andDsscenarios. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Aggregated IAA scores for ﬂuency assess- Table 4: Aggregated IAA scores for adequacy assess- ments. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Aggregated IAA scores for adequacy assess- ments. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 7>  <Abstractive Summary> =  Table 7: IAA for error mark-up assessments for RSs andSCsscenarios. </Abstractive Summary>  <Extractive Summary> =  errorstheyhavefoundinRSsintoDs. Interestingly, higher α is shown for the Ds sce- Results in Table 7 show that IAA is higher for nario, followed by the Sc scenario. Nonetheless, allassessmentsintheRSsscenario. </Extractive Summary>  </Table ID = 7>  <Table ID = 9>  <Abstractive Summary> =  Table 9: IAA for pair-wise ranking evaluation assess- binary 0.27 0.15 Fleissκ mentsforRSs,SCs,Dsscenarios. </Abstractive Summary>  <Extractive Summary> =  being more conﬁdent with their assessment (9). 4.4 Ranking Overwhelmingly,translatorsthinktheygivemore accurate assessments when having access to full Results in Table 9 show that the RS scenario texts(10). presentshigherIAAcomparedtobothdocument- level scenarios, while in test Set 2, it is the SC Wealsoaskedtranslatorsabouttheeffortofgiv- scenario which shows higher IAA. </Extractive Summary>  </Table ID = 9>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Aggregated IAA scores for error mark-up as- tion(6)whenhavingaccesstofulltexts. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  <Table ID = 1>  <Abstractive Summary> =  Table 13: Responses for the statement: In order to 8-Tiredness 3.75 4.62 giveageneralscoreforeachtext,Ihadtore-readthe 9-Conﬁdence 4.12 4.62 full text displayed in the Ds scenario. </Abstractive Summary>  <Extractive Summary> =  Theag- scoreforeachtext,Ihadtore-readthefulltext”. gregated scores in Table 10 conﬁrm the low IAA We note that adequacy was the hardest assess- when the Ds scenario is used, and close IAA for menttobeperformedwhentranslatorsareaskedto RSsandSCs. giveonescoreperdocument. Translatorsansweredthe mentioned that “Occasionally, a text would have questions (see full statements in Section 3) after some great individual sentences translation, but they ﬁnished all tasks in all scenarios. Table 11 thenwouldhavemissedsomekeywordswithmis- showstheaverageresultsforeachstatementforRS translations. Soitwashardtothinkwhichfactor andSCscenarios. </Extractive Summary>  </Table ID = 1>  </Paper ID = 599> 

<Paper ID = 600>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Evaluation criteria in Adequacy. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 1>  <Abstractive Summary> =  Table 14: Precision, recall, and F1-score in adequacy 2010. </Abstractive Summary>  <Extractive Summary> =  ThecategoryContradic- notation distributions for the three annotators on tionindicatesthecontradictionwiththereference, ﬂuency and adequacy, respectively. We can see suchasanegationﬂipathyp4andanumbererror somedifferencesamongtheannotators;forexam- at hyp5 in Table 1. This label was motivated by ple,annotatorBwasverystrictforusingthebest thetaskofnaturallanguageinference(NLI),which categoryExcellent inbothdimensions,andanno- hasalsobeenusedforthepre-trainingofMTeval- tator C gave more bad labels (Contradiction and uation(Sellametal.,2020). ThecategorySerious Serious)thantheothers. coverstheotherkindofseriouscontenterrorssuch On average, the translation hypotheses in the as hyp6, and hyp7 in Table 1. These hypotheses WMTMetricsdatasetfor2015-2017stillinclude deliversomewhatrelatedbutdifferentinformation manytranslationerrors. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Annotation distributions for the three annota- evaluationguidelines,becausetheannotatorsgave tors(ﬂuency). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Annotation distributions for the three annota- Among the evaluation corpus, we reserved the tors(adequacy). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ThelabelorderwasIncompre- evaluationthantheothers. hensible<Poor<Fair<Good <Excellent One important ﬁnding here is the differences forﬂuencyandContradiction<Serious<In- amongtheadequacycategoriesIncomprehensible, comprehensible<Unrelated <Fair<Good Unrelated, Contradiction and Serious in Table 7. <Excellentforadequacy14. </Extractive Summary>  </Table ID = 7>  </Paper ID = 600> 

<Paper ID = 601>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  clear divide between the low agreement on raw countsandthehighagreementonthederivedmet- 6 ResultsandDiscussion rics. We investigate this by comparing the facts Table 1 shows the results for all derived metrics, selectedbyeachannotatorandnoticeadegreeof calculated on the raw counts from the evaluators. variabilityinthelevelofgranularitytheyemployed. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Example of evaluators disagreement in fact eration,pages131–137. </Abstractive Summary>  <Extractive Summary> =  today stating that she would like dia- betic foot care. Interestingly, all four models score almost- perfect accuracy, meaning they don’t hallucinate Table 3 shows the facts selected by the three medicalfacts. ThisisnotasurpriseforLead-3and evaluators. </Extractive Summary>  </Table ID = 3>  </Paper ID = 601> 

<Paper ID = 602>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Aggregated scores for each evaluator, each criterion, and each note. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 602> 

<Paper ID = 603>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Number of samples produced by the method thingelsethanwhatwasmodeledintheproposed thatwereevaluated. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 603> 

<Paper ID = 604>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Spearman’s Rank Correlation Coefﬁcient (ρ) Rank 2 4 1 3 betweeneachretrievalmodelandthehumanratingsfor test thedevsetandthetestset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Average ratings assigned to the gold and top and1.20to1.38intest. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Forthisreasonwedecidenottoresolve and Rater 2. Table 1 shows the results of each theannotator’sdisagreements,andintheanalysis combinationfortheMDC’sdialoguedevsetand thatfollowsweusetheaverageratingbetweenthe testset. Theinter-annotatoragreementseemscon- three(orlessbecauseofblacklisting)scoresgiven sistent between the test and dev set apart for the bythecrowdforeachdialoguecontext-predicted highesttworatingsandtherandomtworatingssce- replypair. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  tisticallysigniﬁcantasallthep-valuesapproached 6.2 CrowdRatingsofRetrievalTopChoices 0), we can notice a mixed behavior. The models performing better (See Table 4) do not necessar- Nextweconsidertheaverageratinggivenbythe ily correlate more with human ratings. This is a AMT workers to the gold answer, and to the top rankingcorrelation. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  by retrieving the top 10 answers Ourfutureworkincludesrecognizingversatile usingBERTq-A1:100andﬁne-tuningare-ranking questionsandanswers,designingmethodstoelicit methodologythatpushesontopthebestanswers. more precise answer recordings at the avatar cre- Table 5 shows the SR@k metrics for the test set, ation stage, and forcing yes/no answers with ac- andit’sinterestingtonoticethatBERTq-Qyields ceptable degrees of conﬁdence. We plan to use a better SR@10 on the crowd’s annotations than transfer-learningandone-shotlearningforleverag- theBERTq-A1:100model. </Extractive Summary>  </Table ID = 5>  </Paper ID = 604> 

<Paper ID = 605>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  showingareferencesummary(JonesandGalliers, Weconductedseriesofhumanexperimentson 1996). However, the extrinsic evaluation, called this data set shown in Table 1 in chronological also task-based evaluation, aims to measure the order. In experiment 1, crowd workers created summary’simpactonthecompletionofsometask extractive summaries for 67 post-query pairs. vanderLeeetal.(2021)showedonly thatthispapermakesasigniﬁcantcontributionto 57%ofpapersspeciﬁednumberofevaluatorsand humanevaluationresearchoftextsummarization. the median was 3 among the papers which have As Table 1 demonstrates, the time and organiza- reported the evaluator number. But our analysis tional efforts and the cost of human experiments in Section 4.1.2 showed that when using crowd- can be enormous. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  To do so, we calcu- scores. latedtheSpearmancorrelationsbetweenthecrowd Looking at Table 2, we observe that the me- evaluations from experiment 2 (3 crowd workers diation meetings increased the agreement scores peritem)andexperiment4(24crowdworkersper enormouslybothfortheevaluationofcrowdand item) for the six intrinsic measures. To have the TextRank summaries. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 605> 

<Paper ID = 606>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Statistics of data from the WMT19 and and 2020, segment and document-level assess- WMT20 campaigns, including languages, the total mentsofdocumenttranslationswerecollected,but numberofannotatorsandcollectedsegment-leveland using different methods and thus user interfaces. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Inter-annotator agreement (Krippendorff’s al- pha) on document-level and averaged segment-level scoresfordifferentmetrics(4commonlanguages). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  </Paper ID = 606> 

<Paper ID = 607>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Example of disagreement and agreement in intrinsic evaluation tasks can be used to agree on thecoherencetask.Bolded/markedtermswerechosen semantic relatedness between word embeddings by participants to be outliers, underlined terms were even when the target language variety is highly modeloutlierswithlowerwordembeddingsimilarity. </Abstractive Summary>  <Extractive Summary> =  However, other studies on mationisameaningfulbigraminQuine’sthought expertexplicitknowledgealsoexecutetaskswith experiment on radical translation. In Table 3 we two(Dynomantetal.,2019)orthree(Padarianand see disagreement on the ambiguity outlier. While Fuentes,2019;Gomesetal.,2021)participants. </Extractive Summary>  </Table ID = 3>  </Paper ID = 607> 

<Paper ID = 608>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Manual and automatic evaluation scores of the systems in our study. </Abstractive Summary>  <Extractive Summary> =  Not only was thesamescreen,allowingthusbetterreliabilityin iteasytorecognize,butwelearnedseveralnovel comparisons;seeSection5fordetails. insightsintomanualevaluationoftranslation, by examiningpost-editedandindependentreference 3 Automaticanalysisofreferences translationsandprovidingasmallcontrastivestyle Table 1 shows the translation quality of the three ofmanualevaluation. references and two selected MT systems accord- 2 Dataset ingtotwomanualevaluations,DA(DirectAssess- ment,Grahametal.,2013)andRankME,andfour WeusedtheEnglish-CzechpartofWMT2020(Bar- types of BLEU scores. </Extractive Summary>  </Table ID = 1>  </Paper ID = 608> 

<Paper ID = 609>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Average (weighted across different capabilities by number of examples in each) failure rates (%) of differentmodelsonindependentlycreatedtemplatesets. </Abstractive Summary>  <Extractive Summary> =  itevenmoredifﬁculttoquantifyeventherelative Themainreasonwhyitisimportanttoestimate qualityoftemplatesetsthatspanmultipletemplates thequalityoftemplatesetsisevidentfromthere- andcapabilities. sults of Table 3. None of the augmented models arebetterforallthetemplatesetsacrosstheboard, Further, template generation by humans is an andperformanceonthesametemplatesetcanvary endlessprocess;onecankeepongeneratingmore signiﬁcantly for different augmented models cre- andmoretemplatesgiventime. </Extractive Summary>  </Table ID = 3>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Interestingly,theper- similarhuman-in-loopfeedbackprocesswithdata formanceonthestaticevaluationtestsetsneither augmentation to create iterations of datasets and improved nor degraded substantially, which can bettermodelsforsentimentanalysis. be seen in Table 1. Here, Aug-1 was the model Our process in spirit is similar, except instead obtainedbyretrainingthebasemodelwiththeorig- of adversarial examples, we focused on speciﬁc inaldataplusdatafromTrSofTS-1. </Extractive Summary>  </Table ID = 1>  </Paper ID = 609> 

<Paper ID = 610>  </Paper ID = 610> 

<Paper ID = 611>  </Paper ID = 611> 

<Paper ID = 612>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Overall results. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Forexample,thequestionWhatactivity additionalknowledgechangesthestructureofthe aretheydoing? islabeledwiththecorrectanswer learned embedding space, we examine nearest video game, and our model’s answer play video neighborsofembeddingsofindividualwords(see gameisconsideredincorrect. Table 3). Our model clearly captures more lexi- Thehardestquestionsforourmodelarethose cal and semantic information. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Comparison with existing methods. </Abstractive Summary>  <Extractive Summary> =  nentretrievesWikipediaarticlesfromwhichitex- tracts an answer for each question. These mod- 4.5 ComparisonwithExistingMethods elsperformmuchworsethantheLXMERTbase- In Table 4 we compare our results with the top line, which is trained on multiple datasets. We entries from the leaderboards of OK-VQA1 and include an LXMERT model provided by its au- NLVR22. </Extractive Summary>  </Table ID = 4>  </Paper ID = 612> 

<Paper ID = 613>  <Table ID = 1>  <Abstractive Summary> =  Table 1: The effect of image representations on cross- Associative(object-based) 1 9.0 35.6 modal JMLM perplexity on the COCO dataset with Associative(object-based) 16 8.6 4.7 K = 1imageperexample;N isthenumberofvisual regionsperimage. </Abstractive Summary>  <Extractive Summary> =  et al., 2018) embeddings6 which are competitive WeevaluatetheimagerepresentationswithJMLM withstate-of-the-artmodelsonsemanticsimilarity pretraining perplexity. Table 1 shows the results tasks(ReimersandGurevych,2019).Queriesand of this experiment. Overall, the presence of im- keysarematchedaccordingtocosinedistancewith age features (N > 0) signiﬁcantly improves the theFaiss(Johnsonetal.,2017)library. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Downstream tasks transfer results. </Abstractive Summary>  <Extractive Summary> =  of a shared task on verb metaphoricity detection Thissuggeststhattheresultingmodelisabletouse (Klebanov et al., 2020) as well as another ver- theobtainedmultimodalfeaturestobetterperform sionwherewekeptonlythenounswhichwecall thedownstreamtasks.Theperformanceoftheasso- VUAN. ciativestrategiesdependsontheimageretrievalsys- Table4inappendixA.2showsthedatasetsizes, tembutthecomparisonsuggeststhatobject-based and Table 3 reports the results of the Wiki-BC andscene-basedretrievalperformwellenoughto trained models on the above tasks. We perform yieldmeaningfulresults. </Extractive Summary>  </Table ID = 3>  </Paper ID = 613> 

<Paper ID = 614>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  considered since they include common conversa- Models +NE +NE- (cid:8)N(cid:8)E (cid:8)N(cid:8)E- tional phrases like “day before yesterday”, “my overlap overlap two cents”, “an English breakfast” etc. Exam- Image-only 0.89 0.70 1.41 1.05 Text-only 1.39 1.46 1.38 1.27 plesofcaptionswithNEs: “Justthe(Earth/LOC) Contextual 1.29 1.28 1.14 1.13 letting off some steam (Iceland/GPE)”, “The (ﬁrst/CARDINAL) Chipotle , opened in (Den- ver/GPE)in(1993/DATE).”Examplesofcaptions A.4 Examples without NEs: “Texture of the paint on a skull I painted.”,“MygirlfriendandIhandlesocialsitua- Table 5 shows a few good and bad examples of tionsdifferently.” contextualcaptions. 36Image Paragraph Shes pretty. </Extractive Summary>  </Table ID = 5>  </Paper ID = 614> 

<Paper ID = 615>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Validation set comparison of different tex- Figure2:Reportedresultsforzero-shotlearningonthe tualinputsandencodingsforzero-shotlearningon ImageNet dataset and the mp500 test split. </Abstractive Summary>  <Extractive Summary> =  mp500, unless stated differently, for a fair com- 42parison,wecomputetheaccuraciesofthemodels Top-1accuracy(%) Classnames using Wikipedia articles assuming 0 accuracy on 30 Wikiarticles theremainingclasses. 25 22.2722.27 5C.o3mpRaersiusoltns of different auxiliary data encod- 112050 9.96 12.33 9.88 10.39 10.4 10.8 11.91 15.83 15.99 16.5816.58 ings First, in Table 3, we compare ways to en- 5 5.1 code the auxiliary data on the val set only. We ZSLMethod 0 obebtsteerrvtehatnhautsuinsginjgustthtehewahboslteraWctisk(iﬁarrstticplaersagwraoprkhss CMT SAECONSE SJEDEVISE ALELATEMESZSLSCYANCDA-SViAmEpleCZASDLA-VAE ofthearticlesbeforeanysectionstarts). </Extractive Summary>  </Table ID = 3>  </Paper ID = 615> 

<Paper ID = 616>  </Paper ID = 616> 

<Paper ID = 617>  </Paper ID = 617> 

<Paper ID = 618>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  786 13.NOT ARB NotArabic,e.g.Persian. 161 Total 8000 Table1: Annotationclassesanddistribution: Importantclasses(top)andLessImportantclasses(bottom) Figure 1: Examples for REP, ACT, INFO, RUMOR, Figure 2: Examples for CURE, VOLUNT, PRSNL, ADVICEandSEEK ACTclasses SUPPORT,PRAYERandUNIMPclasses Table 2 shows country distribution and top ac- countsfortheoriginalauthorsoftweets. Typically,peopleretweettweetsfromministryof agencies and celebrities. </Extractive Summary>  </Table ID = 2>  </Paper ID = 618> 

<Paper ID = 619>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Barnes et al. (2020) devi- In example sentence (1) in Table 1 above, the atefromthetwo-stepsequencelabelingapproach negationcueno(denotedinbold)affectstheunder- andproposeasequencelabelingmodelthatdetects linedpartsofthesentence,whicharereferredtoas cuesandscopeinonestep. Kurtzetal.(2020)show thescopeofthisnegationcue. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Dataset statistics and difference in annotation schemes. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Results are shown F1-scoreontheBIOSCOPEvalidationsplit. in Table 3.17 We report zero-shot performances (ZS) of a single task model trained on a single 6.1 Results dataset (ST) and on a concatenation of datasets Baselines Thein-domainexperimentsshowthat (ST ). Forthemulti-taskmodel, weeithertrain cat NUBES and FRENCH are harder to predict than onseveralnegationscoperesolutiontasks(MTL ), n IULA. </Extractive Summary>  </Table ID = 3>  </Paper ID = 619> 

<Paper ID = 620>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Similar to prior work (Wang et al., 2012), we 3 Dataset measuretheextentofsocialsupportexpressedin Our dataset consists of posts published on a posts. Wehad3medicalstudentsratearandomly Reddit forum, /r/Coronavirus - which is focused selected sample of 1,000 posts from our dataset; on discussions around COVID-19 and is the Table 2 shows a brief description of the sample COVID-19 subreddit with the most number of posts. Table2: Summaryofsampledataset members(i.e. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Number of posts in our dataset published in 7meant“aparticularsocialsupportwasexpressed MarchandApril2020 alot”theannotatorsratedthe1,000sampleposts. Table 12: Results from LIWC categories associated statisticallydifferent;wedothesameforfeatures withpostsfromourdatasetpublishedinMarch representing the informational support given and sought,respectivelyandalsodeterminethatthese LIWCcategory Cohen’sD featuresarestatisticallydifferent. Table 13: Results from LIWC categories associated userssoughtmoreemotionalsupportandgaveless withpostsfromourdatasetpublishedinApril emotionalsupportand(ii)usersgavemoreinforma- tionalsupportcomparedtoinformationalsupport LIWCcategory Cohen’sD sought. </Abstractive Summary>  <Extractive Summary> =  Speciﬁcally, we collected 64,074 posts Attributes Result Avg.numberofsentencesperpost 3.6 published daily in the “Daily Discussion Post” Avg.numberofwordspersentence 52.53 thread in the /r/Coronavirus subreddit between Avg.standarddeviationofnumberofsentences 2.23 March 3 and April 30 2020 (Stokes et al., 2020). Avg.standarddeviationofwordspersentence 39.5 Table 1 shows information about the number of Givenapost,theannotatorsratedthepostson: posts published in March and April, respectively. (i) the extent of emotional support given, (ii) the Figure 1 shows information about the number extent of emotional support sought, (iii) the ex- of posts published weekly in our dataset; it can tent of informational support given, and (iv) the be observed that the number of posts published extent of informational support sought. Wehadregularweekly lishedinthesetimeperiodsmayprovideinsights meetingswiththeannotators(whoareco-authors as to why there was a change in these expressed ofthispaper)toaddressanyconcernstheymight socialsupports. WeobservedthatinMarch, peo- haveandtoensurethatthecontentofthedatathey ple posted about topics related to traveling and werelabelingdidnotnegativelyimpactthemand COVID-19 symptoms as shown in Table 10 and theirwell-being. usedmorewordsassociatedwiththeLIWCcate- goriesonhealthandanxiety(Table12). </Extractive Summary>  </Table ID = 1>  <Table ID = 9>  <Abstractive Summary> =  Table 9: The results of the t-tests. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 9>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Correlation between the emotional support in Figure 2, we observed that emotional support given and the informational support given and sought, soughtincreasedovertimeandtheemotionalsup- respectively. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Correlation between the emotional support whichtheygaveinformationalsupport,asshown soughtandtheinformationalsupportgivenandsought, inFigure3. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  </Paper ID = 620> 

<Paper ID = 621>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Precision@1 and Mean Average Precision (MAP) for the entity disambiguation task on the BC5CDR datasetwhenthegoldmentionspansareknown.’N/A’standsfor’NotApplicable’.’DR’standsfordenseretrieval. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 621> 

<Paper ID = 622>  <Table ID = 2>  <Abstractive Summary> =  Table 2: The set of gazetteer lists used for each task, subsumedunderthelabelGaz Propernamesandnamesoforganizationsareex- tractedusingtheANNIEmodule. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 622> 

<Paper ID = 623>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Descriptive statistics about the number of unobservedhypernymbitewoundoveranotherpre- names per class for the different levels sampled from viouslyunobservedhypernymstingofskinforthe thesubgraphwithparentconceptC1290864(disorder name tick-borne fever. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Toavoidoverﬁttingon matterfordeeprepresentationlearning. thelargestclasses,wealwayssampleonesiamese triplet per name, using random sampling for the Unsupervised hypernym detection Table 5 positivenameanddistance-weightedsamplingfor showsthetestperformanceforunsupervisedhyper- thenegativename. Asstoppingcriterionweusethe nymdetection. </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Test performance of semantic similarity ranking per level, as measured by mAP. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: A comparison between our DAN encoder and the BNE reference model for unsupervised hypernym rankingoftheLevel3testmentionpoisoningcausedbymexicanbeadedlizardbite. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Spearman’s rank correlation coefﬁcient be- fullpotentialinsimpleways,assuggestedbyWi- tween cosine similarity scores of name embeddings andhumanjudgments,reportedonsemanticsimilarity etingandKiela(2019)),itstillraisesthequestion (sim) and relatedness (rel) benchmarks. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  </Paper ID = 623> 

<Paper ID = 624>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In all three cases, we expect the classiﬁcation In all three cases (Table 6), the model perfor- metrics to drop. For the case when 10% of the mance drops by some degree when compared to test case was modiﬁed the drop was much lower Table 4. The ﬁrst two cases showed a somewhat as compared to when 100% of the test case was similar performance drop. </Extractive Summary>  </Table ID = 4>  </Paper ID = 624> 

<Paper ID = 625>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (2010) shows that the hold-out samples. Table 1 is a statistical descrip- auto-evaluationoftopic-semanticcoherenceusing tion of the corpus, and we release this maternal PMI is highly correlated with human evaluation, healthcorpusonthegithub. andNPMIhasbeenwidelyusedasaquantitative measurementoftopicquality(AletrasandSteven- #documents 30647 son,2013;Hoyleetal.,2020). </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Identiﬁed topics counts for 10-topics and 25- ingtopicinferenceonnewdocuments,bothLDA topicmodels(separatedby’/’) andtheknowledge-distilledmodelperformwell. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  74LDA KD ETM LDA KD ETM 10-topic -1.9756 -2.3899 -2.5505 pre-train teachermodel embedding 25-topic -2.2545 -2.5681 -2.6672 6409s 10s 50-topic -2.2841 -2.5807 -2.6649 10-topic 111s 364s 1112s 100-topic -2.3867 -2.5663 -2.7402 25-topic 131s 186s 1121s 200-topic -2.5528 -2.5592 -2.7535 50-topic 148s 236s 1110s 100-topic 167s 340s 1062s Table6: Coherencefortopicsgenerated 200-topic 204s 547s 1086s LDA KD ETM Table9: Trainingtimefordifferentmodels 10-topic 0.1181 0.1469 0.1243 25-topic 0.1401 0.1212 0.1384 (LDA-penilehealth,KD-vaginalhealth) 50-topic 0.1460 0.1107 0.1347 100-topic 0.13190 0.1041 0.1105 Comment2 Sowillyoubebackforafollowup 200-topic 0.0997 0.0862 0.1073 papinafewmonthsorgettingthecolposcopyright away? Alotoftheseclearupontheirown. Trynot Table7: NPMIfortopicsgenerated topanic! (LDA-vaginalhealth,KD-vaginalhealth) A Appendix: ExperimentResults Comment 3 I was on Loestrin24Fe for about a The experiment results are shown in Table 6, Ta- yearbutatmyFebruaryyearlygynoappointment, ble7andTable8 Iaskedmydoctoraboutapilltogivemelesspe- We trained the LDA model with 4 CPU cores riodsbecausemine,likeyours,were*brutal.*He andtrainedtheothertwomodelwithasingleGPU switched me to Lo Loestrin Fe and I haven’t had core. Table 9 shows that LDA implemented by aperiodsince! It’sAWESOME.I’vealsonoticed Mallet is quite faster than the other two models, thatIamnotasemotionalasIwasonL24Fe. </Extractive Summary>  </Table ID = 6>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Trynot Table7: NPMIfortopicsgenerated topanic! (LDA-vaginalhealth,KD-vaginalhealth) A Appendix: ExperimentResults Comment 3 I was on Loestrin24Fe for about a The experiment results are shown in Table 6, Ta- yearbutatmyFebruaryyearlygynoappointment, ble7andTable8 Iaskedmydoctoraboutapilltogivemelesspe- We trained the LDA model with 4 CPU cores riodsbecausemine,likeyours,were*brutal.*He andtrainedtheothertwomodelwithasingleGPU switched me to Lo Loestrin Fe and I haven’t had core. Table 9 shows that LDA implemented by aperiodsince! It’sAWESOME.I’vealsonoticed Mallet is quite faster than the other two models, thatIamnotasemotionalasIwasonL24Fe. Don’t andKDisthemosttime-consumingonebecause getmewrong-Forabouttheﬁrstmonth,Iwassu- ofthepre-trainingofteachermodel. </Extractive Summary>  </Table ID = 9>  </Paper ID = 625> 

<Paper ID = 626>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Werealisethis Continuous 5.360 4.508 16.261 compressestwoseparateentitiesintoone(e.g. the Discontinuous –Disjoint 100 225 909 entities‘paininmyhands’and‘paininmyupper –Composite 828 70 286 arms’ in Table 1). However, this does not pose a problem to normalization, as the state-of-the-art Table2: Sizeofdatasets. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (2015). See Table 2 for more details. Data sets model. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Extrinsic evaluation of ADR extraction. </Abstractive Summary>  <Extractive Summary> =  fortheremainingdataset(PsyTAR)theend-to-end performanceislowerforallentitytypes. 5.2 Extrinsicevaluation WebelievethatthedifferencebetweenPsyTAR As can be seen in Table 4, using the FuzzyBIO andtheotherdatasetsmayberelatedtoeitherthe schemeimprovesend-to-endperformanceforcon- low number of discontinuous entities or the low tinuousandcompositeentitiesintwoofthreedata numberofcompositeentitiesinthePsyTARdata, sets, namely the CADEC (+0.4 and +1.3) and whichmayhavehinderedthetrainingofanNER CLEFdata(+0.4and+5.7). Incontrast,theend-to- modelfortheseentitytypes. </Extractive Summary>  </Table ID = 4>  </Paper ID = 626> 

<Paper ID = 627>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In technical terms, exemplars are the data points Normalizedpointwisemutualinformation(NPMI) that lie at the heart of a cluster, around which (Bouma,2009),ontheotherhand,hasbeenshown the ultimate cluster forms. Table 2 provides the to correlated with human judgement (Lau et al., statisticsoftheclustersandtheirrespectiveexem- 2014). Thus, in this study, we use NPMI as an plarsizes. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  i j=1 j Total7themeswereextractedfromthe9clusters The coherence scores for the topics generated generated. Theyareasfollows: by LDA and the clusters generated by the above- Abusestories: Cluster3revealsself-disclosure mentioned embeddings are reported in Table 3. storiesofabusethatarephysical,emotional,orsex- Thetop15nounswiththehighestc-TF-IDFscores ualinnature. Other variants of autoencoders and objec- thus, making it difﬁcult to draw out interpretable tivelossescouldbeemployedtofacilitatetighter topics of discussion. This is attested by the low integrationoftopicalinformationwiththecontex- NPMI scores in Table 3. Furthermore, the posts’ tualizedembeddings. </Extractive Summary>  </Table ID = 3>  </Paper ID = 627> 

<Paper ID = 628>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  toasAbstract andAbstract , Label-Only Label+Rationale COVID-19 Scientiﬁc (Lee et al., 2020) contains respectively. 142 claims (label distribution in Table 2) gath- eredbycollectingCOVID-relatedscientiﬁctruths • Sentence-level evaluation, where systems are andmythsfromsourcesliketheU.S.Centersfor evaluatedonwhethertheycanidentifysentences DiseaseControlandPrevention(CDC),Medical- sufﬁcient to justify the abstract-level predic- tions. First, sˆ ∈ Sˆ(q,a) is correctly selected NewsToday, and the World Health Organization (WHO). </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Comparison of different sentence selection T5(MSMARCOMED) 85.65 89.00 T5(SCIFACT) 86.60 89.40 methodsonSCIFACT’sdevelopmentset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thiscomesasnosurpriseseeingthatour showssigniﬁcantimprovementsoverthebaseline modelsdisplaynotableimprovementsforeachof RoBERTa-largethatwasﬁne-tunedonFEVERfol- thethreesub-tasks. lowed by ﬁne-tuning on SCIFACT’s label predic- Notice that in Table 6, using T5 (MARCO) tiontask. Webelievesomeofthiscanbecredited bringslargegainsintermsofR@3overtheBM25 toT5’spretrainingonamixtureofmultipletasks. </Extractive Summary>  </Table ID = 6>  <Table ID = 9>  <Abstractive Summary> =  Table 9: Full pipeline abstract-level effectiveness on Table10: Fullpipelinesentence-leveleffectivenesson SCIFACT’sdevelopmentset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 9>  <Table ID = 1>  <Abstractive Summary> =  Table 11: Full pipeline effectiveness of VERT5ERINI onSCIFACT’stestset. </Abstractive Summary>  <Extractive Summary> =  tivelysmallsizeofthedevelopmentset;below,we choosetoprobetheSCIFACThiddentestsetwith 4.5 FullPipeline bothconﬁgurations. In Tables 9 and 10, we report the precision, re- From Table 11, it is clear that in the hidden call, and F scores of abstract-level evaluation test set, both our systems outperform the base- 1 andsentence-levelevaluation,respectively,forfull line VERISCI, with evaluation aspects like Sen- pipelinesystems. tence+Label (rows 10–12) showing relative im- Rows 1, 2, 6, 7 present the scores in the or- provements of around 50%. </Extractive Summary>  </Table ID = 1>  </Paper ID = 628> 

<Paper ID = 629>  </Paper ID = 629> 

<Paper ID = 630>  </Paper ID = 630> 

<Paper ID = 631>  </Paper ID = 631> 

<Paper ID = 632>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Distribution of the ﬁne-grained class labels ‘Hate’examples,yieldinganearly-balanceddistri- of the original German GermEval training and test butionthatresultedinoptimalperformanceforour datasets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  languagewordsarerepresentedinacommonvec- After relabeling was completed, we addressed torspace,enablingneuralmodelsbasedonBWEs the imbalanced class distributions of the English totrainonthesourcelanguageandtestonthetarget and German datasets. Examining Table 2, it is languagewithoutanyintermediatesteps. clearthatthemajorityofsamplesintheStormfront To produce our bilingual English-German em- datasetare‘noHate’. </Extractive Summary>  </Table ID = 2>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Confusion matrix of the ensemble-relabeled and macro-average F1 performance on DE-DEV. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  <Table ID = 9>  <Abstractive Summary> =  Table 9: Model performance on DE-TEST with bootstrapping, i.e., after training on EN-OS and ﬁne-tuning on eitherDE-REL*orDE-NEW.Improvementscomparedtothemodelswithoutﬁne-tuningshowninbold. </Abstractive Summary>  <Extractive Summary> =  afterﬁne-tuning,decreasinginmacro-averageF1 by0.27points. Acloserlookatitsclass-wiseF1 scores for ‘Hate’ and ‘noHate’ in Table 9 shows The Labels of DE-REL* Table 8 shows a se- thatthemodel’sscoreinthelatterclassimproved, lection of correct and incorrect classiﬁcations in whileitsscoreintheformerdecreased. Thispoints DE-REL*, according to the original gold labels to the fact that the model became skewed during of the examples. </Extractive Summary>  </Table ID = 9>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Correct and incorrect ensemble labels for either DE-REL* or DE-NEW. </Abstractive Summary>  <Extractive Summary> =  afterﬁne-tuning,decreasinginmacro-averageF1 by0.27points. Acloserlookatitsclass-wiseF1 scores for ‘Hate’ and ‘noHate’ in Table 9 shows The Labels of DE-REL* Table 8 shows a se- thatthemodel’sscoreinthelatterclassimproved, lection of correct and incorrect classiﬁcations in whileitsscoreintheformerdecreased. Thispoints DE-REL*, according to the original gold labels to the fact that the model became skewed during of the examples. (2018). Table 8 shows four classiﬁcations performed some manual preprocessing to ensure made by the ensemble. Comments 5 and 6 are compatibility with the format of a tweet. </Extractive Summary>  </Table ID = 8>  <Table ID = 1>  <Abstractive Summary> =  Table 10: Optimalhyperparameters forﬁne-tuning on DE-REL*and DE-NEW.The ﬁrsttwo columns represent classweights. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 632> 

<Paper ID = 633>  </Paper ID = 633> 

<Paper ID = 634>  </Paper ID = 634> 

<Paper ID = 635>  <Table ID = 3>  <Abstractive Summary> =  Table 3: P, R and Macro-average F1 score values World 0.93 0.92 0.9 achievedbytheclassiﬁeronthetestsets. </Abstractive Summary>  <Extractive Summary> =  Training Thereare5participantsinthestudy: thetwosocial setswereoversampledtoreachclassbalance. researchers that participated in the annotation of Table 3 presents the performance achieved by thedatasets,twodatascientistsandapolicymaker. theclassiﬁers,foreachsocialchallenge. </Extractive Summary>  </Table ID = 3>  </Paper ID = 635> 

<Paper ID = 637>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  China mon vannallo- Phew! Here comments in Tamil, and 10,705 comments in comestheChineseguyNon-hopespeech Malayalam. Table 1 and Table 2 show the dis- tributionofHopeEDIdatasetbylanguageandan- • paambu kari saappittu namma uyirai notation label respectively. To calculate corpus vaanguranunga- These guys (Chinese) eat statistics nltk tool (Bird et al., 2009) was used to snakemeatandmakeourlivesmiserableNon- tokenise the sentences in the comments. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  China mon vannallo- Phew! Here comments in Tamil, and 10,705 comments in comestheChineseguyNon-hopespeech Malayalam. Table 1 and Table 2 show the dis- tributionofHopeEDIdatasetbylanguageandan- • paambu kari saappittu namma uyirai notation label respectively. To calculate corpus vaanguranunga- These guys (Chinese) eat statistics nltk tool (Bird et al., 2009) was used to snakemeatandmakeourlivesmiserableNon- tokenise the sentences in the comments. </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Rank list based on F1-score along with other evaluation metrics (Precision and Recall) for Malayalam language 66Team-Name Precision Recall F1Score Rank Zeus(Zhou,2021) 0.93 0.94 0.93 1 TeamUNCC(Mahajanetal.,2021) 0.93 0.94 0.93 1 team-hub(HuangandBai,2021) 0.93 0.93 0.93 1 res-sisun 0.93 0.93 0.93 1 NLP@CUET(Hossainetal.,2021) 0.93 0.93 0.93 1 KU-NLP(MKandAP,2021) 0.92 0.93 0.93 1 Hopeful-men(Upadhyayetal.,2021) 0.93 0.93 0.93 1 GCDH 0.93 0.93 0.93 1 EDIOne-sumant(DowlagarandMamidi,2021) 0.93 0.94 0.93 1 cs-english(ChenandKong,2021) 0.93 0.94 0.93 1 Autobots(GundapuandMamidi,2021) 0.93 0.93 0.93 1 Hopeful-nlp(Awatramani,2021) 0.93 0.94 0.93 1 ZYJ(Zhao,2021) 0.92 0.93 0.92 2 ssn-diBERTsity(Setal.,2021a) 0.91 0.93 0.92 2 MUCS(Balouchzahietal.,2021) 0.92 0.93 0.92 2 IRNLP-DAIICT-LR(Daveetal.,2021) 0.92 0.93 0.92 2 IIITK(Ghanghoretal.,2021b) 0.92 0.92 0.92 2 HopeIsAllYouNeed 0.92 0.93 0.92 2 dhivya-hope-detection(Chinnappa,2021) 0.92 0.92 0.92 2 CFILT-IITB-Submission 0.92 0.93 0.92 2 snehan-coursera 0.92 0.91 0.91 3 IIITT-KarthikPuranik(Puraniketal.,2021) 0.92 0.91 0.91 3 MUM 0.89 0.91 0.9 4 IIIT-DWD(SaumyaandMishra,2021) 0.9 0.91 0.9 4 e8ijs 0.91 0.92 0.9 4 wrecking-crew 0.9 0.91 0.87 5 HopeFighters 0.83 0.91 0.87 5 Amrita-CEN-NLP(Setal.,2021b) 0.83 0.91 0.87 5 mlGeng 0.86 0.85 0.85 6 TeamX-OlawaleOnabola 0.9 0.77 0.81 7 KBCNMUJAL 0.88 0.5 0.61 8 Table 6: Rank list based on F1-score along with other evaluation metrics (Precision and Recall) for English lan- guage 67theStratiﬁed-K-Foldmethodtoaddressclass achieved an F1 score of 0.73 (9th rank) in imbalance. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Rank list based on F1-score along with other evaluation metrics (Precision and Recall) for English lan- guage 67theStratiﬁed-K-Foldmethodtoaddressclass achieved an F1 score of 0.73 (9th rank) in imbalance. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  </Paper ID = 637> 

<Paper ID = 638>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Examples of hope speech, non hope model perspectives (Park et al., 2018; Bender speech, and not lang in English, Tamil and Malay- alam. </Abstractive Summary>  <Extractive Summary> =  texts. For instance, the non hope example The datasets are generated from YouTube for Tamil dataset in Table 1 shows a combi- comments, and manually labeled for the three nation of code-mixed (English and Tamil) and labels hope speech, non hope speech, and not English transliteration of Tamil. It is common lang. lang to refer that the text does not belong to We attribute these labels to the error caused the specific language. Table 1 presents exam- by the language detector. ples of hope speech, non hope speech, and not Figure 1 presents wordclouds for the hope lang from the three datasets. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Results obtained with hope speech identified from text (YouTube comments) across the three languages English, Tamil, and Malyalam. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 638> 

<Paper ID = 639>  </Paper ID = 639> 

<Paper ID = 640>  </Paper ID = 640> 

<Paper ID = 641>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Weighted average precision, recall and F1- input sequentially. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  </Paper ID = 641> 

<Paper ID = 642>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  mBERTcased alam and Tamil languages in tables 5, 6 and 7 re- withBiLSTMarchitectureappearedtobethebest spectively. model with an F1-score of 0.6183 for validation Table 5 demonstrates that the character-aware butdroppeddrasticallyby8%forthetestdata. We model CharacterBERT performed exceptionally witnessedaconsiderablefallinthescoresofother modelslikemBERTandXLM-RoBERTawithlin- 7http://commoncrawl.org/the-data/ earlayersofupto15%. </Extractive Summary>  </Table ID = 5>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Improving wordnets for under- resourced languages using machine translation. In shown in Table 8. They were the best performing Proceedings of the 9th Global Wordnet Confer- modelsonthevalidationset. </Extractive Summary>  </Table ID = 8>  </Paper ID = 642> 

<Paper ID = 643>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  timentanalysisapproachtoidentifyspamreviews Thedescriptionofbothtraininganddevelopment onthee-commerceplatform. Threeclassiﬁersran- set for all three corpora is shown in Table 1. As domforest,gradientboosting,andsupportvector showninTable1,thecurrentsystemwastrainedon machinewastrainedonAmazonreviews. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Themetric AscanbeseenfromTable2and3,weightedF1 usedtoevaluatetheperformanceofamodelwas reportedondevelopmentdataandtestdataareal- the weighted F1-score (or weighted F1). Table 2 mostequal. Thatveriﬁestheauthenticityofmodel showstheexperimentalresultsofvariousmachine training. </Extractive Summary>  </Table ID = 2>  </Paper ID = 643> 

<Paper ID = 644>  </Paper ID = 644> 

<Paper ID = 645>  </Paper ID = 645> 

<Paper ID = 646>  <Table ID = 1>  <Abstractive Summary> =  Table 1: The result of our model and method on the two languages are very consistent. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: The results of our model and method on the In this task, our team used the XLM-RoBERTa testset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 646> 

<Paper ID = 647>  <Table ID = 1>  <Abstractive Summary> =  Table 1: The resulting score of our model method on agingtheresultsofkdifferentgrouptraining. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 647> 

<Paper ID = 648>  </Paper ID = 648> 

<Paper ID = 649>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We Evaluation for Tamil. Table 3 overviews use the RoBERTa transformer (Liu et al., 2019) the results of detecting hope-speech, non- forEnglish. WeselectXLM-RoBERTa(Conneau hope, and not-tamil. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Sentence trans- formerswerecreatedbyutilizingword-levelrep- Observations. In Table 5, we present a quali- resentation models such as BERT and RoBERTa tative evaluation using select examples from the forbetterdownstreamcomputationalperformance English test set. First, we examine misclassiﬁ- on sentence-level tasks, since utilizing word- cations in the English test set and report on our level representations for tasks such as determin- observations. We use the paraphrase-xlm-r- 3 samples predominantly contain English words multilingual-v1pretrainedmodeltogetour thatseemtobenon-hopesentences. Wedemon- sentenceembeddings,andthenuseK-meansclus- strateoneofthenon-englishsentencesinthe teringusingscikit-learn(Pedregosaetal.,2011)to test set in the sixth row of Table 5). The third clusterthetestsetinto3clusters,thesamenumber not-englishsamplecontainsalatin-alphabet of classes as the classiﬁcation task. </Extractive Summary>  </Table ID = 5>  </Paper ID = 649> 

<Paper ID = 650>  </Paper ID = 650> 

<Paper ID = 651>  <Table ID = 5>  <Abstractive Summary> =  Table 5: F1-score for development and test Weimplementedallourmodelsintensorﬂow5. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: F1-score for development and test data.CNN_ft_UP: 1-D CNN with up-sampled data using FastText embeddings, CNN_ft_NORM: 1-D CNN without any augmentation technique using FastText embeddings, CNN_ft_AUG: 1-D CNN with dataaugmentationusingbacktranslationwithFastText embeddings. </Abstractive Summary>  <Extractive Summary> =  1- D CNN valuesbetween10−5 to10−3. with data augmentation using back translation (CNN_ft_AUG)obtainedaweightedF1-scoreof Epochs: Range of values between 30 and 100 0.63 on development data set, see Table 6 and withanintervalof10wereconsidered. Figures: 5,6,7 Dataimbalance: Themajorityofthecomments Bi-LSTM with attention using FastText: Our inthedatasetbelongtonon-hopespeechcategory. </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  5 Conclusion Weevaluatedtheperformanceofourmodelsonthe test data using sklearn F1-score 6, we found that 1-DCNNonup-sampleddatawithbothWord2Vec andFastTextembeddingsperformedbetterthanthe Bi-LSTMmodelwhichwesubmittedforthetask. However,Bi-LSTMperformedrelativelybetteron the development data set (see Table 7). We also observedthatrandomup-samplingyieldedbetter resultscomparedtodataaugmentationwithback Figure 6: Confusion matrix for test data: 1-D CNN translationmethod. </Extractive Summary>  </Table ID = 7>  </Paper ID = 651> 

<Paper ID = 652>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Results for both the in Malayalam and 0.79 in Tamil. Table 2 shows approaches on our test split generated from the thedatadistributionbetweentraining,development providedtrainanddevdatasetsarereportedinTa- andtestdatasets. Thereareatotalof28,451com- bles3,4and5forEnglish,TamilandMalayalam mentsinEnglish,10,705commentsinMalayalam respectively. </Extractive Summary>  </Table ID = 2>  </Paper ID = 652> 

<Paper ID = 653>  <Table ID = 2>  <Abstractive Summary> =  Table 2: WeightedF1 score over Test Sets using Para- Table1:WeightedF1scoreoverEnglishValidationSet phrasingDataAugmentation 1654 Results Bharathi Raja Chakravarthi and Vigneshwaran Mural- idaran. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 653> 

<Paper ID = 654>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Performance comparison of different models on test set where P, R, F denotes precision, recall and weighted f -score. </Abstractive Summary>  <Extractive Summary> =  Increasing the numberofinstancesintheNHSclassmaymitigate 5.1 ErrorAnalysis thechanceofexcessivemisclassiﬁcation. It is evident from the Table 3 that XLM-R is the 6 Conclusion best performing model to detect hope speech for English,TamilandMalayalamlanguages. Adetail ThispaperdescribesandanalysestheseveralML, error analysis is carried out using the confusion DL,andtransformer-basedmethodsthatwehave matrixtoinvestigatemoreinsightsconcerningthe adoptedtoparticipateinthehopespeechdetection individual class performance (Figure 2). </Extractive Summary>  </Table ID = 3>  </Paper ID = 654> 

<Paper ID = 655>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Label distribution of Malayalam language speciﬁcembeddingcanimprovetherepresentation subtask ofthetargetgroup,andthusimprovethescoreof F1. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 655> 

<Paper ID = 656>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Examplesofinputtextandextractedfeaturesfor onﬁnancialmarkets., code-mixedtexts Ec,Eco,con,ono,nom, omi,mic,ic , ne,new, ews,ws , ha,hav,ave, ve , li,lit,itt,ttl,tle,le , function from NLTK library to extract char se- ef,eff,ffe,fec,ect,ct , on, quences of length 3 to 6 from texts along with “Economicnews on , ﬁ,ﬁn,ina,nan,anc, tokenized words for Ma-En and Ta-En language havelittleeffect nci,cia,ial,al , ma,mar, onﬁnancialmarkets.” ark,rke,ket,ets,ts.,s. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Examplesofinputtextandextractedfeaturesfor Englishtexts modelisdevelopedasahardvotingclassiﬁerbased on bagging by ensembling three sklearn9 classi- ﬁers,LogisticRegression(LR),eXtremeGradient Boosting (XGB) (Chen and Guestrin, 2016) and Estimators Parameters Multi-LayerPerceptron(MLP)10.Ideabehinden- max depth=20,n estimators=80, semblingsimpleclassiﬁersasestimatorsistobuild learning rate=0.1, a robust classiﬁer utilizing the strength of each XGB colsample bytree=.7, classiﬁer. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Due to simplic- ityandefﬁciencyofbaggingmethod,CoHope-ML Table 2: Examplesofinputtextandextractedfeaturesfor Englishtexts modelisdevelopedasahardvotingclassiﬁerbased on bagging by ensembling three sklearn9 classi- ﬁers,LogisticRegression(LR),eXtremeGradient Boosting (XGB) (Chen and Guestrin, 2016) and Estimators Parameters Multi-LayerPerceptron(MLP)10.Ideabehinden- max depth=20,n estimators=80, semblingsimpleclassiﬁersasestimatorsistobuild learning rate=0.1, a robust classiﬁer utilizing the strength of each XGB colsample bytree=.7, classiﬁer. Parametersusedforeachestimatorare gamma=.01,reg alpha=4, given in Table 3. CoHope-ML model is trained objective=’multi:softmax’ onTFIDFvectorsobtainedinfeatureengineering hidden layer sizes=(150,100,50), module. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  • MaxPooling1Dwithpoolsizeof2 Training BERT LM: BERT LM is trained us- • Length of words sequences = 250 with ingthetrainedtokenizerandrawtextsusedinprevi- paddingforshortsentences ousstepandtransformerslibrary13 withfollowing conﬁgurations: is used to train CoHope-TL model for 50 epochs with a batch size of 126. Table 4 gives summary • vocab size=52 000 of the layers in BiLSTM-Conv1D model and the • max position embeddings=514 frameworkofCoHope-TLisshowninFigure3. • num attention heads=12 4 ExperimentalResults • num hidden layers=6 4.1 Datasets DatasetsusedinthisstudyincludeunannotatedRo- • type vocab size=1 manized text from Dakshina (Roark et al., 2020) 11https://keras.io/ combined with texts from (Chakravarthi et al., 12https://huggingface.co/transformers/ 2020c), (Chakravarthi et al., 2020a) and anno- tokenizer summary.html 13https://pypi.org/project/transformers/ tated datasets provided by organizers which are 183Figure2: FrameworkofCoHope-NNmodel Figure3: FrameworkofCoHope-TL 184Layer(Type) Outputshape P R F1 Rank LP CoHope-ML Ta-Enand Ma-En 0.85 0.85 0.85 1 English Ta-En 0.59 0.59 0.59 3 Ma-EN English 0.92 0.93 0.92 2 Embedding (None,250,768) (None,250,1024) CoHope-NN Conv1D (None,250,32) (None,250,32) Ma-En 0.83 0.83 0.83 MaxPooling1D (None,125,32) (None,125,32) Ta-En 0.57 0.57 0.56 English 0.91 0.92 0.91 Bidirectional (None,600) (None,600) Dense (None,3) (None,3) CoHope-TL Ma-En 0.79 0.76 0.77 Table4: LayersinBiLSTM-Conv1D Ta-En 0.55 0.54 0.54 English 0.90 0.90 0.90 Set LP NO HS OL Table6: Resultsoftheproposedmodels Train Ma-En 6205 1668 691 Ta-En 7872 6327 1961 English 20778 1962 22 Dev. </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Annotated datasets include two code-mixed datasets Ta-En and Ma-En along with English Figure4: Statisticsofrawtexts datasets. Texts in the datasets for each Lan- guagePairs(LP)aredistributedinthreecategories namely,“Hope speech(HS)”,“Non hope speech models proposed by MUCS is shown in Table 6. (NO)”,and“other languages(OL)”. (NO)”,and“other languages(OL)”. Statisticsof As it is illustrated in Table 6, both CoHope-ML labelsdistributionintrain,development(Dev.) and andCoHope-NNmodelsutilizingcharsequences, test sets and given in Table 5. It can be observed traditionaln-gramsandsyntacticngramsfeatures thatasMa-Encode-mixedtextsincludesigniﬁcant outperformedtheCoHope-TLmodel. </Extractive Summary>  </Table ID = 6>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (NO)”,and“other languages(OL)”. Statisticsof As it is illustrated in Table 6, both CoHope-ML labelsdistributionintrain,development(Dev.) and andCoHope-NNmodelsutilizingcharsequences, test sets and given in Table 5. It can be observed traditionaln-gramsandsyntacticngramsfeatures thatasMa-Encode-mixedtextsincludesigniﬁcant outperformedtheCoHope-TLmodel. </Extractive Summary>  </Table ID = 5>  </Paper ID = 656> 

<Paper ID = 657>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Validation set perplexity of RoBERTa and ofdatasetsofbothtasks. </Abstractive Summary>  <Extractive Summary> =  OurbaselineusesKNNclassiﬁer epochsusingalearningrateof5e-5andadropout onembeddingsgeneratedusingcode-mixedULM- of 0.1 for attention and hidden layers. Table 2 FiT model from iNLTK8 (Arora, 2020b). We set comparesperplexityofourpre-trainedRoBERTa k=5forallourexperimentswithuniformweighting modelwiththatofULMFiTmodelwhichisalso onneighbors. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  This ﬁne-tuned language model. Table 3 contains de- stepisrequiredbecausebothofourpre-trainedlan- tailsofhyperparametersoftheﬁrstclassiﬁerwhich guagemodels,ULMFiTandRoBERTa,aretrained is trained on our pre-trained RoBERTa. We train on code-mixed data in Latin script. </Extractive Summary>  </Table ID = 3>  </Paper ID = 657> 

<Paper ID = 658>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Distribution of training data samples across XLM-RoBERTabypre-trainingmultilingualmod- threeclasses els at scale. </Abstractive Summary>  <Extractive Summary> =  for all three languages. Table 1 shows the class Thedatasetforthistaskiscode-mixedsuchastag, distributionofthedatasetforEnglish,Tamil,and inter-sentential,andintra-sentential(Chakravarthi, Malayalam. 2020). </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We also In line with shared task organizers, we also used plantosystematicallystudytheeffectofabolish- weightedF1-scoreasanevaluationmetric. ingthe‘Notinintendedlanguage’classfromthe In Table 2, the performance of our system us- dataset as it opens up the opportunity to have a ing above mentioned four transformer model as singlesystemordeeplearningmodelforallthree 195languages. deep bidirectional transformers for language under- standing. </Extractive Summary>  </Table ID = 2>  </Paper ID = 658> 

<Paper ID = 659>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We have tried several different combinations of ThecompetitionorganizershaveusedtheSVMs, models discussed in the section 4 across the MNBs , Decision Trees and other machine learn- datasets of each language and have reported our ing models as the baseline models for the given resultsonthedevelopmentsetandthetestsetinthe datasets. So, we went with using the mod- Table 2 and Table 3 respectively. The results are els built upon the transformer architecture while beingreportedintermsofweightedF1scoresasit approaching the problem. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We have tried several different combinations of ThecompetitionorganizershaveusedtheSVMs, models discussed in the section 4 across the MNBs , Decision Trees and other machine learn- datasets of each language and have reported our ing models as the baseline models for the given resultsonthedevelopmentsetandthetestsetinthe datasets. So, we went with using the mod- Table 2 and Table 3 respectively. The results are els built upon the transformer architecture while beingreportedintermsofweightedF1scoresasit approaching the problem. </Extractive Summary>  </Table ID = 3>  </Paper ID = 659> 

<Paper ID = 660>  </Paper ID = 660> 

<Paper ID = 661>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Number of text samples in the training, the modelssuchasBERTorXLM-R. </Abstractive Summary>  <Extractive Summary> =  Ro- addedthelanguagemodeladaptationtechniquede- manian(RO).Thenumberofsamplesinthetrain- scribed by (Jauhiainen et al., 2018). They used ing, the development and the test sets are shown oneepochoflanguagemodeladaptationtothetest in Table 3. All text samples were automatically data. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  inwhichthein-domaindataavailablefordevelop- mentwasscarce. As shown in Table 4, the best results in the 2021 6 SocialMediaVarietyGeolocation RDIsharedtaskwereattainedbytheSUKIteam. (SMG) ComparedwiththeirownresultsJauhiainenetal. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Resultsandrankings coordinate and the longitude coordinate. Table 5 forthethreetasksarepresentedinTable6. showsthekeyﬁguresofthedatasets. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: SMG task results. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  </Paper ID = 661> 

<Paper ID = 662>  </Paper ID = 662> 

<Paper ID = 663>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Distribution of z-scores for each coarse- weﬁlteroutfeaturesthatshowacomparabledis- grained POS for standard German (negative) vs. </Abstractive Summary>  <Extractive Summary> =  The clear preference for pro- between two corpora that are systematic across nouns in Kiezdeutsch, as opposed to nouns, can classesandgobeyondtheidiosyncraticuseofex- beexplainedbythetopicsofspontaneousspeech tremelycorpus-speciﬁctags. being much more related to conversations in- Table 1 reports the z-scores associated to each volving self-reference and reference to further POS.AcrossthetenPOStagsunderanalysis,ﬁve actors present in the scene. Corpus examples aresigniﬁcantlymorepredictiveofGRAIN(neg- for the three most predictive KidKo trigrams in ative values) and ﬁve of KidKo (positive values). </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  presence/absenceofeachPOS(suchasNOUN). Table 2 lists the most predictive trigrams for GiventheextremegranularityofthePOStypes GRAIN(left)andforKidKo(right). Overall,178 in the original collections, we decided to use a trigramsarehighlysigniﬁcantforGRAINand181 coarse-grainedclassiﬁcationof10POStypesonly, for KidKo. </Extractive Summary>  </Table ID = 2>  </Paper ID = 663> 

<Paper ID = 664>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ber of samples per variety per register to 20. As In the ideal case, there would be a consistent shown in Table 2 for Spanish, there is variation levelofsimilarityacrosssamplesrepresentingall in the number of samples per data source. For languages and all countries. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  focusedonaspeciﬁcpartofthisproblem. The results are shown in Table 3 by language, Theﬁrstexperiment(Section6),focusesonthe divided into Word and Character features as dis- internalconsistencyofeachvariety-registercom- cussed in Section 4. Data from tweets (TW) is bination: given similarity relationships between shownontheleftandfromtheweb(CCforCom- manysamplesofMexicanSpanishtweets,howsta- monCrawl)isshownontheright. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thisissimilarto arangeofdifferentcommunicationpurposes;(ii) previousevaluationmethodsforcorpussimilarity itcouldbethattweetsrepresentamorerestricted measures(Kilgarriff,2001). subset of local populations (e.g., Twitter users), The results, shown in Table 4, have a high ac- whichhavebeenshowntobemoreinﬂuencedby curacy in general, as we expect given that each demographicattributeslikepercapitaGDP. sample is 1 million words. </Extractive Summary>  </Table ID = 4>  </Paper ID = 664> 

<Paper ID = 665>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (SCONJ) for which both precision and recall in- ThedifferencesbetweentheEWTconcatenated creased. Interjections(INTJ)weresurprisinglynot modelandtheEWTzero-shotbaselinearemuch predicted correctly, achieving 0% for both preci- smaller, as can be seen in Table 2. While there sionandrecall,despitethezero-shotmodelachiev- are some improvements in both precision and re- ing100%precisionandrecall. </Extractive Summary>  </Table ID = 2>  </Paper ID = 665> 

<Paper ID = 666>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (2017)5, due to its open- source access. It uses a Seq2Seq architecture to Table 1 shows the BLEU scores on the test sets back-translate bilingual sentence pairs. For each for source and target language. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  53trainingsentences. Yet,eveninthissettingthepro- 30 h posedmodelremainsmuchmoreefﬁcient,asseen s gli in the column cost (FLOPs) @ 100K in Table 2. n20 E n- Theback-translationmodelisthusrequiredtotrain a m on more data and ultimately longer until conver- Ger10 OAuntlyo eAnuctoodeinncgo +di nPgaraphrasing gence and cannot achieve similar efﬁciency with Conneau and Lample (2019) scorescomparabletoourapproach. </Extractive Summary>  </Table ID = 2>  </Paper ID = 666> 

<Paper ID = 667>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Statistics of corpora used for model training. </Abstractive Summary>  <Extractive Summary> =  In brary2. Thestatisticsforeachcorporaisshownin contrast,weobserveatendencyofboththemeth- Table 2 with the vocabulary size calculated after odstowardsalargerwindowsize(win = 5). Such ignoringwordsappearinglessthan100times. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Hyperparameter search space studied in our fractionofsentencesfromtheentirecorpus(4.3.2). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  the previously released fastText. Table 5 shows 4.3.2 Low-resourcesetting the evaluation results for all ﬁve models. Out of We analyze the performance of all three models thethreestudiedmodels,SVDconsistentlyshows with varying data sizes for each language. </Extractive Summary>  </Table ID = 5>  </Paper ID = 667> 

<Paper ID = 668>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  FastTextbuildsuponthisworkbyproposing 4.2 Baselinewithlinearmodels an extension to the skipgram model which takes intoaccountsub-wordinformation. Table 1 shows results for running the models on Bothmodelsuseaneuralnetworktolearnword a data set with 10K sentences in each language embedding from using a context windows con- category. Modelstendtoperformbetterifweuse sisting of the words surrounding the current tar- characterbi-gramsinsteadofsinglecharacters. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Overview of results for the "classical" ML blockhas128ﬁlterswhilethesecondhas64. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Overview of results for the neural network models for the data set with 10K data points in each accuracyof93.3%,isstillworsethanfortheCon- language. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 668> 

<Paper ID = 669>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  1 score slightly to 0.8118. If the blacklists gener- Table 1 lists the results of these and some of the ated from the dev-dev set were not pruned using following experiments using the training and the the training set, the score was merely 0.5719 us- developmentsets. ingthecompleteblacklistand0.8076ifonlythose Thesecondexperimentwastocombinethetrain- n-grams that occurred more than 16 times were ingdatawiththedev-devdata,retraintheclassiﬁer used. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: The results of each team participating on the acter n-grams used were from 2 to 5. </Abstractive Summary>  <Extractive Summary> =  We would like to thank the RDI organizers for organizingthesharedtask. Theresearchpresented 5.3 Results inthisarticlehasbeenpartlyfundedbyTheFinnish Table 2 shows the results of the shared task. Our ResearchImpactFoundation,theKoneFoundation, secondrunwonbyaconsiderablemargin. </Extractive Summary>  </Table ID = 2>  </Paper ID = 669> 

<Paper ID = 670>  </Paper ID = 670> 

<Paper ID = 671>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Impact of the parameter values on the test mance,especiallyontheMacro-F1. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 671> 

<Paper ID = 672>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thedeeplearningmodelsthat 3.1.3 Evaluation wereusedinlastyear’stask(PopaandStefa˘nescu, Thetweetsdatasetwasalreadybalanced,with2625 2020;Zahariaetal.,2020)allreliedonpre-trained standard Romanian tweets and 2612 Moldavian BERT models (Dumitrescu et al., 2020), which tweets. As we see in Table 2, data augmentation might not always be available when working on improved the performance of the CNNs dramati- low-resourcelanguages. EventhoughCNNshave cally,tothepointthatitbecamecomparabletothat beensuccessfullyappliedtothetaskofdistinguish- ofshallowmodels. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Final performance of the teams’ submissions totheRDItask. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Evaluation of the models used to distinguish shallowmodelsinthisﬁrststage. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Evaluation of the models used to distinguish 1 asexpected. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 1>  <Abstractive Summary> =  Table 14: Summary of our experiments on data aug- mar, Bornini Lahiri, and Mayank Jain. </Abstractive Summary>  <Extractive Summary> =  In Proceedings of the 7th Theresultsoftheexperimentsonthemodelpre- Workshop on NLP for Similar Languages, Varieties trained on news (the news+tweets model) are in andDialects,pages232–241. Table 14. All techniques led to an improvement Marcos Zampieri, Shervin Malmasi, Nikola Ljubesˇic´, of the performance of the network, and the best Preslav Nakov, Ahmed Ali, Jo¨rg Tiedemann, Yves improvementwasobtainedbyshufﬂingthewords Scherrer, and Noe¨mi Aepli. </Extractive Summary>  </Table ID = 1>  </Paper ID = 672> 

<Paper ID = 673>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ger,themodelissurpassedbymBERT.Onereason forthiscanberepresentedbytheinclusionofmore 4.2 ImplementationDetails RomanianentriesinthemBERTpre-trainingcor- For running the Transformer-based models pus. we used Adam with weight decay optimizer Table 2 contains the results obtained by us- (AdamW)(KingmaandBa,2014)andanepsilon ingvariousmachinelearningtechniquesalongside valueof1e-8. Weﬁne-tunedthemforfourepochs character n-gram features. </Extractive Summary>  </Table ID = 2>  </Paper ID = 673> 

<Paper ID = 674>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: The results of each team participating on the DLIsharedtaskintermsofMacroF1. </Abstractive Summary>  <Extractive Summary> =  Thisvotedensemblemethod 8Indic transliteration tools provide such func- hasimprovedresultsinmanytasks(Hettiarachchi tionalities - https://pypi.org/project/ indic-transliteration/ andRanasinghe,2020b)fortransformers. 1245.3 ResultsontheTestSet 7 Conclusion WepresentthesubmissionsbyteamHWRtothe Table 4 shows the results of the shared task. Our Dravidian Language Identiﬁcation (DLI) shared ﬁrstrunwasclearlybetterthanthesecondandnot taskatVarDial2021. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  6 Discussion References We did not experiment with the NB identiﬁer us- ingnon-alphabeticcharactersnorwiththeoriginal BharathiRajaChakravarthi,MihaelaGa˘man,RaduTu- casingofthealphabeticcharacters. Inlightofthe dor Ionescu, Heidi Jauhiainen, Tommi Jauhiainen, results for the HeLI method in Table 3, it might Krister Linde´n, Nikola Ljubesˇic´, Niko Partanen, Ruba Priyadharshini, Christoph Purschke, Eswari beapromisingdirection. Itseemsthatatleastfor Rajagopal, Yves Scherrer, and Marcos Zampieri. </Extractive Summary>  </Table ID = 3>  </Paper ID = 674> 

<Paper ID = 675>  <Table ID = 1>  <Abstractive Summary> =  Table 1: F-scores of Probcat on dev and dev-test sets, withrespectton-gramlength(n). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We made a single submission using this ap- proach,usingonly5-gramsasfeatures. Theentire The results, summarized in Table 2, suggested trainingsetwasusedtotraintheclassiﬁerforthis frequency-based sampling worked best, but all submission. sampling functions achieved high scores, with Asforthetransformermodel,weconductedvar- accuracy-based sampling working slightly better iousexperimentsinvolving: optimizingthearchi- thanuniformsampling. </Extractive Summary>  </Table ID = 2>  </Paper ID = 675> 

<Paper ID = 676>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ing, development and test data for the three sub- tasks (Chakravarthi et al., 2021). Table 1 gives an overview of the data. It can be seen that the BERT models are pre-trained from scratch using BCMSandDE-ATdatasetsareroughlyequivalent the VarDial training data, and unconstrained sub- in size, whereas the CH dataset is more than one missions, where we use pre-trained off-the-shelf orderofmagnitudesmaller. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Effect of different pre-training schemes for denselypopulatedanddonotﬁtintoasquare. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  However,the tunitytotestanewmodel,BERTic´ (Ljubesˇic´ and k-meansdevelopmentreconstructionerrorsarestill Lauc, 2021).6 Comparing the two models on the belowtheﬁxed-gridones. developmentdatashowedsigniﬁcantgainsforthe The right side of Table 3 contains the results BERTic´ model,withamediandistanceof30.11in of the experiments on the development data. In comparisontoamediandistanceof40.05forthe termsofdistance,the(parameter-free)regression CroSloEngualBERT. </Extractive Summary>  </Table ID = 3>  </Paper ID = 676> 

<Paper ID = 677>  </Paper ID = 677> 

<Paper ID = 678>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Themajoradvantageofthedialectalrelativepro- SurveyingArabicspeakingproﬁles,itiscustomary noun (cid:250)(cid:10)(cid:206)(cid:203)(cid:64) is that it is present in most (if not all) toseeusersdeclaringtheirpatriotismandnational Arabicdialectswiththesamemeaningbutnotin belonging by using their county’s ﬂag or explic- MSA. Table 1 shows some examples of such us- itlynamingthecityorcountrythattheyarefrom age across different dialects. In doing so, we la- (e.g. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  AppropriatenessIdentiﬁcation Thethirdﬁlter In all, we had 3,303 test tweets (with 183 tweets removed users who were mostly tweeting vulgar, on average for each of the 18 countries). Table 2 sexuallyexplicit,orpornographictweets. Toﬁlter liststhenumberoftesttweetspercountry. percountryonaverage)withatotalof8.8Mwords. Table 2 provides per country breakdown of the 6The data set can be downloaded from: http:// dataset. hidden-for-blind-review/ 4Country IQ BH KW SA AE OM QA YE SY Users 142 169 160 149 172 176 139 138 139 TrainingTweets(k) 18.4 28.3 49.9 35.4 27.8 24.8 36.7 11.6 18.3 Testtweets 178 184 190 199 192 169 198 193 194 Country JO PL LB EG SD LY TN DZ MA Users 146 145 141 150 139 149 68 130 73 TrainingTweets(k) 34.1 48.6 38.4 67.8 16.3 40.9 12.9 17.6 12.8 Testtweets 180 173 194 200 188 169 154 170 178 Table2: Thenumberofusersandtweetspercountryinourtweetcorpus. Thedetailsofthe countrydoesnotguaranteethatauserwillalways trainingandtestsplitsforthedatasetasasfollows: tweet in the dialect of that country. Often users QADI Dataset: Table 2 provides the statistics of from different countries use MSA (or even other thetrainingandtestpartsofQADIdataset. Given languages). </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Recently, deep SVM classiﬁer with a linear kernel. When using 6TrainingSet Results Table 3 reports on the macro-averaged Classiﬁer QADI MADAR F1-scoreresultsoftrainingandtestingusingQADI MultiLangBERT 58.9 25.3 AraBERT 60.6 29.0 andMADARdatasets. AsQADIresultsshow,us- Mazajak 39.8 24.6 ingcontextualembeddingsyieldedthebestresults SVM 57.3 25.6 C{2−6} withAraBERTresultsedgingmBERTresults. </Extractive Summary>  </Table ID = 3>  </Paper ID = 678> 

<Paper ID = 679>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Overall, for the same word exist, usually with additional pre- each dialect and each relation, around 100 word ﬁxes or sufﬁxes such as the equivalent of the arti- pairs were generated. Table 1 shows the statis- cle“the”orpossessivedeterminerssuchas“her”, ticsofDiaLex’swordpairslistsandTable2shows “his”,or“their”. Forexample,consider(cid:13)oneques- someexamplewordpairsinDiaLexandtheirEn- (cid:201)(cid:103)(cid:46) (cid:64)(cid:80) (cid:73)(cid:16) (cid:131) (cid:81)(cid:30)(cid:10)(cid:211)(cid:64) tion which asks to is like to “ ?”, glishandMSAtranslations. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Evaluation of six word embedding models using our benchmark across the ﬁve dialects. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 679> 

<Paper ID = 680>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Statistics of training and testing datasets, showing the number of examples for both sarcasm detectionandsentimentanalysistasks. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Nevertheless,itisalsonotice- resultsachievedbyArabic-speciﬁclanguagemod- abletheappliedprepossessinghasaneffectwhere els. As can be seen in Table 2, most models, in- similarmodelsachievedifferentresults. Forexam- cludingtheBiLSTMbaseline,areachievinggood ple,AraBERT-baseandArabicBERTarebasedon resultsonthesentimentanalysistask. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  els, we can conclude that the representation learntbyBERT/ELECTRAisbettersuitedfor 5 SummaryofFindings classiﬁcation. From the experimentation of the existing Table 3 summarises all our ﬁndings on these transformer-based LMs on Arabic sentiment and models,includingtheirdetails,performance,archi- sarcasmdetection,wecanlearnthefollowing: tecture, number of parameters, training data size • Existing Arabic speciﬁc models are bet- andsource. ter than multilingual ones. </Extractive Summary>  </Table ID = 3>  </Paper ID = 680> 

<Paper ID = 681>  <Table ID = 2>  <Abstractive Summary> =  Table 2: F1 scores for ﬁne-tuned AraBERT (version 0.1) on the original and perturbed test sets with adversarial examples. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  4.3 ResultsandDiscussion Token Transformation Analysis Figure 3 The performance results are reported in Tables 1 shows the vector representation for the question and 2. Table 1 shows the EM score for the and the passage tokens reduced into 2D using AraBERT (version 0.1) model tested on the orig- PCA. Vector transformation from the bottom to inalMRCdatasetsandtheattackedvariantswith the top layers suggests that the model follows adversarialexamplesdiscussedinSection3. </Extractive Summary>  </Table ID = 1>  </Paper ID = 681> 

<Paper ID = 682>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  However, we are satisﬁed that our sandstorms and explosions. Table 1 lists these datacapturedthekeyaspectsofthecrises. Inthe crisesbydate;ﬂoodeventsoccurringinthesame caseofCOVID-19,wetrackedonlyninekeywords areaarereferencedbycountryandyearofoccur- referring to the crisis by name because the event rence. To identify duplicate content, corpus and retained only tweets with unique IDs. we ﬁrst cleaned the tweet text by removing RT, Table 1 shows the number of unique tweets and URL,username,punctuationandspecialcharac- uniqueauthorsforeachcrisis,alongwiththenum- ters. This pre-processing also removed diacritics ber of tweets published by veriﬁed accounts. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Toidentifysuchwords,sam- new or retweeted. Table 2 shows the percentage plesoftweetswerecollectedfromthecountriesin ofduplicatesinKawa¯rithbycrisis,alongwiththe ourlistandthedialectalstopwordsweremanually numberofnewmessagesandretweets. identiﬁed from the most frequent words in each sample. </Extractive Summary>  </Table ID = 2>  </Paper ID = 682> 

<Paper ID = 683>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We then converted the dataset to and are trained with a huge amount of data usu- tfrecords for faster training. In Table 1, we allylargerthan20GBoftextandthemodelsare presentthetotalnumberoftokensforeachdataset. typicallymorethan100millionparametersinsize. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Parameter Value Figure 1: Comparison between the teacher models Betadistillationfactor 500 whenﬁne-tunedintheAJGTtask. Gammadistillationfactor 5 In Table 3, we compare IBERTA to AraBERT Hiddendistillationfactor 100 fordifferentdatasetsandtasks. Wecannoticethat Distillationgroundtruth 0.5 ourteachermodelachievesbetterresultsonmost ofthetextclassiﬁcationtasks. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Moreover,we thereasonbehindthatisthecomplexityofthetask showed that using smaller datasets with smaller andthequalityofthedatasetwhichhassomeau- vocabulary,wecanachievereasonableresultscom- tomatically created translations. In Table 4, we paredtoothermodelsintheliterature. Thisshows compare all the models in terms of the number thatcarefullycurateddatasetswithhighdiversity of parameters, on-disk space and quantized mod- evenifsmallercangetsomegoodresults. </Extractive Summary>  </Table ID = 4>  </Paper ID = 683> 

<Paper ID = 684>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Correlation and agreement tests results of the common emoji in Arab-ESL and Euro-ESL under eight differentcategories. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 684> 

<Paper ID = 685>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Comparison between ArCOV19-Rumors and existing Arabic COVID-19 datasets for veriﬁcation over tweets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Statistics of Claims in ArCOV19-Rumors ing the claim can be over an external link in the (RPs=Replies,RTs=Retweets). </Abstractive Summary>  <Extractive Summary> =  Tomeasureannotationquality,werandomly 4 ImmediateUseCases selected 10% of the relevant tweets, and asked a second annotator to label them. We found that Table 2 presents a statistical summary of the la- theagreementratiobetweenannotatorsis0.87and beledtweetsinArCOV19-Rumors. Wedeﬁnetwo 0.80forrelevanceandstancerespectively. </Extractive Summary>  </Table ID = 2>  </Paper ID = 685> 

<Paper ID = 686>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Examples of trending topics in social media and diversity of the tweets in our dataset, we ex- matchedbyspikesintweetingfrequencyinArCOV-19 aminedtheplaceandcoordinatesattributesofthe Tweetobject.16 Wenotethattheplaceattributeis We further explore the topics discussed in anoptionalattributethatallowstheusertoselecta 12https://tinyurl.com/rfj5khn 13https://tinyurl.com/yykgwbox 16https://developer.twitter.com/en/ 14https://tinyurl.com/yxkg78g5 docs/tweets/data-dictionary/overview/ 15https://tinyurl.com/y4nlgz5h geo-objects 87locationfromalistprovidedbyTwitter(therefore, 105 thelocationmightnotnecessarilyshowtheactual 104 locationfromwherethetweetisposted),whereas thecoordinatesattributerepresentsthegeographic weets103 lcoliceanttioanppolfictahtieontw. </Abstractive Summary>  <Extractive Summary> =  Fig- topics. Table 2 shows a timeline covering dates ure 4 shows that URLs from news websites are ofspeciﬁctopicsofdiscussiontrendingonsocial dominantly the most-commonly shared. We ob- mediaaroundtimesofpeaksintweetingfrequency serve that these news websites mainly originate inFigure3(c). </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Alternatively,to haveaninsightaboutthegeographicaldistribution Table 2: Examples of trending topics in social media and diversity of the tweets in our dataset, we ex- matchedbyspikesintweetingfrequencyinArCOV-19 aminedtheplaceandcoordinatesattributesofthe Tweetobject.16 Wenotethattheplaceattributeis We further explore the topics discussed in anoptionalattributethatallowstheusertoselecta 12https://tinyurl.com/rfj5khn 13https://tinyurl.com/yykgwbox 16https://developer.twitter.com/en/ 14https://tinyurl.com/yxkg78g5 docs/tweets/data-dictionary/overview/ 15https://tinyurl.com/y4nlgz5h geo-objects 87locationfromalistprovidedbyTwitter(therefore, 105 thelocationmightnotnecessarilyshowtheactual 104 locationfromwherethetweetisposted),whereas thecoordinatesattributerepresentsthegeographic weets103 lcoliceanttioanppolfictahtieontw. eet as reported by the user or mber of ret102 Table 1 shows that ArCOV-19 has 60,873 ge- Nu olocated tweets (i.e., having values in the place 101 attribute)and2,078geotaggedtweets(i.e.,having valuesinthecoordinatesattribute). Thosetweets 100 were posted by 24,072 and 256 unique users re- 27/0129/0131/0102/0204/0206/0208/0210/0212/0214/0216/0218/0220/0222/0224/0226/0228/0201/0303/0305/0307/0309/0311/0313/0315/0317/0319/0321/0323/0325/0327/0329/0331/0302/0404/0406/0408/0410/0412/0414/0416/0418/0420/0422/0424/0426/0428/0430/04 spectively. </Extractive Summary>  </Table ID = 1>  </Paper ID = 686> 

<Paper ID = 687>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Statistics of our ﬁne-tuning datasets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Performance of CAMeLBERT models trained on MSA datasets with different sizes. </Abstractive Summary>  <Extractive Summary> =  words), and MSA-1/16 (6GB, 636M words). In Experimental Setup Our models were ﬁne- Table 3, we show the results on our ﬁne-tuning tuned for 10 epochs with a learning rate of 3e-5, subtasks. batchsizeof32,andamaximumsequencelength We observe that the full MSA model and the of 128. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  This is the largest tuning performance and OOV rates, we assessed datasetusedtopre-trainanArabiclanguagemodel the correlation between model performance and to date. As shown in Table 4, the CAMeLBERT- Mix model improves over other models in three 6Weuseasimpletokenasaunit,wherewesegmenttext withwhitespaceandpunctuation. cases,allofwhicharedialectal,suggestingthatthe%Performance Task Dataset Variant Star Mix MSA DA CA X X X X X X X X 1 2 3 4 5 6 7 8 NER ANERcorp MSA 82.4 80.8 82.4 74.167.9 76.782.882.080.377.382.079.383.6 PATB(MSA) MSA 98.3 98.1 98.3 97.797.8 97.998.298.398.398.198.498.098.4 POS ARZTB(EGY) DA 93.6 93.6 93.6 92.792.3 92.093.094.193.193.393.693.593.6 Gumar(GLF) DA 98.1 98.1 97.9 97.997.7 97.497.898.197.897.798.097.997.9 ASTD MSA 76.9 76.3 76.9 74.669.4 64.574.278.173.574.274.377.074.9 SA ArSAS MSA 93.0 92.7 93.0 91.889.4 88.491.593.392.391.392.292.991.9 SemEval MSA 72.1 69.0 72.1 68.458.5 57.569.572.769.570.770.070.470.2 MADAR-26 DA 62.9 62.9 62.6 61.861.9 60.461.962.258.459.859.161.260.7 MADAR-6 DA 92.5 92.5 91.9 92.291.5 90.891.992.390.891.591.492.191.4 DID MADAR-Twitter-5 MSA 77.6 75.7 77.6 74.271.4 71.879.079.074.777.777.678.676.5 NADI DA 24.7 24.7 24.9 20.117.3 16.721.124.524.025.021.327.024.6 PoetryAPCD CA 80.9 79.8 79.7 79.680.9 78.879.679.978.879.179.179.078.4 MSA 83.4 82.1 83.4 80.175.7 76.182.583.981.481.682.482.782.6 Variant-wise-average DA 74.4 74.4 74.2 72.972.1 71.573.174.272.873.572.774.373.6 CA 80.9 79.8 79.7 79.680.9 78.879.679.978.879.179.179.078.4 Macro-average 79.4 78.7 79.2 77.174.7 74.478.479.577.678.078.178.978.5 Table5: PerformanceofCAMeLBERTmodelsandotherexistingmodels. </Extractive Summary>  </Table ID = 4>  </Paper ID = 687> 

<Paper ID = 688>  <Table ID = 1>  <Abstractive Summary> =  Table 10: Confusion Matrix with binary classiﬁer fulﬁllthetaskwithoutextensiveuseoflinguistics Arabic-BERTversusXLM-RonDatasetTwo. </Abstractive Summary>  <Extractive Summary> =  Compared to the gold standard CEFR- corporaforlanguagelearners. label,theclassiﬁersagreedinpredicting10204in- Table 1 shows distribution of the number of stances. Thenwhatweneedtoconsideriswhenall usedsentencesandtokenspereachCommonEu- classiﬁersagreeonthepredictedlabelanditcon- ropeanFrameworkoflanguageproﬁciencyRefer- tradictswiththegoldstandard’sone. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Number of Sentences and Tokens available pereachCEFRLevelinDatasettwo speechtagger(Pashaetal.,2014). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Part of speech tagging features (POS- word-wordrelations,itassumesthatthestructure features); Syntactic structure features (Syntactic- of a sentence consists of lexical items that are at- features); CEFR-level lexical features; Sentence tached to each other by binary asymmetrical re- embeddings. </Abstractive Summary>  <Extractive Summary> =  of text-level readability (Forsyth, 2014; Saddiki 3.1.3 CEFR-levellexicalfeatures et al., 2015; Nassiri et al., 2018a,b). We decided toexcludethesentencelengthfromthefeatureset, Features (28-34) from Table 3 are used to assign asthiscreatesanartiﬁcialskewinunderstanding each word in the sentence with an appropriate what is difﬁcult: more difﬁcult writing styles are CEFR level. For this, we created a new Arabic often associated with longer sentences, but it is word list consisting of 8834 unique lemmas la- notthesentencelengthwhichmakesthemdifﬁcult. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: 3-way classiﬁcation using weighted macro- ableatHuggingfacetransformers(AraBERT4 and averagedprecision, recallandF-1, DatasetOneUsing Arabic-BERT5). </Abstractive Summary>  <Extractive Summary> =  The pre- 4.1 ReadabilityasaClassiﬁcationProblem training data used for the AraBERT model con- sistof70millionsentences(Antounetal.,2020). Table 4 presents the results of classiﬁcation us- Arabic-BERTtrainedonbothﬁlteredArabicCom- ing updated version of dataset one after applica- monCrawlandarecentdumpofArabicWikipedia tionoftheerroranalysis. ApplyingdifferentML contain approximately 8.2 Billion words(Safaya approacheswith10-foldcross-validationonthe3- etal.,2020). way multi-class classiﬁcation. The classiﬁcation results as presented in Table 4 divided into two 4 Experiments categories: 1)[Linguistics]addingXLM-Rvectors totheoriginalsetoflinguisticsfeaturesandtrain CEFRlanguageproﬁciencylevelscanbepresented with 1058 features [1024 XLM-R dimensions + as labels or as a continuous scale. The former is 34linguisticsfeatures];2)[Neural]representsthe solvedasaclassiﬁcationtaskwithmacro-averaged sentenceonlybysentenceembeddingswithneural F-1asthemainmeasureforaccuracy. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Confusion Matrix of SVM (rbf) on 3-way removingsomefeaturesandtestingtheremainfea- classiﬁcationwithXLM-R. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Theresultsarelisted knowledge. in Table 6. As with classiﬁcation, error analysis leadstoimprovedresultsacrossallmethods. </Extractive Summary>  </Table ID = 6>  <Table ID = 8>  <Abstractive Summary> =  Table 8: SVM Classiﬁcation ablation experiment on 3-wayclassiﬁcation withoutsimpliﬁcationalignedwiththeexactsen- tencewithoutmodiﬁcationandlabeledthemwith 0 indicating not paraphrased/simpliﬁed. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The two models trained on this similarity XML-R respectively. However, when testing the task (AraBert and Arabic-Bert) achieve the F-1 binary classiﬁers trained from DataSet One on measure of 0.98, leading to the ability to detect DatasetTwotheaccuracydropsconsiderably,see sentenceswhichneedsimpliﬁcationaccordingto Table 9. theDatasetTwostandard. </Extractive Summary>  </Table ID = 9>  </Paper ID = 688> 

<Paper ID = 689>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ANERcorp is baseduponweb-documentswritteninmodernAra- bic collected from 316 articles from newspapers such as bbc1 and aljazeera2. Table 2 compares thecorpora’stokencountsandclassdistributions. The datasets are tagged into four classes, Person, Organization, Location, and Other. </Extractive Summary>  </Table ID = 2>  </Paper ID = 689> 

<Paper ID = 690>  </Paper ID = 690> 

<Paper ID = 691>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ThisisalsoachievedbySVMwith characterandwordn-gramvectorsasfeatures. In Table 2 and Table 3, we present results of dif- ferentmodelsonvariantsofinformationavailable. • When a single tweet is added to username Wereportprecision(P),recall(R),andF1forthe anduserdescription,themaximummF1score Adultclassonthetestset. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ThisisalsoachievedbySVMwith characterandwordn-gramvectorsasfeatures. In Table 2 and Table 3, we present results of dif- ferentmodelsonvariantsofinformationavailable. • When a single tweet is added to username Wereportprecision(P),recall(R),andF1forthe anduserdescription,themaximummF1score Adultclassonthetestset. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ages) and compare performance with our model thatdependsonlyontextualinformation. 5 ErrorAnalysis Theconfusionmatrixofpredictionsbyourbestsys- References tem,AraBERTtrainedonuserinformation+tweet, is shown in Table 4. We manually analyzed all Ahmed Abdelali, Hamdy Mubarak, Younes Samih, Sabit Hassan, and Kareem Darwish. </Extractive Summary>  </Table ID = 4>  </Paper ID = 691> 

<Paper ID = 692>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We observedthatgeopyhasdifﬁcultiesinidentifying 5 DataAnalysis manylocationswhentheyareshort,havespecial characters,orunreallocationsinadditiontomany Byassigningcountriestouniqueuserlocations,we correct locations. Table 1 shows some examples could map locations of ≈ 90K users to countries fortheseerrors. which represent 56% of the 160K users in our dataset. </Extractive Summary>  </Table ID = 1>  </Paper ID = 692> 

<Paper ID = 693>  <Table ID = 1>  <Abstractive Summary> =  Table 10: Results of the MTL experiment using with a small improvement for the (Frenda et al., BERTmodel. </Abstractive Summary>  <Extractive Summary> =  We also dead,hurt,etc.Giventhedifﬁcultyassociatedwith ﬁlteredoutthenon-textual,Arabic-Arabizimixed therecognitionofmisogynisticbehaviors(Schmidt tweets,retweetsandduplicatedinstances.Inaddi- and Wiegand, 2017), they were clearly deﬁned tion,basedonregularexpressions,wespottedmany throughourdesignedannotationguidelines.This tweetswhosecontentrepresentsasinglehashtagor enabled the annotators to have a uniﬁed perspec- asequenceofhashtags.Weoptedtoremovethese tive about misogynistic language categories and tweetsastheywerenon-informativeandwerewrit- contributedtoimprovingtheinter-annotatoragree- tenjusttomakeahashtagtrending.Moreover,to mentscores.Basedonthedeﬁnitionofmisogynis- assurethatthecollectedrepliesarewrittentotar- tic behaviors in (Poland, 2016), we designed the getthejournalistherselfandnotapartofsidede- annotationguidelinesforLet-Midatasetsuchthat batesamongtheuserswithinathread,weremoved theeightlabelcategoriesareidentiﬁedasfollows: tweets that mention accounts other than the jour- nalist’s.Thus,weendedupwith6,603directtweet • Non-Misogynistic (none): tweets are those replies. Table 1 lists the journalist names4, their instances that do not express any hatred, in- newsagencies,andthenumberoftweetrepliescol- sultingorverbalabusetowardswomen. lectedfromthetimelineofeach.Inordertoprepare thecollectedtweetsforannotation,wenormalized • Discredit refers to tweets that combine slur- thembyeliminatingTwitter-inheritedsymbols,dig- ring over women with no other larger inten- its,andURLs.Itshouldbementionedthatasthe tion. </Extractive Summary>  </Table ID = 1>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Distribution of top 5 frequent terms in tionstatisticsispresentedinTable3. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Aimingtoinvestigatethedisagreementamongthe None 3,388 28,610 12,763 annotators, we explored the tweets for which the Discredit 2,327 16,587 6,817 annotatorsgavedifferentjudgments.Asampleof Stereotyping&objectiﬁcation 290 2,235 1,426 thetweetsalongwiththeannotationsassignedby Damning 256 1,479 868 theannotatorsislistedinTable6whereFandM Threatofviolence 175 1,356 971 Derailing 59 497 391 denotefemaleandmale,respectively. Dominance 38 292 228 As seen in Table 6, the disagreements spotted Sexualharassment 17 94 87 amongtheannotatorscanbejustiﬁedbythegender Table4: TweetsdistributionacrossLet-Miclasses. oftheannotatorwherebothfemaleannotatorshad thesamejudgmentfortweetsthatdescribewomen- To identify the words commonly used within relatedissues(e.g. </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Results of the misogyny identiﬁcation theBERTmodelinourexperiment.InFigure1we task. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Results of the categories classiﬁcation Task Acc. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  </Paper ID = 693> 

<Paper ID = 694>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Samples of utterances and empathetic responses from the ArabicEmpatheticDialogues dataset for three emotionlabels: Excited,Furious,andEmbarrassed fsouucnhdamsobreeaemffescetaivrceht,hawnhcicohnvteenntdiosntaolaypieplrdoaccohmes- P(cid:233)(cid:16)(cid:131)r(cid:89)e(cid:74)-(cid:9)(cid:234)S(cid:203)(cid:64)e(cid:233)g(cid:16)(cid:10)(cid:74)m(cid:202)(cid:191)e(cid:225)(cid:9)n(cid:211)ta(cid:241)(cid:16)(cid:74)t(cid:202)i(cid:203)o(cid:73)n(cid:16) (cid:107)(cid:46)(cid:81)(cid:109)(cid:9)(cid:26)(cid:16)(cid:39) (cid:89)(cid:174)(cid:16)(cid:203) (cid:250)(cid:10)(cid:230)(cid:16)(cid:74)(cid:9)(cid:75)(cid:46)(cid:65)(cid:13)(cid:75)(cid:46) (cid:64)(cid:89)(cid:103)(cid:46) (cid:80)(cid:241)(cid:109)(cid:9)(cid:175)(cid:9) (cid:65)(cid:75)(cid:9)(cid:13)(cid:64) morosnimreislpaor,nsselisgfhotulyn-dvarerypientigtivveelrysiionntsheotfrathineinsgamseet Pos(cid:72)t(cid:16)-S(cid:43)eg(cid:104)(cid:46)m(cid:81)(cid:109)(cid:9)(cid:26)e(cid:16)(cid:39)n(cid:89)t(cid:174)(cid:16)a(cid:203)ti(cid:248)(cid:10)o(cid:43)n(cid:72)(cid:16)(cid:43) (cid:225)(cid:9)(cid:75)(cid:46)(cid:13)(cid:64) (cid:43)(cid:72)(cid:46) (cid:64)(cid:43)(cid:89)(cid:103)(cid:46) (cid:80)(cid:241)(cid:109)(cid:9)(cid:175)(cid:9) (cid:65)(cid:75)(cid:9)(cid:13)(cid:64) high-likelihoodsequences(Ippolitoetal.,2019). </Abstractive Summary>  <Extractive Summary> =  Eachsample Anticipating Caring isalsolabeledwiththeemotionofthespeaker’sut- Sentimental terance. Threeexamplesfromthedatasetforthree Love Trusting different emotion labels are provided in Table 1. Faithful Nostalgic By training a sequence generation model on the Surprised samplesofutterancesandtheircorrespondingre- Surprise Impressed sponsesfromthedataset,themodelwillbeableto Sad Lonely infertheemotionsininpututterancesandprovide Guilty suitable empathetic responses. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Giventhe 167morphologicalcomplexityoftheArabiclanguage, Model PPL BLEU segmentationisanimportantpre-processingstep Baseline(Naousetal.,2020) 38.6 0.5 thatcangreatlyenhancetheperformanceofneural- EmoPrepend 24.1 3.16 based sequence generation models. An example BERT2BERT-UN 159.8 0.1 of this process is shown in Table 3. By perform- BERT2BERT 17.0 5.58 ingsegmentation,thevocabularysizeisdrastically reducedfrom47Ktokenstoaround13Ktokens. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Examples of responses generated by the BERT2BERT model for multiple utterances with various emo- tionalstatesanddomaincontexts. </Abstractive Summary>  <Extractive Summary> =  UnliketheEnglishlan- aremuchsuperiortoboththeBaselineandtheEmo- guagewhichhasseengreatadvancementsinlan- Prependmodels,whichindicatesBERT2BERT’s guagegenerationmodelsduetolargecorporaand abilitytodeliverhighlyempatheticresponseswhile million parameter pre-trained models like GPT, abidingbylinguisticcorrectness. Thisisreﬂected Arabicisconsideredalow-resourcelanguagewith in some examples of the generated responses by limitedavailabilityofconversationaldatasetsand BERT2BERT that can be seen in Table 6. The pre-trainedmodelsforresponsegeneration. </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Examples of responses generated by the BERT2BERT model for multiple utterances with neutral emo- tions. </Abstractive Summary>  <Extractive Summary> =  Theraters BERT2BERTmodelingeneratingrelevantempa- wereaskedtoanswerthefollowingquestions: thetic responses in open-domain settings, it was showntopoorlyhandleregularchit-chatutterances • Empathy: Doesthegeneratedresponseshow withneutralemotions,suchas”Hey,howareyou?” an ability to infer the emotions in the given or”Whatareyoudoing?”. Insteadofprovidinga utterance? regularresponse,theBERT2BERTmodelwillopt • Relevance: Howrelevantisthegeneratedre- togenerateanempatheticresponseasweshowin sponsetotheinpututterance? Table 7. This issue can be explained by the fact that the model was ﬁne-tuned on a dataset com- • Fluency: Howunderstandableisthegenerated prised of utterances with pure emotional context response? Isitlinguisticallycorrect? andcorrespondingempatheticresponses. </Extractive Summary>  </Table ID = 7>  </Paper ID = 694> 

<Paper ID = 695>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Evaluation scores for our baseline models on the various ALUE tasks, with Pearson Correlation and JaccardIndexscoresforSVREGandSECtasksrespectively,MatthewsCorrelationCoefﬁcientfortheDiagnostic dataset(DIAG),AccuracyforXNLI,andF1-scorefortherest. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  First,wecomputea ondialecticaldata. correlationscorebetweenboth,privateandpublic results as displayed in Table 3. As expected, all 6.2 DiagnosticResultsAnalysis ofthesescoresarepositivelycorrelatedbutsome We report the performance of our models on the datasetsaremoresothanothers. </Extractive Summary>  </Table ID = 3>  </Paper ID = 695> 

<Paper ID = 696>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Duplicated pairs of verses with same label, the Holy Quran (Sharaf and Atwell, 2012). </Abstractive Summary>  <Extractive Summary> =  In pairswerelabelledthesamebutordereddifferently. NamedEntityRecognition(NER),AraBERTv01 Forexample,chapter2verse2ispairedwithchap- hadbetterresultsoverBi-LSTM-CRFmodelwith ter 2 verse 3 and the pair is labelled ‘2’ and vice macro-F1 score of 84.2, in which AraBERT new versa, shown in Table 1. For this case, we opted state of the art for NER on ANERcorp (An- to remove the duplicates since they were redun- toun et al., 2020). </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Forexample,chap- TermFrequency-InverseDocumentFrequency(TF- ter 1 verse 5 paired with chapter 73 verse 9 were IDF). QurSim dataset classiﬁes pairs of Quranic labelled‘2’;however,theywerelabelled‘1’when 186ordered differently, shown in Table 2. Since the Also, this is a preliminary research to use the labelassignmentsweremainlyinﬂuencedbyIbn AraBERT model for semantic similarity and the Kathir’s Tafsir, it is difﬁcult for the authors to in- authorschosetheextremesimilaritylabel,whichis terpretIbnKathirs’scommentsinordertoassign label’2’,fortrainingandtesting. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Results of both versions of AraBERT with tionship will there be among them that threedatasets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 696> 

<Paper ID = 697>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Performance of all tested model on the various Arabic downstream tasks. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 697> 

<Paper ID = 698>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  De- All models were trained on a TPUv3-128 slice9 tails on the datasets, prompts, and evaluation are with different batch sizes and the total number presentedinAppendixB. of steps as shown in Table 2. Base and mega were trained for approximately 20 epochs, while 4.4 EvaluatingtheHumanAbilitytoDetect medium and large were trained for 10 and 6 Machine-GeneratedText epochsrespectively,duetoTPUaccesslimitations. </Extractive Summary>  </Table ID = 2>  </Paper ID = 698> 

<Paper ID = 699>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Speed of CAMeL Tools vs QuranTree.jl in and Evaluation, LREC 2010, 17-23 May 2010, Val- milliseconds(ms). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 699> 

<Paper ID = 700>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  withOOV(outofvocabulary),webackofftothe MorphTrans technique (Model MLE Morph) or Special Cases The Arabic ALA-LC guidelines (cid:225)(cid:9)(cid:75)(cid:46) CharTrans Technique (Model MLE Simple). In includeanumberofspecialcases,e.g.,theword Table 2, we also study the performance of MLE bn‘sonof’isRomanizedasibn,andpropernoun (cid:240)(cid:81)(cid:212)(cid:171) Simplewithdifferentcorpussizes. ςmrwisRomanizedas‘Amr. The Seq2Seq performance is compar- 6 ExperimentalResults atively much poorer with less data. With 31K words, MLE Simple’s performance is 10 times Table 2 presents the Dev and Test results for the better than Seq2Seq; and their performance only models discussed in the previous section. All re- becomescomparablewith250Kwords. </Extractive Summary>  </Table ID = 2>  </Paper ID = 700> 

<Paper ID = 701>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In total, there are 49280 query- EntityRetrieval SincetheintroductionofDEv2, entity pairs. Table 1 details the four different severalworkshavereportedsigniﬁcantprogresson groups the queries are categorized into. The set entityretrieval. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Normalized Discounted Cumulative Gain. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 701> 

<Paper ID = 702>  </Paper ID = 702> 

<Paper ID = 703>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  collected claims. Table 1 gives an overall sum- Additionally,weextractedthecontentofthefact- mary of the collected claims and some of the ex- 233Figure4: Claimsdistributionovertime. Figure 3: Distribution of normalized labels and cate- gories. </Extractive Summary>  </Table ID = 1>  </Paper ID = 703> 

<Paper ID = 704>  </Paper ID = 704> 

<Paper ID = 705>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (Subtask 2.2). We provide examples DA tweets fromanumberofcountriesrepresentingdifferent regions in Table 3. For each example in Table 3, welisttheprovinceitcomesfrom. We provide examples DA tweets fromanumberofcountriesrepresentingdifferent regions in Table 3. For each example in Table 3, welisttheprovinceitcomesfrom. Similarly,we provideexampleMSAdatainTable4. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: List of teams that participated in one or more of the four subtasks and submitted a system description paper. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Results for Subtask 1.1 (country-level MSA). </Abstractive Summary>  <Extractive Summary> =  5.3 SharedTaskResults thebestF scorethatis32.26%. Table8showsthe 1 bestTESTresultsforallfourteamswhosubmitted Table 6 presents the best TEST results for all systemsforSubtask2.1. CairoSquadachievedthe 5 teams who submitted systems for Subtask bestperformancewith6.43%F score. </Extractive Summary>  </Table ID = 6>  <Table ID = 1>  <Abstractive Summary> =  Table 11: Distribution of the NADI 2021 MSA data over provinces, by country, across our TRAIN, DEV, and TESTsplits(Subtask2.1). </Abstractive Summary>  <Extractive Summary> =  mancewith8.60%.7 and CS-UM6P developed their system utilizing MARBERT(Abdul-Mageedetal.,2020a),apre- 5.4 GeneralDescriptionofSubmitted trained Transformer language model tailored to Systems Arabic dialects and the domain of social media. In Table 10, we provide a high-level descrip- TeamPhonemerutilizedAraBERT(Antounetal., tion of the systems submitted to each subtask. 2020a)andAraELECTRA(Antounetal.,2020b). </Extractive Summary>  </Table ID = 1>  </Paper ID = 705> 

<Paper ID = 706>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Sequence length (ﬁtted to an Erlang distribu- toemployadapterfusion(Pfeifferetal.,2021)to tion) attenuatethisbias. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 706> 

<Paper ID = 707>  </Paper ID = 707> 

<Paper ID = 708>  <Table ID = 1>  <Abstractive Summary> =  Table 1: NADI’2021 DA and MSA identiﬁcation sub- model for NADI’2021 uses MARBERT. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 708> 

<Paper ID = 709>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Results of the development data for Experi- tecturecombiningwordembeddingswithembeddings ment 1 & 2. </Abstractive Summary>  <Extractive Summary> =  In order to fromExperiment1whichdoesnotincorporateany improveperformancethroughfeatureengineering, engineeredlinguisticfeatures. thecontentofthedatahastobecharacterizedby Our results, shown in Table 2, emphasize the more local genre. Sociologists have shown that mainﬁndingsofthisarticle: linguisticfeatures(at participantsofdifferentlinguisticcommunitiesin 278No Label Tweet Gloss Type 1234.... </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  Table 1: A small sample of the engineered linguistic features for Egyptian, Iraqi, Saudi, Moroccan dialects low. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: A sample of the tweets from the labeled data subtask 1.2. </Abstractive Summary>  <Extractive Summary> =  Inthesamerespect,Bouamor Locationgranularity Anumberofthefeatures etal.(2019)presentedalargeparallelcorpusof25 included in this work are regional, rather than Arabiccitydialectsinthetraveldomainandalexi- unique to a single dialect. An example of such conof1,045conceptswithanaverageof45words a situation is example 8 in Table 3 in which an from25citiesperconcept. Whiletheapproachpre- Iraqiparticipantsharesthesamelinguisticfeatures (cid:88)(cid:65)(cid:171) sentedheredoesnotmakeuseoftheseresources, “ ”thatarefoundintheSaudidialect. </Extractive Summary>  </Table ID = 3>  </Paper ID = 709> 

<Paper ID = 710>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  282 ProceedingsoftheSixthArabicNaturalLanguageProcessingWorkshop,pages282–286 Kyiv,Ukraine(Virtual),April19,2021.MSAsubset DAsubset Train Dev Test Total Train Dev Test Total #sentences 21,000 5,000 5,000 31,000 21,000 5,000 5,000 31,000 #words 233.4k 65.4k 63.1k 408.2k 168.3k 45.7k 44.7k 258.7k Max#wordpersentence 49 58 57 - 51 58 59 - Min#wordpersentence 1 1 1 - 1 1 1 - Max#charpersentence 592 382 285 - 268 279 283 - Min#charpersentence 1 2 1 - 1 2 2 - Table1: Datasetstatisticsafterapplyingsomepre-processingsteps Wedividedthesesubsetsintothreeparts: train,dev 4 Proposedsystem andtest,forwhichweaddressedsomestatisticsin Table1,afterapplyinganumberofpre-processing ItshouldbenotedfromFigure1,thatweﬁrstcom- steps. As can be noticed from Table 1, the mini- binedalltheaforementionedpreprocessingstepsto mumnumberofwordsandcharacterspersentence generateallthepossibletextualpresentations,af- inthetraininganddevelopmentsetsis1. Thisin- terthatweappliedasecondcombinationbetween formation is useful to determine the number “n” n-grams (n=1 to 10) and tokenizer (word, char, of grams to be selected in the features extraction char with boundary, union of the three) to gener- phase. </Extractive Summary>  </Table ID = 1>  </Paper ID = 710> 

<Paper ID = 711>  </Paper ID = 711> 

<Paper ID = 712>  </Paper ID = 712> 

<Paper ID = 713>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thesarcasmdetectiontask(subtask1)re- Thesharedtaskonsarcasmdetectionandsentiment ceived27submissions,whilethesentimentanaly- analysisinArabiccontainstwosubtasksasfollows: sistask(subtask2)received22submissions. Table 3 shows the list of the participating teams whose • Sarcasm Detection (subtask 1): the goal is paperswereaccepted3. toidentifywhetheratweetissarcasticornot. </Extractive Summary>  </Table ID = 3>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  in descending order based on the ofﬁcial metric The data for both subtasks was provided as ofthecorrespondingsubtask,whereF1-sarcastic train/testsplitwithoutaspeciﬁcdevelopmentset. andFPN aretheofﬁcialmetricsforsubtask1and 1 Table 1 shows the statistics of the two sets. The subtask 2 respectively. </Extractive Summary>  </Table ID = 1>  </Paper ID = 713> 

<Paper ID = 714>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The dataset provided inthetrainingphasecontains12548trainingexam- 4.2.1 Multi-headed-LSTM-CNN-GRUModel ples, while the test set provided in the test phase Inthismodel,Arabicnewsembedding(Altowayan consistsof3000examples. andTao,2016)wasusedastextrepresentationin Table 1 shows the statistics of ArSarcasm-v2 additiontoemoji2vec(Eisneretal.,2016)foremoji dataset. Mostofthedialectexamplespresentedin informationpresentedintheexamples. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Results on our initial development set in shared task one and two, based on our splitting criteria when evaluatingmodels Model F1-PN Accuracy Macro-F1 Precision Recall MARBERT 0.7321 0.6957 0.6587 0.6498 0.6748 Table3: LeaderboardresultsonProvidedtestsetforsharedtasktwo Model F1-Sarcastic Accuracy Macro-F1 Precision Recall MARBERT 0.5662 0.7803 0.7095 0.7231 0.7004 Multi-headed-LSTM-CNN-GRUmodel 0.1657 0.7047 0.4932 0.5497 0.5185 Table4: Leaderboardresultsonprovidedtestsetforsharedtaskone were inputted to the network. </Abstractive Summary>  <Extractive Summary> =  themtogether. 6 ResultsandDiscussion 5 ExperimentalSetup 6.1 Developmentphaseresults ExperimentswereconductedviaPythonandTen- Table 2 shows that Multi-headed-LSTM-CNN- sorﬂow framework, running on Google Colab re- GRUoutperformsMARBERTbasedonoursplit- sources,whichareNvidiaTeslaP100-PCIE-16GB tingcriteriainsharedtaskone,whileMARBERT GPU,Intel®Xeon®CPU@2.20GHz,and12GB outperformsinsharedtasktwo. Webelievethatif RAM.Weused70%-10%-20%strategyfortrain- ensemblelearningweretobeused,itwillperform validation-test splits respectively for the training betterasitwillincorporateallinformationneeded phase. </Extractive Summary>  </Table ID = 2>  </Paper ID = 714> 

<Paper ID = 715>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Label distributions for both tasks. </Abstractive Summary>  <Extractive Summary> =  For The past few years have witnessed a huge the second shared task (Subtask 2) on sentiment revolution in building various bidirectional analysis the labels are (NEU, NEG or POS) for transformer-based models, particularly for neutral, negative or positive, respectively. Table 1 Arabic. Where they perform as powerful transfer illustrates the label distributions for both tasks. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Results on original dataset for subtask 1. </Abstractive Summary>  <Extractive Summary> =  Both metrics were specified by the competition organizers. 4.1.1 Subtask 1: Table 2 shows the results on the validation set in addition to the time the models took to train. MARBERT outperforms the other models with 0.647 F1-score on sarcastic class followed by QARIB with 0.597 F1-score. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Results with data augmentation on subtask 1. </Abstractive Summary>  <Extractive Summary> =  While mBERT gives the lowest F1- score of 0.411. The results of using MARBERT and QARiB with data augmentation are shown in Table 3. Data augmentation improves Figure 1: BERT- based model with data augmentation results by about 15% for both MARBERT and for sarcasm detection. </Extractive Summary>  </Table ID = 3>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Examples of mis-clasffied tweets for sarcasm tweets and try to find the reasons behind this mis- class. </Abstractive Summary>  <Extractive Summary> =  314 Model F1- sarcastic Training time mis-classified examples. Table 7 lists some mis- (valid set) (min: sec) classified tweets by our best performing model on MARBERT 0.80 18:27 sarcasm detection task. We found that there are QARIB 0.75 18:00 several reasons for classifying sarcastic tweets as Table 3: Results with data augmentation on subtask 1. </Extractive Summary>  </Table ID = 7>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Results on original dataset for subtask 2. </Abstractive Summary>  <Extractive Summary> =  not sarcastic and vice versa. We summarise these reasons as follows: 4.1.2 Subtask 2:  Human annotation is not 100% correct Similarly, the results of subtask 2 are shown in because annotators’ cultures and Table 4, QARiB achieved slightly higher F-PN backgrounds diversity might not be score than MARBERT however, it has higher considered in the annotation process. For overfitting than MARBERT. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Results with data augmentation on subtask 2. </Abstractive Summary>  <Extractive Summary> =  should be annotated as FALSE/ not sarcastic Expanding dataset size improves the performance and TRUE respectively. of MARBERT by 15% as shown in Table 5 which  The absence of context: in some tweets the also shows the effectiveness of our data context is missed and it was not possible for augmentation approach. our model to understand the context and predict the label correctly. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Results of submitted model (MARBERT) for 7 ناربج" ديهشلل ةتفل اهيف ةقلح" FALSE TRUE both tasks. </Abstractive Summary>  <Extractive Summary> =  Tweet True Predicted label label 1 " كلاب دخاو ةيدوعسلا ايناطيرب TRUE FALSE 4.2 Official Results: تنأ😁😁😁" Based on the results above for both tasks we 2 لافطأ كراعملا_رابخأ_ةكبش#" TRUE FALSE submitted the results of MARBERT on the test ةيسورلا تاراغلا ةجيتن اوبيصا set. Table 6 presents the results of the MARBERT مويلا حابص بلح# ةنيدم ىلع on the test set as reported by the competition ليكولاو معنو الله انبسح https://t.co/JHhjktp7qu" organizers, compared to the results on the 3 ف هيجراخلا ءارزول عامتجا " TRUE FALSE validation set. Obviously, there is a significant اهرقم يللا هيبرعلا ةعماجلا decrease in the model performance on the test set نيياج شم لود لك .ةرهاقلا for both tasks, this is likely because of the لودلا ةعماجل نيياج لود رصمل overfitting issue. </Extractive Summary>  </Table ID = 6>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Similarly, we examined a random sample of mis- ASAD: A Twitter-based Benchmark Arabic classified tweets for subtask 2 and investigated Sentiment Analysis Dataset. arXiv preprint them as shown in Table 8. We found some reasons arXiv:2011.00578. </Extractive Summary>  </Table ID = 8>  </Paper ID = 715> 

<Paper ID = 716>  <Table ID = 1>  <Abstractive Summary> =  Table 1: The static word embeddings utilised for our totrainAra2Vec,itisnotpossibleforsuchaword- systems. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  FortheSAsubtask wastheF-scoreofthesarcasticclass(F1-sarcastic) (2), the model achieved a macro F1-PN of 0.701 andthemacroaverageoftheF-scoreofthepositive on the test set. Table 5 presents the ofﬁcial re- andnegativeclasses(F-PN)forsubtask2. sultsachievedbyourproposedmodel(MTL-CNN- Table 4 presents the models’ performance us- LSTM)onthetestsetforSubtasks1and2. </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Table 5 presents the ofﬁcial re- andnegativeclasses(F-PN)forsubtask2. sultsachievedbyourproposedmodel(MTL-CNN- Table 4 presents the models’ performance us- LSTM)onthetestsetforSubtasks1and2. ing the average of cross-validation for both sub- tasks. </Extractive Summary>  </Table ID = 4>  </Paper ID = 716> 

<Paper ID = 717>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  For decoding, the emoji in each dataset (Abu Farha et al., 2021). Table 1 shows tweetisreplacedwithitsequivalenttextualdescrip- some statistics of the released training set. The tioninArabic. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Weusedacontrastiveloss. Thus,iftwotweets As shown in Table 2, BOW yields an F1-score areexpressingsimilarmeanings,theirembedding of%45.88comparedto%49.66toTF-IDF.These vectorsaremappedtobeclosedtoeachotherwhile results indicate that the traditional word encod- maximizingthedistancebetweentheembedding ingsmethodsareinsufﬁcienttomodelthecomplex oftweetswithdifferentmeanings. Figure2shows meaningsinsarcastictweets. Next, we compared the results of the different F1-score is the ofﬁcial metric in this task, which BERT-based models on the validation set. As combinestherecall(TP/TP+FN)andtheprecision shown in Table 2, the results indicate that using (TP/TP+FP)metrics. TP,FN,andFPreferrespec- BERT-basedmodelscanremarkablyincreasethe 326F1-sarcastic Accuracy Macro-F1 Precision Recall Full-model 59.89 78.30 72.51 72.68 72.35 Table3: OfﬁcialResultsAchievedbytheFull-ModelontheTestSetfortheSarcasmDetectiontask. </Extractive Summary>  </Table ID = 2>  </Paper ID = 717> 

<Paper ID = 718>  </Paper ID = 718> 

<Paper ID = 719>  </Paper ID = 719> 

<Paper ID = 720>  <Table ID = 1>  <Abstractive Summary> =  Table 1: This table shows us part of random for doing something very close to the stemming forest hyperparameters 342 tasks) with its default hyperparameters config- of it duplicated tweets from the training set and this uration using scikit-learn package in python see is why these fake results are here i(.e,. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 720> 

<Paper ID = 721>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: The total size of the training and testing R,havedefaultlearning rate=6e,butwechanged dataset,andanindicationofthenumberifit’ssarcastic itto1e-5,whichincreasedtheresults. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 721> 

<Paper ID = 722>  </Paper ID = 722> 

<Paper ID = 723>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ThesuccessoftheEnglishword2vec(Mikolov Theorganizersprovideaseparatedatasetfortest- etal.,2013)andfast-Text(Bojanowskietal.,2016) ing,consistingof3,000tweets. Table 1andTable motivatedotherworkstoachievethesamefeatby 2provideadescriptiveanalysisoftheﬁnaltraining, creatinglanguage-speciﬁcwordembeddings. For validation and test sets for the tasks of sentiment ArabicNLP,someearlyattemptsincludeword2vec- identiﬁcationandsarcasmdetectionrespectively. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  To replace emo- toensureafaircomparison. jisandemoticonswiththeircorrespondingArabic Table 3 shows the results for the sarcasm translations,wecreatedacustomdictionarymap- detection subtask. The proposed model shows a ping. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ﬁgurativemeaningoftweetsandidentifyimplicit negative sentiments. Table 4 lists the baseline 5.2 ParametersandTrainingEnvironment models’ performances and the proposed system For the CNN-BiLSTM ensemble part of the pro- forsentimentidentiﬁcationsubtask. Itisobserved posed system, we employ a CNN layer with 256 that both the proposed and AraBERT baseline ﬁltersandreluactivation. </Extractive Summary>  </Table ID = 4>  </Paper ID = 723> 

<Paper ID = 724>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Ofﬁcial Shared Task Results for the Testing ofdialecticalArabicinwordembeddings. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Ofﬁcial Shared Task Results for the Testing opmentdataset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 724> 

<Paper ID = 725>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The test dataset contains 3K tweets for which Since most transformer models are trained using only the dialect value is available. Dataset statis- ‘Mask-LM’(MLM)and‘NextSentencePrediction’ tics are provided in Table 1. As observed in the (NSP)(Devlinetal.,2018)theyarecapableofaug- table, train and train distribute differ- menting new sentences out of existing ones. We do the join together the the Levantine and Maghrebi di- following adjustments for the particular task we alectsduetotheirrelativesmallsupport. Following face: (i) Since data are imbalanced (see Table 1), this way, four distinct classiﬁers are trained. We were-weighttheinstancesaccordingtotheinverse setthemaximumepochsizetoﬁve,batchsizeto proportion of their class weight. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Sentiment classiﬁcation results. </Abstractive Summary>  <Extractive Summary> =  whiletraining,asithasbeenshowntoworkwell forimbalanceddatasets. (iii)WesettheF1score 4 Results over the positive and negative classes (F1PN) as theevaluationcriteriaoftheclassiﬁer(testedover Theresultsofthesentimentclassiﬁcationandthe the eval dataset) as this is the target evaluation sarcasm detection tasks are reported in Table 3 372Table 2: Augmentation example. Replacement and addition of tokens is possible. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Sarcasm detection results. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Mageed et al., 2020). In Table 5 we present the confusionmatrixofthebestmodelwetrainedfor thesentimentclassiﬁcationproblem. Asobserved, the distinction between ‘Negative’ and ‘Neutral’ Theresultsofthesentimentclassiﬁcationandthe instancesisthevulnerablepointofthemodel. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Asobserved, the distinction between ‘Negative’ and ‘Neutral’ Theresultsofthesentimentclassiﬁcationandthe instancesisthevulnerablepointofthemodel. sarcasm detection tasks over the test dataset are reported in Table 6. We were ranked in the 5’th 4.1 OfﬁcialSubmissionResults place (out of 22 participation teams) in the senti- ment classiﬁcation task and 9’th place (out of 27 As explained in the Section 2, the test dataset participationteams)inthesarcasmdetectiontask. </Extractive Summary>  </Table ID = 6>  </Paper ID = 725> 

<Paper ID = 726>  </Paper ID = 726> 

<Paper ID = 727>  <Table ID = 1>  <Abstractive Summary> =  Table 11: Ranking and results on the Sentiment • Sarcasm Detection: F1-sarcastic equal to AnalysisTestset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: The dataset description for Subtask 2 - outof110k). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: The best results using MARBERT for purposemodels. </Abstractive Summary>  <Extractive Summary> =  Hence, the vocabulary size is equal 4.1 Subtask1-SarcasmDetection to 100k WordPieces. MARBERT enhances the Thebestresultsthatweobtainedforsarcasmdetec- languagevarietyasitfocusesonrepresentingthe tion on the sarcastic and non-sarcastic class are previously underrepresented dialects and Arabic shown in Table 3. The accuracy achieved was variants. </Extractive Summary>  </Table ID = 3>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Team Rank F-PN CS-UM6P 1 0,7480 4.3 Ofﬁcialsubmissionresults DeepBlueAI 2 0,7392 Theﬁnalresultsthatweachievedonthetestsetof rematchka 3 0,7321 the2021SharedTaskonSarcasmandSentiment iCompass 8 0,7085 DetectioninArabicwere: Table 11: Ranking and results on the Sentiment • Sarcasm Detection: F1-sarcastic equal to AnalysisTestset. 48.6% • SentimentAnalysis: F-PNequalto70.85% Team Rank F1-sarcastic BhamNLP 1 0,6225 5 Discussion SPPU-AASM 2 0,6140 DeepBlueAI 3 0,6127 Table 9 shows the results obtained over develop- iCompass 21 0,4860 ment data for the Sarcasm Detection task (True beingsarcasticandFalsebeingnon-sarcastic)and Table12: RankingandresultsontheSarcasmDe- Table10showstheresultsobtainedoverdevelop- tectionTestset. ment data for the Sentiment Analysis task. </Extractive Summary>  </Table ID = 9>  </Paper ID = 727> 

<Paper ID = 728>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  test set and the output of both tasks have been Theperformancecanbeimprovedusingdifferent submitted to the shared task. Table 3 shows the weightingscoressuchaswordembeddings. Deep detailedresultsoftheproposedmodelforbothsub- learning based models also can be used for the tasks. </Extractive Summary>  </Table ID = 3>  </Paper ID = 728> 

<Paper ID = 729>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ArSarcasm-v2 contains 12,548 training sam- Theoutputofdenselayerxisdepictedasbelow, ples with annotations and 3,000 testing samples without annotations. Table 1 shows statistics of x = ReLU(W0dropout(x[CLS])) (1) trainingset,wherewecanﬁndthat17.28%ofthe dataissarcastic(2,168tweets). Mostofthedatais eitherinMSAortheEgyptiandialect,whilethere x[CLS] Dense MuDltrio-Spaomutple arefewexamplesoftheMaghrebidialect. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Inordertofurtherimprovetheperformanceofour model, we adopt two training strategies and are PLMs with Training Strategies As shown in introducedbelow. Table 2, for both subtask 1 and 2, we use two kindsofPLMswhichareXLM-RandAraBERT. Task-AdaptivePre-training Task-adaptivepre- Theresultsaretheaveragescoresof7-foldcross- training(TAPT)caneffectivelyimprovemodelper- validationonthetrainingdataset. </Extractive Summary>  </Table ID = 2>  </Paper ID = 729> 

<Paper ID = 730>  </Paper ID = 730> 

<Paper ID = 731>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Accuracy (%) on the 20 QA tasks for models using 1k and 10k training examples. </Abstractive Summary>  <Extractive Summary> =  4.3 Results generates a further boost in performance consis- tentlyonboththe1kand10kdatasets. Performance results on the 20 bAbI QA dataset are presented in Table 1. For comparison pur- poses,westillpresentMemN2N(Sukhbaataretal., State-of-the-art performance on both the 2015) in Table 1 but accompany it with the accu- 1k and 10k dataset. Performance results on the 20 bAbI QA dataset are presented in Table 1. For comparison pur- poses,westillpresentMemN2N(Sukhbaataretal., State-of-the-art performance on both the 2015) in Table 1 but accompany it with the accu- 1k and 10k dataset. The best performing racy obtained by our implementation of the same GMemN2N model achieves state-of-the-art perfor- modelwiththesameexperimentalsetuponv1.2of mance, an average accuracy of 87.3 on the 1k thedatasetinthecolumn“OurMemN2N”forboth dataset and 96.3 on the 10k variant. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Per-response accuracy and per-dialog accuracy (in parentheses) on the Dialog bAbI tasks. </Abstractive Summary>  <Extractive Summary> =  (OOV) test sets, are constructed separately. In addition, a supplementary dataset, task 6, is pro- 5.3 Results vided with real human-bot conversations, also in Performance results on the Dialog bAbI the restaurant domain, which is derived from the dataset are shown in Table 2, measured using secondDialogStateTrackingChallenge(Hender- both per-response accuracy and per-dialog accu- son et al., 2014). It is important to notice that the racy (given in parentheses). </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  transformgatecellvalues, Tk(uk) /d N/A N/A N/A 0.22 0.23 0.45 i i Question: Whogavethefootball? Answer: Fred,MemN2N:Mary,GMemN2N:Fred Table3: MemN2Nvs.GMemN2N-bAbIdataset-Task5-3argumentrelations 1 6 1 ops2 h 4 3 0 5 10 15 2 hops321 0.5Weight 0 0 5 10 15 2 1 4 ops2 h 3 6 0 5 10 15 0 memory position 8 Figure 2: 3 most frequently observed gate value 10 Incorrect Answers Correct Answers Tk(uk) patterns on T6 of the Dialog bAbI 126 4 2 0 2 4 6 8 10 12 dataset Figure 3: t-SNE scatter plot of the ﬂattened gate values inference. Lastly, Table 3 shows the comparison oftheattentionshiftingprocessbetweenMemN2N and GMemN2N on a story on bAbI task 5 (3 ar- sonetal.(2013)constructedanopen-domainread- gumentrelations). NotonlydoesGMemN2Nman- ing comprehension task, named MCTest. </Extractive Summary>  </Table ID = 3>  </Paper ID = 731> 

<Paper ID = 732>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Test accuracy for sentence classiﬁcation. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  among the LSTM models The hyper-parameters Bin: binary,FG:ﬁne-grained5classes. arethesameasthepreviousmodel.4 Table 1 shows the results of our models. For comparison, we include the results from the pub- berobustonthistask. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Followingpreviouswork, 2 Lastly, we evaluated NTI on the Stanford Senti- weadoptMAPandMRRastheevaluationmetrics forthistask.5 ment Treebank (SST) (Socher et al., 2013). This datasetcomeswithstandardtrain/dev/testsetsand Table 2 presents the results of our model and two subtasks: binary sentence classiﬁcation or the previous models for the task.6 The classiﬁer ﬁne-grained classiﬁcation of ﬁve classes. We withhandcraftedfeaturesisaSVMmodeltrained trainedourmodelonthetextspanscorresponding with a set of features. </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  SLSTM-LSTMmodel20%ofinputand20%/30% of input/output dropouts for binary and ﬁne- grainedsettings. 5Weusedtrec evalscripttocalculatetheevaluationmet- NTI-SLSTM-LSTM (as shown in Table 5) rics set the state-of-the-art results on both subtasks. 6Inclusionofsimplewordcountfeatureimprovestheper- formancebyaround0.15-0.3acrosstheboard OurNTI-SLSTMmodelperformedslightlyworse 17Adogmouthholdsaretrievedball. </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Inaddition, SNLI test data. As shown in Table 4, NTI seems ourmodelisabletore-orientitsattentionoverdif- to distinguish plural from singular forms (similar ferentpartsofthehypothesiswhentheexpression phrasesto”aperson”). Inaddition,NTIcaptures is more complex. </Extractive Summary>  </Table ID = 4>  </Paper ID = 732> 

<Paper ID = 733>  <Table ID = 2>  <Abstractive Summary> =  Table 2: F1 results for UD. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: F results for UD. </Abstractive Summary>  <Extractive Summary> =  average for attention. ? indicates signiﬁcantly Table 4 compares k-max pooling, attention and worsethanbestmodel(bold). two“hybrid”designs,asdescribedinSection3.3. Ink-max ﬁnaldecisioncanbeboosted. pooling, we select the k largest weighted values In summary, the CNN with external attention per dimension (per-dim in Table 4). In contrast, achievesthebestresultsoverall. ing,bothonWikipediaandbiomedicaltexts. This In Table 4, the sequence-preserving architec- demonstratesthattheCNNcanmakeeffectiveuse tures are slightly worse than standard attention of external information – a lexicon of uncertainty (i.e.,sequence-agnosticaveraging),butnotsignif- cuesinourcase. icantly: performance is different by about half a Sequence-agnostic vs. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  Table 3: F results for UD. </Abstractive Summary>  <Extractive Summary> =  The lengths have been achievesreasonableresults.8 accumulated, i.e., index 0 on the x-axis includes The results in Table 6 show the same trends the scores for all sentences of length l ∈ [0,10). as the CNN results in Table 3, suggesting that Mostsentenceshavelengthsl < 50. Inthisrange, our methods are applicable to other tasks as theCNNperformsbetterthantheRNNbutthedif- well. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Comparison of our best model with the stateoftheart pooling internal external pinrgesoefrvainsegnatettnecneticoannwvaasrythdaetptehnedisnegmoanntwichmereeaann- Alternatively,thieslansdowmaetsimeksnownasBrazil,andsormiegprhtesentthesamieslandastheBraziloffthewestcoastIrofeland. </Abstractive Summary>  <Extractive Summary> =  dardCNNs,ontheotherhand,canonlyselectthe kmaximumvaluesperdimension,i.e.,itcanpick 4.4 ComparisontoStateoftheArt atmostkuncertaintycuesperdimension. Table 5 compares our models with the state of the art on the uncertainty detection benchmark Pooling vs. Internal vs. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Accuracy on SST-2, different focus and 0 50 100 150 200 sourceofattention. </Abstractive Summary>  <Extractive Summary> =  Our model is much simpler 1 the CNN and the RNN with external attention for thanthestate-of-the-artmodelsforSST-2butstill different sentence lengths. The lengths have been achievesreasonableresults.8 accumulated, i.e., index 0 on the x-axis includes The results in Table 6 show the same trends the scores for all sentences of length l ∈ [0,10). as the CNN results in Table 3, suggesting that Mostsentenceshavelengthsl < 50. </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Accuracy on SST-2, sequence-agnostic bio: 25.3k), average number of out-of-vocabulary vs. </Abstractive Summary>  <Extractive Summary> =  Inthisrange, our methods are applicable to other tasks as theCNNperformsbetterthantheRNNbutthedif- well. Table 7 shows that the beneﬁt of sequence- ference is small. For longer sentences, however, preservingattentionisindeedtaskdependent. </Extractive Summary>  </Table ID = 7>  </Paper ID = 733> 

<Paper ID = 734>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Illegal activities dataset classes (A por- not take into considerations the words order, they tionofDUTAdataset) are simple, computationally efﬁcient and compat- iblewithmediumdatasetsizes. </Abstractive Summary>  <Extractive Summary> =  the eight classes text representation techniques. A) Bag-of-Words plus the Others one ( Table 2). After the text (BOW)isawell-knownmodelfortextrepresenta- pre-processing, we got 5,002 sample split it into tionthatextractsthefeaturesfromthetextcorpus a training set that contains 3,501 samples and a by counting the words frequency. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: A comparison between the classiﬁcation a single value, we calculated the macro, micro pipelineswithrespectto10foldscross-validation and weighted average for each class as Table 3 accuracy (CV), precision (P), recall (R) and F1 shows. </Abstractive Summary>  <Extractive Summary> =  SVM R 0,882 0,971 0,971 +/-0,011 F1 0,924 0,971 0,970 P 0,865 0,941 0,943 BOW 0,924 5.2 ResultsandDiscussion R 0,790 0,941 0,941 NB +/-0,009 F1 0,812 0,941 0,940 Sinceweareworkingonanunbalancedmulticlass P 0,530 0,885 0,855 TFIDF 0,863 R 0,425 0,885 0,885 problem, every class has a precision, a recall, and NB +/-0,012 F1 0,460 0,885 0,860 an F1 score. To combine these three values into Table 3: A comparison between the classiﬁcation a single value, we calculated the macro, micro pipelineswithrespectto10foldscross-validation and weighted average for each class as Table 3 accuracy (CV), precision (P), recall (R) and F1 shows. We can see that the pipeline of TF-IDF score metrics for micro, macro and weighted av- with LR achieves the highest value with a macro eraging. </Extractive Summary>  </Table ID = 3>  </Paper ID = 734> 

<Paper ID = 735>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Datasets for main tasks (above) and auxiliary tasks (below) with their number of sentences, tokens,type-tokenratio,sizeoflabelinventory,proportionofOlabels,kurtosisofthelabeldistribution, entropyofthelabeldistribution,andentropyofthelabeldistributionwithouttheOlabel. </Abstractive Summary>  <Extractive Summary> =  2.3 Dataproperties Inthenextsection,wedepictallmainandaux- iliarytasksconsideredinthispaper. Table 1 lists the datasets used in this paper, both to train main tasks and auxiliary tasks. For each 2.1 Maintasks dataset we list the following metrics: number of sentences, number of tokens, token-type ratio We use the following main tasks, aimed to repre- (TTR),thesizeofthelabelinventorycountingB- sentavarietyofsemanticsequencelabelingtasks. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Baseline (BL) and best system per- 1. </Abstractive Summary>  <Extractive Summary> =  Giventhesizeofthespaceofpossibletaskcombi- An interesting observation from the BIO task nations for MTL, we only report the baseline and analysisisthatwhilethestandardbi-LSTMmodel the results of the best system. Table 2 presents used here does not have a Viterbi-style decoding theresultsforallmainsemantictasks, comparing like more complex systems (Ma and Hovy, 2016; the results of the best system with the baseline. Lample et al., 2016), we have found very few in- The last column indicates the amount of systems validBIOsequences. Thisisthecaseforallsetups. Noneofthe from distinct sources to address data paucity, like results is better than the best systems in Table 2, done recently (Kshirsagar et al., 2015; Braud et and the effective number of systems that outper- al.,2016;Plank,2016). form the baseline are fewer (FRAMES 0, MPQA: Sutton et al. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Comparison different POS variants (data haveapreferencefortheinnerlayerhavinghigher source/taggranularity): Baseline(BL)andthedif- performance, which is consistent with the results ferenceinperformanceonthe+POSsystemwhen forPOSin(SøgaardandGoldberg,2016). </Abstractive Summary>  <Extractive Summary> =  we use POS for CHUNK (cf. Table 3), note that even though the language in WSJ is closer to the 4.2 Auxiliarytaskcontribution language in the training corpora for CHUNK and As follows from the results so far, the bi-LSTM NER, it is not the best auxiliary POS source for will not beneﬁt from auxiliary loss if there are eithertask. many labels and entropy is too high. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Label inventory size (|Y|), FREQBIN- whichroundsuptozero. </Abstractive Summary>  <Extractive Summary> =  the POS sequence. Table 4 lists all datasets with the size of their A high R2 indicates there is a high proportion of label inventory for reference (|Y|), as well as the thevarianceoflogfrequencyexplainedbythela- absolute difference in performance between the beltrigram. Weuselinearregressionimplemented FREQBIN-UNIFORMsystemandthebaseline(∆). </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Comparison default hierarchical systems corpussize. </Abstractive Summary>  <Extractive Summary> =  Infact,ifwedisablethechar- for POS, chunks and NER, and observe only im- acter features, making the system only depend on provements in chunking (similar to our ﬁndings, word information (cf. Table 5), we observe that cf. Section 4.2), however, did not investigate data two of the tasks (albeit the ones with the overall propertiesofthesetasks. </Extractive Summary>  </Table ID = 5>  </Paper ID = 735> 

<Paper ID = 736>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Results of Experiment 1; evaluation on speciﬁcsubsets,andinzero-shotlearning allphrasesfromHeiPLAS-Test attributes,and(ii)itspredictivecapacityinazero- In comparison to the best full additive model, shotlearningscenario. </Abstractive Summary>  <Extractive Summary> =  Results of Experiment 1 are shown in by linguistic curators. The data is separated into Table 2. The upper part of the table contains developmentandtestset(comprising869and729 theresultsbasedonwordembeddings(comprising triples, respectively, which correspond to a to- non-parametric, parametric, dilation and trainable tal of 254 target attributes). </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The target attributes compositionmodels);thecount-basedC-LDAand aresubdividedintovarioussemanticallyhomoge- L-LDAbaselinesaredisplayedbelow. neous subsets, as shown in Table 1. Due to cov- Focussing on the non-parametric models ﬁrst, erage issues in the pre-trained word2vec embed- weﬁndthatrelyingontheadjectiveembeddingas dings(Mikolovetal.,2013a),someadjectivesand a surrogate of a composed representation already nounsfromHeiPLAScannotbeprojectedintothe outperformsbothcountmodelsbyawidemargin. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Results of Experiment 3 (Spearman’s ρ 0.1 betweenhumanjudgmentsandmodelpredictions) 0 >0 >1 >2 >3 >4 >5 >6 Similarity Rating Level Figure 2: ASTA-5 scores over different levels of humansimilarityratings(cf.Experiment4) Withrespecttoweightedaddition,allresultsre- ported in Table 3 are based on the weighting pa- rameters (α=0.88; β=0.12) that have been found as optimal by Mitchell and Lapata (2010). </Abstractive Summary>  <Extractive Summary> =  interesting alternative to previous distributional Results. As shown in Table 3, the best cor- modelswhichexplicitlyencodeattributemeaning relation scores between human similarity judg- inwordvectorsandrelyonvectormixtureopera- ments and model predictions are achieved by our tionsinordertocomposethemintoattribute-based model that is built upon word embeddings and a phraserepresentations,and(ii)bearsthepotential trained full additive composition function based of being used as a generalized attribute extraction onweightingadjectiveandnounvectors(ρ=0.50). modelonvariousdomainsofapplicationsthatde- This model outperforms all distributional base- mandfordifferentattributeinventories. 7Ascoreof1expresseslowsimilaritybetweenphrases,7 indicateshighsimilarity. 60UnderlyingWord Weighted Full (cid:12) ⊕ Representation Addition Additive 0.5 Full Additive Weighted Addition word2vec 0.36 0.48 0.42 0.50 0.4 Tensor Product Adjective Dilation Noun Dilation MM&&LL--TBoopWic 00..2456 00..3367 00..3448 nn//aa 5 score 0.3 C-LDA 0.28 0.19 n/a n/a A- ST 0.2 A Table 3: Results of Experiment 3 (Spearman’s ρ 0.1 betweenhumanjudgmentsandmodelpredictions) 0 >0 >1 >2 >3 >4 >5 >6 Similarity Rating Level Figure 2: ASTA-5 scores over different levels of humansimilarityratings(cf.Experiment4) Withrespecttoweightedaddition,allresultsre- ported in Table 3 are based on the weighting pa- rameters (α=0.88; β=0.12) that have been found as optimal by Mitchell and Lapata (2010). Based on a grid search, we ﬁnd α=0.60 and β=0.40 to are compared wrt. </Extractive Summary>  </Table ID = 3>  </Paper ID = 736> 

<Paper ID = 737>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  For such relation, for each dataset, we the holonym and y is the meronym. For consis- ranked the measures by their AP@100 score, se- tencywithEVALution,weswitchedthosepairsin lecting those with score ≥ 0.8.6 Table 3 displays BLESS,placingthemeronyminthexslotandthe theintersectionofthedatasets’bestmeasures. holonyminthey slot. </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  catisatypeofanimal). ity measures computed on syntactic contexts suc- Table 2 shows that SLQS performs well in this ceed to discriminate between hypernyms and at- task on BLESS. This is contrary to previous ﬁnd- tributes. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (2015b). The large perfor- Table 4 displays the performance of the best mancegapsreportedbyLevyetal.(2015b)might classiﬁeroneachdataset,inahypernymvs. asin- beattributedtothesizeoftheirtrainingsets. </Extractive Summary>  </Table ID = 4>  </Paper ID = 737> 

<Paper ID = 738>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The ﬁnal beddings. Inthiswork,theauthorsappliedanun- datasetcontainsthenumberofwordpairsaccord- supervised algorithm for the automatic extraction ing to word classes described in Table 1. More- of symmetric patterns from plain text. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  More- of symmetric patterns from plain text. The sym- over, Table 2 shows the average number of pat- metric patterns were deﬁned as a sequence of 3-5 ternsforeachwordpairinourdataset. tokens consisting of exactly two wildcards and 1- 3words. </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Moreover, we demon- and compare the effects of the GloVe word em- strated that the distance feature outperformed a beddings and the dLCE word embeddings on the previously suggested direction feature, and that performanceofthetwoproposedmodels. our embeddings outperformed the state-of-the-art Table 5 illustrates the performance of our two GloVe embeddings. Last but not least, our two models on all word classes. </Extractive Summary>  </Table ID = 5>  </Paper ID = 738> 

<Paper ID = 739>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  and F-score of this combined model outperforms 4.1.2 DiscussionofResults that of the dependency-based model with feature expansionbyalargemargin. The results of the TWSI evaluation are presented in Table 1. In accordance with prior art in word Figure 3 illustrates how granularity of the in- sense disambiguation, the most frequent sense duced sense inventory inﬂuences WSD perfor- (MFS) proved to be a strong baseline, reaching mance. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: WSD performance of the best conﬁguration of our method identiﬁed on the TWSI dataset as comparedtoparticipantsoftheSemEval2013Task13andtwosystemsbasedonwordsenseembeddings (AdaGramandSenseGram). </Abstractive Summary>  <Extractive Summary> =  sensesper 4.2.2 DiscussionofResults word). Our method outperforms the knowledge- Table 3 presents results of evaluation of the based system of La Sapienza according to two of best conﬁguration of our approach trained on the three metrics metrics and the SenseGram system ukWaC corpus. We compare our approach to based on sense embeddings according to four of fourSemEvalparticipantsandtwostate-of-the-art ﬁve metrics. </Extractive Summary>  </Table ID = 3>  </Paper ID = 739> 

<Paper ID = 740>  <Table ID = 1>  <Abstractive Summary> =  Table 1: number of documents (#Docs), sentences (#Sents), tokens (#Tokens), sense anno- 2010). </Abstractive Summary>  <Extractive Summary> =  • SemEval-15 task 13 (Moro and Navigli, 4.3 Statistics 2015). This is the most recent WSD dataset Table 1 shows some statistics6 of the WSD available to date, annotated with WordNet datasets and training corpora which we use in the 3.0. It consists of 1022 sense annotations evaluation framework. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  al.,2014,Lesk +emb)14. ext 5.2 Results • UKB (Agirre and Soroa, 2009; Agirre et al., Table 2 shows the F-Measure performance of all 2014) is a graph-based WSD system which comparison systems on the ﬁve all-words WSD makes use of random walks over a seman- datasets. Since not all test word instances are tic network (WordNet graph in this case). </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thisresulted fact that in many cases the main disambiguation in a large evaluation dataset of 7,253 instances to clueisgivenbytheimmediatelocalcontext. This disambiguate (see Table 3). Table 4 shows the F- is particularly problematic for knowledge-based Measure performance of all comparison systems systems, as they take equally into account all the on the concatenation of all ﬁve WSD evaluation words within a sentence (or document in the case datasets, divided by PoS tag. Corpus System Nouns Verbs Adjectives Adverbs All IMS 70.4 56.1 75.6 82.9 68.4 IMS+emb 71.8 55.4 76.1 82.7 69.1 SemCor IMS +emb 71.9 56.9 75.9 84.7 69.6 -s Context2Vec 71.0 57.6 75.2 82.7 69.0 MFS 67.6 49.6 73.1 80.5 64.8 Supervised Ceiling 89.6 95.1 91.5 96.4 91.5 IMS 70.5 56.9 76.8 82.9 68.8 IMS+emb 71.0 53.3 77.1 82.7 68.3 SemCor+ IMS +emb 72.0 56.5 76.6 84.7 69.7 -s OMSTI Context2Vec 71.7 55.8 77.2 82.7 69.4 MFS 65.8 45.9 72.7 80.5 62.9 Ceiling 90.4 95.8 91.8 96.4 92.1 Lesk 54.1 27.9 54.6 60.3 48.7 ext Lesk +emb 69.8 51.2 51.7 80.6 63.7 ext UKB 56.7 39.3 63.9 44.0 53.2 Knowledge - UKB gloss 62.1 38.3 66.8 66.2 57.5 Babelfy 68.6 49.9 73.2 79.8 65.5 WN1st sense 67.6 50.3 74.3 80.9 65.2 Table4: F-MeasurepercentageofdifferentmodelsontheconcatenationofallﬁveWSDdatasets. Table 3), considerably greater than the ambiguity In this sentence, state is annotated with its ad- onotherPoStags,e.g.,4.8innouns. Nonetheless, ministrative districts of a nation sense in the gold supervised systems manage to comfortably out- standard. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  This disambiguate (see Table 3). Table 4 shows the F- is particularly problematic for knowledge-based Measure performance of all comparison systems systems, as they take equally into account all the on the concatenation of all ﬁve WSD evaluation words within a sentence (or document in the case datasets, divided by PoS tag. IMS +emb trained -s of Babelfy). </Extractive Summary>  </Table ID = 4>  </Paper ID = 740> 

<Paper ID = 741>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Oneisbasedonentitysearchtechnique(IR and manually classiﬁed the left into two parts: approach) and the other is based on a text entail- EQs and SQs. The number of different kinds of mentapproachwherewespeciﬁcallyemploydeep questions are listed in Table 1. The examples of neural networks (NN approach). </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  SQ bestforSQs. WetesttheweightsonEQsandSQs ofGKHMCwiththeircorrespondingweights,and Sentence Encoder: We experimented several re- result is shown in Table 3. As we can see, with current neural networks with different structures theseweights,weachievepromisingresult. </Extractive Summary>  </Table ID = 3>  </Paper ID = 741> 

<Paper ID = 742>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Each question in this markersincludeby, as, because, but, and, for and dataset contains at least four user-generated an- of – the full list can be found in Appendix B in swers. Some examples can be found in Table 1. (Marcu,1998). </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: The systems results versus the base- frozen on their best development epoch, the test lines. </Abstractive Summary>  <Extractive Summary> =  Weexperimentallyevaluatethefollowingmodels: The model described in Section 3 is regular- 7http://nlp.sista.arizona.edu/ ized with L2-regularization and dropout. The de- releases/acl2014/ velopment sets are used solely for early stopping 8http://webscope.sandbox.yahoo.com/ 126• MLP-discourse: The discourse features are Yahoo!Answers extracted as described in Section 4, an MLP Model P@1 MRR isusedtoproducetheranking; RandomBaseline 15.74 37.40 CRBaseline 22.63 47.17 • GRU-MLP: The system described in Sec- Jansenetal.(2014) 30.49 51.89 tion 3 without the interaction matrix S and BogdanovaandFoster(2016) 37.17 56.82 anyotherexternalfeatures(xext inSection3 MLP-Discourse 32.72* 53.54* andinFigure1); GRU-MLP 36.12* 56.63* • GRU-MLP-Sim: The system described in GRU-MLP-Sim 37.13* 57.56* Section 3 with the interaction matrix S and GRU-MLP-Sim-Discourse 38.74* 58.37* noexternalfeatures; AskUbuntu Model P@1 MRR • GRU-MLP-Sim-Discourse: Thesystemde- RandomBaseline 26.60 53.64 scribed in Section 3 with the interaction ma- CRBaseline 35.36 60.17 trixS andthediscoursefeaturesastheexter- MLP-Discourse 37.80* 61.75* nalfeaturesx ; ext GRU-MLP 38.56* 62.53* GRU-MLP-Sim 39.28* 62.64* Table 3 reports the answer reranking P@1 and GRU-MLP-Sim-Discourse 41.40* 64.42* MRR of the described models along with the re- sults of the baseline systems. The models were Table 3: The systems results versus the base- frozen on their best development epoch, the test lines. * The improvements over the CR and Ran- set had been used neither for model selection nor dombaselinesarestatisticallysigniﬁcantwithp < forparametertuning.9 0.05. All signiﬁcance tests are performed with Table 3 shows that the discourse features on one-tailed bootstrap resampling with 10,000 iter- their own with an MLP (MLP-Discourse) outper- ations. form the random and the CR baselines for both datasets. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Pre- alwaysreliable. Inmanycasestheuserstend viousstudiesinthisareahaveeitherbeenfeature- to select as the best those answers that are based or purely neural approaches that require no most sympathetic (see (Q1) in Table 4) or manual feature engineering. We show that these funny (see (Q2) and (Q3) in Table 4), rather twoapproachescanbesuccessfullycombined. Inmanycasestheuserstend viousstudiesinthisareahaveeitherbeenfeature- to select as the best those answers that are based or purely neural approaches that require no most sympathetic (see (Q1) in Table 4) or manual feature engineering. We show that these funny (see (Q2) and (Q3) in Table 4), rather twoapproachescanbesuccessfullycombined. We thantheonesprovidingmoreusefulinforma- propose a novel neural architecture whereby the tion. </Extractive Summary>  </Table ID = 4>  </Paper ID = 742> 

<Paper ID = 743>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Several highly probable clauses learnt lakantan et al. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We 1 Mean Top-k alsocollecttheentitytypeinformationfromFree- s s0.9 o base. Table 2 summarizes some important statis- L g 0.8 n tics. ForthePathQAexperiment,weusethesame ni ai0.7 train/dev/testsplitofWordNetdatasetreleasedby Tr 0.6 Guuetal.(2015)andhenceourresultsaredirectly 0.5 comparabletothem. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We use Adam (Kingma and SumExp. The results are shown in the second Ba,2014)foroptimizationforallourexperiments section of Table 3. It is not surprising to see withthedefaulthyperparametersettings(learning that the Single-Model, which leverages parame- rate = 1e´3, β1 “ 0.9, β2 “ 0.999, (cid:15) “ 1e´8). Thelast Avg. pooling performs the worst, which shows section of Table 3 lists the performance gain ob- that it is also important to weigh the paths scores tainedbyinjectinginformationaboutentities. We accordingtotheirvalues. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Model performance when trained with a butthisdidnotyieldsigniﬁcantimprovements. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 743> 

<Paper ID = 744>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Statistics for the CADEC corpus (See section2). </Abstractive Summary>  <Extractive Summary> =  While this changes the embedding of a small subset of the words,thesearemeaningfulandfrequentlyoccur- The motivation for using external knowledge ringinoursetting(seedetailsinSection6). bases in our case stems from the relatively small Figure1showsthecompletearchitectureofour size of the CADEC corpus (see Table 1), in com- model, including the RNN transducer LSTM and parisonwithotherneuralmodelstrainingcorpora. the pretrained word embeddings augmented with For example, The Penn Treebank (Marcus et al., DBpedia entity embeddings. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  1http://www.askapatient.com 143In order to allow ML algorithms to make use This task depends heavily upon context, as the of the information encapsulated in such graphs, a same word span can appear as an ADR in one recent line of work (Bordes et al., 2011; Bordes text, and as a Symptom in another. For exam- et al., 2013; Wang et al., 2014; Ji et al., 2016) ple, the ﬁrst entry in Table 2 mentions several has compellingly suggested to embed entities as ADRs (“made me gain 30 lbs”, “made my BP d dimensional dense real vectors, and relations as go up so high”, “gave me more anxiety”) asso- two projection matrices: Rlhs and Rrhs. Simi- ciating each with a different drug (“Klonopin”, larly to word embedding techniques (Collobert et “Lexapro”, etc.), while the second entry in the ta- al., 2011; Mikolov et al., 2013), a deep learning bleusessomeofthesamesurfaceformstoreferto modelistrainedtodifferentiatebetweenobserved an addressed Symptoms (e.g., “It helped both my (positive) and non-observed (negative) triples, by anxietyandIBS”). This This improvement does have a cost, however; is due to the fact that most of the text in the Ask RASCAL only allows a single annotation type at a Patient forum describes background situa- a time, so the annotation of two predeﬁned types tionandisnotdirectlyrelatedtoanADR(see,for (e.g., Drug and ADR) requires two passes. Sec- example, the ﬁrst entry in Table 2). This poses ond, RASCAL does not support non-contiguous a problem for training accuracy oriented models, spanannotations. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Performance of the different baselines model by testing both in a supervised scenario as by training from RASCAL annotations (top) vs. </Abstractive Summary>  <Extractive Summary> =  6.3 Erroranalysis In analyzing the RASCAL model, we ﬁnd that it External knowledge improves performance in relativelylacksinrecall. Thisisduetoourlimited both scenarios - As can be seen from the abla- annotation effort having predictably limited cov- tion test in Table 3, in both supervised and anno- erage. Examining our annotations, we ﬁnd 449 tatordevelopmentsettings,ourpretrainedembed- uniqueADRsannotatedinRASCALoutoftheto- dings improve performance by at least 13 points tal 3685 unique ADR phrases in the full CADEC annotation. </Extractive Summary>  </Table ID = 3>  </Paper ID = 744> 

<Paper ID = 745>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The ﬁ- false positive rate (FPR) against the true positive nal dataset contains 9,611 users in total, with an rate(TPR).Thisgivesusareceiveroperatingchar- average of 3521 tweets per user. The number of acteristics(ROC)curve,allowingustoinspectthe users with each condition is included in Table 1. performance of each model on a speciﬁc task at Users in this joined dataset may be tagged with anylevelofFPR. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  factors,though,suchasfeaturesparsity. We also found that feature counts have a pro- Table 3 shows parameters sweeps with hidden nouncedeffectonthelosscurves: Relativefeature layer width 256, training the MTL model on the frequencies yield models that are much easier to socialmediadatawithcharactertrigramsasinput trainthanrawfeaturecounts. features. </Extractive Summary>  </Table ID = 3>  </Paper ID = 745> 

<Paper ID = 746>  </Paper ID = 746> 

<Paper ID = 747>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  totle’slogosdimensionwithcogency,whichbetter ﬁtsreal-worldargumentation. Similarly,weomit 3.1 OverviewoftheTheory-basedTaxonomy those dimensions from Table 1 in the taxonomy Ourobjectiveisnottocomeupwithanewtheory, thathaveuncleardeﬁnitions, suchasstrength, or but to provide a uniﬁed view of existing theories thatarecoveredbyothers,suchaswell-formedness, thatissuitableforqualityassessment. Weaimfor which merely reﬁnes the acceptability part of co- acommonunderstandingofthedimensionsthataf- gency(Govier,2010). </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thethreemostagreeingannotatorsfor bythethreeannotatorswas“cannotjudge”(forglo- eachdimensionachievedmuchhigherα-valuesbe- balrelevance),suggestingthatourguidelineswere tween.23(clarity)and.60(credibility),though.5 comprehensive. Regarding agreement, we see in Thestudyresultswerediscussedbyallannota- Table 3(b) that the α-values of all logical and di- tors, leading to a considerably reﬁned version of alecticalqualitydimensionsexceptforglobalsufﬁ- theguidelines. Wethenselectedthreeannotators ciencylieabove0.4forthemostagreeingannotator forthecorpusannotationbasedontheiravailability. </Extractive Summary>  </Table ID = 3>  </Paper ID = 747> 

<Paper ID = 748>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Accuracy and differences between POS NN→NE 9.26 taggers ADV→ADJD 6.09 KOUS→PWAV 4.06 by roughly 6 points in accuracy, but almost dou- Changederrors: 8.15%(218/2674) blesthenumbersofcorrectlytaggedsentences. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Token-based label changes comparing from the response of Clevertagger (9.41). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Coreference resolution evaluation (F1) K butoneinS. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  <Table ID = 9>  <Abstractive Summary> =  Table 9: POS tagging upper bounds and accuracy (HOTCoref→Stanford) (highest scores in green; lowest in red; middle in yellow) We see that less than 50% of the changes that The Stanford tagger, despite performing the the Stanford system introduces are corrections lowest with respect to overall accuracy, achieves (44.62%). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 9>  <Table ID = 1>  <Abstractive Summary> =  Table 11: Mention-based coreference perfor- spectively. Table 10: Parsing accuracy (LAS) and upper puts. Table 12: Largest accuracy differences between TreeTagger (TT) and Clevertagger(CT); number oftokenwithPOStaginthetestset(#tok) A.2 Coreferenceresolution SincetheARCSframeworkisrelativelyunknown and not widely used, we revisit the connection of our diff metric to accuracy and F1 outlined in section 2 in order to use one of the corefer- ence metrics to establish the differences between the outputs. </Abstractive Summary>  <Extractive Summary> =  The resulting F1 score can then be usedasanagreementvalue,which,however,does not provide any detailed analysis of the nature of the differences compared to the ARCS approach. Table 13 shows the F1 scores when using one re- sponseasthekeyandthesecondasresponse. Note that switching the key and the response role pro- vides the same F1 scores for two responses; the only effect is that the recall and precision values areswitched. </Extractive Summary>  </Table ID = 1>  </Paper ID = 748> 

<Paper ID = 749>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  2 EvaluationMetrics METEOR addresses several deﬁciencies of BLEU such as recall evaluation and the lack of explicit A summary of the metrics investigated in our wordmatching. n-grambasedmeasuresworkrea- study is given in Table 1. All these metrics ex- sonably well when there is a signiﬁcant overlap 200between reference and candidate sentences; how- dog ever they fail to spot semantic similarity when mouth stick in the common words are scarce. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Seetextfordetails. Description BLEU METEOR ROUGE CIDEr SPICE WMD original amanwearingaredlifejacketissittinginacanoe 1 1 1 10 1 1 onalake candidate amanwearingalifejacketisinasmallboatona 0.45 0.28 0.67 2.19 0.40 0.19 lake synonyms aguywearingalifevestisinasmallboatona 0.20 0.17 0.57 0.65 0.00 0.10 lake redundancy amanwearingalifejacketisinasmallboatona 0.45 0.28 0.66 2.01 0.36 0.18 lakeatsunset wordorder inasmallboatonalakeamaniswearingalife 0.26 0.26 0.38 1.32 0.40 0.19 jacket ple case in Table 2. In this table, an original cap- Candidate1 A guy with a red jacket is standing on a boat . </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  tioning models in (Aditya et al., 2015; Karpathy Williams test (Williams, 1959) calculates the andFei-Fei,2015). statisticalsigniﬁcanceofdifferencesindependent Table 3 shows Pearson’s, Spearman’s and correlations,andformulatedastestingwhetherthe Kendall’s correlation of the metrics with the hu- populationcorrelationbetweenX andX equals 1 3 man judgements in FLICKR-8K and COMPOSITE thepopulationcorrelationbetweenX andX : 2 3 datasets. For FLICKR-8K, we follow the method- p ology in (Elliott and Keller, 2014) and compute (r −r ) (n−1)(1+r ) t(n−3) = q 13 23 12 correlationswiththehumanexpertscores. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ferentlevelofdifﬁculty. In Table 4, we present caption-level classiﬁca- ABSTRACT-50S(Vedantametal.,2015)dataset tion accuracy scores of automatic evaluation met- is a subset of the Abstract Scenes Dataset (Zit- rics at matching human consensus scores. On nick and Parikh, 2013), which includes 500 im- agescontainingclipartobjectsineverydayscenes. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In addition, racy score. In Table 5, we present the classiﬁca- weshouldaddthatthisuniﬁedmetricsigniﬁcantly tion accuracies of the evaluation metrics for each outperforms the individual metrics according to distraction type. As can be seen, the WMD met- Williamstest(p < 0.01). </Extractive Summary>  </Table ID = 5>  </Paper ID = 749> 

<Paper ID = 750>  <Table ID = 1>  <Abstractive Summary> =  Table 1: WMT15 Test Data Statistics grouped by mance of other MT quality evaluation metrics by source languages. </Abstractive Summary>  <Extractive Summary> =  of news texts. Table 1 shows the statistics for the HT texts) are selected from the WMT15 dataset test data. The target language is English for all basedonthelowest(5)rankingsbyhumanjudges. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Domains of the test data are the same for all languages except for French. 3.2 Features ThetestdatafortheFrench-Englishlanguagepair Table 2 provides examples of MT errors in com- wasfetchedfromanewsdiscussionforuminstead parison to HT. All example translations (MT vs. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Pearson’s r correlation between Trueskill scores of a metric and human judgments with the corresponding95%conﬁdenceintervalsareshown. </Abstractive Summary>  <Extractive Summary> =  of the features and human rankings. Therefore, we also investigate whether we can predict hu- Results Table 3 shows all the Pearson’s corre- manrankingsofMTtranslatedtextbycombining lations. Overall, Formality-RB obtains the high- these features since they capture different aspects est correlation score (80.7%) among all features. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Pearson’s r correlation between Trueskill scores of a metric and human judgments with the corresponding 95% conﬁdence intervals are shown. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 750> 

<Paper ID = 751>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Unlabeled Attachment Score on the UD ce Hindi n test data for TurboParser and Tensor-LSTM with re Persian crossentropyloss. </Abstractive Summary>  <Extractive Summary> =  Another explanation could be that Figure 5: Performance for Tensor-LSTM on En- some feature of the cross entropy loss function glishtestdatawith0-40%oftheedgescoresarti- makesitespeciallywellsuitedforLatinlanguages ﬁciallymaintainedat0. – as seen in Table 1, French and Spanish are alsotwoofthelanguagesforwhichTensor-LSTM As can be clearly seen, performance drops yieldsthehighestperformanceimprovement. faster with the percentage of deleted labels for To compare the effect of missing edge scores the cross entropy model. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Unlabeled attachment scores for the various systems. </Abstractive Summary>  <Extractive Summary> =  French and Spanish, however, 50 do not follow the same trend, with cross entropy S loss outperforming mean squared loss despite the A U highnumberofmissinglabels. 48 In Table 2, performance on French and Span- 46 ish for both systems can be seen to be very high. It may be the case that Indo-European target lan- 44 guages are not as affected by missing labels as 0 10 20 30 40 mostofthesourcelanguagesarethemselvesIndo- %blankout European. This conﬁrms our in- uponperformancewithoutinﬂuencefromlinguis- tuition that the initially lower performance us- tic factors such as language similarity, we repeat ing mean squared loss compared to cross entropy thecross-lingualexperimentononelanguagewith loss is mitigated by a greater robustness towards respectively10%,20%,30%,and40%ofthepro- missing labels, gained by circumventing the de- jected and averaged edge scores artiﬁcially set to coding step in the training process. In Table 2, 0,simulatingmissingdata. WechoosetheEnglish thisisreﬂectedasdramaticperformanceincreases data for this experiment, as the English projected using mean squared error for Finnish, Persian, data has the lowest percentage of missing labels Hindi, and Hebrew – the four languages furthest 226removed from the predominantly Indo-European mine and ﬁlter out high-uncertainty trees. </Extractive Summary>  </Table ID = 2>  </Paper ID = 751> 

<Paper ID = 752>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Matrix representation of the directed words of the input test section receive the FUNC- graphforthewordsinthesentence. </Abstractive Summary>  <Extractive Summary> =  6. A function κ(h,d) that returns whether the dependency (h,d) has a valid attachment di- Table 4 shows the directed multigraph used for rectiongiventhePOSofthed(cf. Sec.3.2). </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  thecanonicalUDtestsets. Table 3 shows a trace of the algorithm, with C = hhad,connection,extremists,speciali and 5.1 Baseline F = {They,also,a,to,some}. We compare our UDP system with the perfor- it word h H mance of a rule-based baseline that uses the head 1 had root ∅ 2 connection had {had} rules in Table 5. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: UAS for baseline with gold POS (BL ) G wouldbedoubled,aseitherawronglytaggedhead with direction (L/R) for backoff attachments, or dependent would break the dependency rules. </Abstractive Summary>  <Extractive Summary> =  Table 3 shows a trace of the algorithm, with C = hhad,connection,extremists,speciali and 5.1 Baseline F = {They,also,a,to,some}. We compare our UDP system with the perfor- it word h H mance of a rule-based baseline that uses the head 1 had root ∅ 2 connection had {had} rules in Table 5. The baseline identiﬁes the ﬁrst 3 extremists had {had,connection} verb(orﬁrstcontentwordiftherearenoverbs)as 4 special connection {had,connection,extremists} themainpredicate,andassignsheadstoallwords 5 They had {had,connection,extremists,special} 6 also had ... bor,accordingtotheestimatedruntimeparameter. We report the best head direction and its score The ﬁrst four iterations calculate the head of for each language in Table 5. This baseline ﬁnds contentwordsfollowingtheirPR,andthefollow- the head of each token based on its closest possi- ing iterations attach the function words in F. This gives French 47.1R 64.5 72.7 70.6 62.1 36.3 German 48.2R 60.6 66.9 62.5 57.0 24.2 usanindicationhowoursystemcomparestostan- Gothic 50.2L 57.5 61.7 59.2 55.8 34.1 Greek 45.7R 58.5 68.0 66.4 57.0 29.3 dardcross-lingualparsers. Hebrew 41.8R 55.4 62.0 58.6 52.8 35.7 Hindi 43.9R 46.3 34.6 34.5 45.7 27.0 Hungarian 53.1R 56.7 58.4 56.8 54.8 22.7 5.3 Results Indonesian 44.6L 60.6 63.6 61.0 58.4 35.3 Irish 47.5R 56.6 62.5 61.3 53.9 35.8 Table 5 shows that UDP is a competitive system; Italian 50.6R 69.4 77.1 75.2 67.9 37.6 because UDP is remarkably close to the super- Latin 49.4L 56.2 59.8 54.9 52.4 37.1 G Norwegian 49.1R 61.7 70.8 67.3 58.6 29.8 vised MSD system, with an average difference Persian 37.8L 55.7 57.8 55.6 53.6 33.9 G Polish 60.8R 68.4 75.6 71.7 65.7 34.6 of 6.4%. Notably, UDP even outperforms MSD Portuguese 45.8R 65.7 72.8 71.4 64.9 33.5 ononelanguage(Hindi). Table 7: Average language-wise domain evalua- However, if we regard the results for UDP in tion. We report average UAS and standard devi- G Table 5, we can see that there are 24 languages ation per language. The bottom row provides the (out of 32) for which the parser performs better averagestandarddeviationforeachsystem. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Evaluation across domains. </Abstractive Summary>  <Extractive Summary> =  Ingeneral,therule-basedUPD Cross-domain test sets To further assess the is less sensitive to domain shifts than the data- cross-domainrobustness,weretrievedthedomain driven MSD counterpart, conﬁrming earlier ﬁnd- (genre)splitsfromthetestsectionsoftheUDtree- ings(PlankandvanNoord,2010). banks where the domain information is available Table 6 gives the detailed scores per language as sentence metadata: from Bulgarian, Croatian, and domain. From the scores we can see that andItalian. </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Note these three pervised parsers require around 100 sentences to small datasets are not included in the results on reach UDP-comparable performance, namely a thecanonicaltestsectionsinTable5. mean of 300 sentences and a median of 100 sen- Table 7 summarizes the per-language average tences, with Bulgarian (3k), Czech (1k), and Ger- scoreandstandarddeviation,aswellasthemacro- man (1.5k) as outliers. The difference between averaged standard deviation across languages. </Extractive Summary>  </Table ID = 7>  </Paper ID = 752> 

<Paper ID = 753>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Whenseveral corporaareavailableforalanguage,wepickedthe 3Spectral-based dimension reduction such as PCA are standard one. Table 1 provides some basic statis- limitedbythenumberofeigenvectorsofthematrixtobere- tics on the language datasets. Also note that our duced.Forexampleamatrixofsize200×500canatmostbe reducedintoa200×200matrixviaPCA.Whenthenumber experiments follow the train/dev/test split as pro- ofeigenvectorsissmallerthan500,weusethatvalueinstead. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Best UAS scores in cross-lingual setting. </Abstractive Summary>  <Extractive Summary> =  Attachment Scores (UAS) measured on the test 4.5 Cross-lingualExperiments sets ignoring the punctuation marks. As a base- linecomparisonweuseourimplementationofthe Table 3 summarizes the UAS scores achieved us- MSTparser without morpho-syntactic attributes ing delexicalized embeddings learned on several representation of any kind. We computed the sig- languages. </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Best UAS scores for each embedding type in monolingual setting. </Abstractive Summary>  <Extractive Summary> =  guages (en, eu, hu, ro) in the cross-lingual set- ting compared to the best monolingual setting. 4.4 MonolingualExperiments Whilethemultilingualembeddingsdonotoutper- Table 2 displays UAS scores for the monolin- form the monolingual ones for the other four lan- gual setting. Except for French and Romanian guages,theystilldeliverparsingperformancethat that do not show real improvement, the six other are better than with the baseline MST parser for languagesshowsubstantialperformanceincreases alllanguages(butGothic). </Extractive Summary>  </Table ID = 2>  </Paper ID = 753> 

<Paper ID = 754>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  arguments. However, as illustrated in Table 1, Weassumethatcincludesaclaimtargetx ,de- claims typically refer to the pros and cons of the c ﬁned as a phrase about which c makes a positive topic target, but do not entail or contradict the or a negative assertion. Speciﬁcally, it is deﬁned topic. Stance(c,t) = sc×R(xc,xt)×st (1) where Stance(c,t) ∈ {−1,1}, 1 indicates Pro 3 TheClaimPolarityDataset and −1 indicates Con. Rows 1-8 in Table 1 show examples for x , s , x , s and R(x ,x ). It is The IBM argumentative structure dataset pub- c c t t c t easytoverifythatthemodelcorrectlypredictsthe lished by Aharoni et al. We required absolute majority agreement (≥3)fors andR(x ,x ),otherwisetheclaimwas c c t We assessed the validity and applicability of the labeledasincompatiblewithourmodel. proposedmodelthroughmanualannotationofthe Rows 1-8 in Table 1 show some examples of IBM dataset.5 The labeled data was also used to annotated claims in our dataset. Row 9 is an ex- trainandassesssub-componentsinthemodelim- ampleofaclaimthatwasfoundincompatiblewith plementation. Here we address a Sentimentmatching: Positiveandnegativeterms more general problem of open domain, generic fromthesentimentlexiconofHuandLiu(2004a) target identiﬁcation. Table 1 illustrates the diver- arematchedintheclaim. sityandcomplexityofclaimtargets. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Features extracted for a target candidate presents in more detail our novel contrast detec- xinaclaimc. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ployed here a simpliﬁed version of the sentiment analyzer (cf. Section 6.2), in which target identi- 8.2 Results,AnalysisandDiscussion ﬁcationisnotperformed,andsentimenttermsare The results are shown in Table 3. Comparing the weighted uniformly. </Extractive Summary>  </Table ID = 3>  </Paper ID = 754> 

<Paper ID = 755>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Five are more likely to be of the tweet only. </Abstractive Summary>  <Extractive Summary> =  The irony occurs because tic literature. Several categories have been pro- thewriterbelievesthathisaudiencecandetectthe posed, as shown in the ﬁrst column of Table 2. disparity between P and P on the basis of con- 1 2 Since all these categories have been found in a textualknowledgeorcommonbackgroundshared speciﬁc genre (literary texts), the ﬁrst step was with the writer. −→ P2: Serge Dassault is involved and has been sentencedinmanycourtcases. 3.2 Ironycategories 3.3 Ironymarkers Both explicit and implicit activation types can be expressed in different ways which we call irony As shown in Table 2, linguistic literature consid- categories. After a thorough inspection of how ers other forms of irony categories, such as sur- categories have been deﬁned in linguistic litera- prise effect, repetition, etc. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  next section). This is an important re- Table 4 gives the percentage of tweets belong- sult that shows that annotators are able to iden- ing to each category of irony split according to tify which are the textual spans that activate the explicit vs. implicit activation, when applicable. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Besides standard frequencies, nations are false assertion/other and false asser- we provide the correlations between irony activa- tion/hyperbole for both English and French; and tiontypesandmarkersandbetweencategoriesand analogy/otherforEnglish4. markersinordertobringoutfeaturesthatcouldbe Table 5 provides the percentage of tweets con- usedinaperspectiveofautomaticironydetection. tainingmarkersforironic(explicitorimplicit)and In each corpus, all the frequencies presented here non ironic tweets (row in gray). </Extractive Summary>  </Table ID = 5>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In French, in- arestatisticallysigniﬁcantfromwhatwouldbeex- tensiﬁers, punctuationmarksandinterjectionsare pectedbychanceusingtheχ2 test(p < 0.05). morefrequentinironictweetswhereasquotations Table 3 gives the total number of annotated are more frequent in non ironic tweets. In En- tweets and the activation type for ironic tweets. </Extractive Summary>  </Table ID = 3>  </Paper ID = 755> 

<Paper ID = 756>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  On the other hand, being only a NLP expert hashtags, can be particularly helpful when per- 276formingaspect-basedsentimentanalysis. An analogous consideration can be drawn for Regarding emojis, the most frequent ones are the emojii distribution (see Table 2). It turns out (asexpected)sentiment-driven,i.e. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  lated to the context where they appear: their con- tribution in terms of conveyed sentiment (or con- veyedtopic)strictlydependsonthedomainwhere they are used. In Table 3, we report a compari- Table1: Labeldistributionperannotator sonbetweenthelabeldistributionoftwoemojisin A A A 1 2 3 Subjective 0.671 0.748 0.657 ourcorpusandthecorrespondingdistributionina Objective 0.330 0.252 0.343 state of the art emoji sentiment lexicon (Novak et None 0.330 0.252 0.343 al.,2015). Positive 0.509 0.476 0.426 In the proposed corpus, the ﬁre emoji has been Neutral 0.038 0.146 0.131 Negative 0.123 0.126 0.100 mainly labelled as positive because it represents None 0.330 0.252 0.343 the word “hot”, whose meaning is intended as Explicit 0.254 0.512 0.416 something beautiful and trendy. </Extractive Summary>  </Table ID = 3>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  As conclusion, any emoji should be Anger 0.014 0.034 0.028 not considered as independent on the context and Trust 0.008 0.034 0.022 thereforeevaluatedaccordingtoitssemantic. Fear 0.001 0.001 0.002 5.1 AgreementMeasures In Table 1, we report some statistics that sum- marize behaviours of the involved annotators. By The kappa coefﬁcient (Cohen, 1960) is the most analysingthedistributions,wecanobservediffer- usedstatisticformeasuringthedegreeofreliabil- entattitudes: A isinclinedtolabelmorepostsas ity between annotators. </Extractive Summary>  </Table ID = 1>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Inordertoestimatetheuncertaintyofthe annotation of each labeller, we sampled a portion Amorereliablemeasureforestimatingtheagree- of tweets to be annotated twice by the same an- ment among annotators is PABAK-OS (Parker et notator. We report in Table 5 the self-agreement al., 2011), which controls for chance agreement. measure,thatisavalidindextoquantifythequal- PABAK-OSaimstoavoidthepeculiar,unintuitive ityofthelabellingprocedure. </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Theannotatorscanbeconsid- lenceofagivenlabel). eredmoderatelyreliableforimplicit/explicitanno- We report in Table 4, the inter-agreement be- tationsandveryaccuratefortheremaininglabels. tweencouplesofannotatorsdistinguishedforeach label. </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ing only the ironic messages. From the results, We described the construction of the corpus, to- reported in Table 6, we can conﬁrm that A and getherwithannotationschema,statisticsandsome 1 A annotators are more willing to interpret irony interestingremarks. Theproposedcorpusisaimed 2 similarly(asalreadystatedinTable4). </Extractive Summary>  </Table ID = 6>  </Paper ID = 756> 

<Paper ID = 757>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Our best feedforward variant signiﬁ- tunethenumberofhiddenlayersandhiddenunits cantly outperforms the systems with surface fea- on the development set. </Abstractive Summary>  <Extractive Summary> =  The test set is not yet available 3,000BrownclustersontheGigawordcorpus. at the time of submission, so the system is eval- Table 5 shows the results for the models which uated based on the average accuracy over 7-fold arebesttunedonthenumberofhiddenunits,hid- cross-validation on the combined set of training den layers, and the types of word vectors. The anddevelopmentsets. </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Comparing various systems on the CoNLL 2016 Shared Task standard datasets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 757> 

<Paper ID = 758>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Number of documents (#Doc), trees (#Trees, less than #Doc when we were unable to parse a document,seeSection4.2),words(#Words,seeSection6),relations(#Rel,originally),labels(#Lab,re- lationandnuclearity),EDUs(#EDU,max/min/avgnumberofEDUsperdocument),andCDUs(#CDU). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Dictionary coverage for each dataset on could also contain discourse connectives, adverbs thetrainsetwhenavailable,onthedevsetelse. </Abstractive Summary>  <Extractive Summary> =  Position and length Other features are used to The word features for the non-English datasets representthepositionoftheEDUinthedocument aretranslatedusingavailablebilingualWiktionar- anditslengthintokens. Weusethresholdstodis- ies19withoutdisambiguation,thecoverageofeach tinguishbetweenverylong(lengthl > 25tokens), dictionary is given in Table 2. We also look for a long(l > 15),short(l > 5)andveryshort(l ≤ 5) translation of the lemma (and of the stems for the EDUs. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Performance of our monolingual and cross-lingual systems for Span (Sp), Nuclearity (Nuc) and Relation (Rel). </Abstractive Summary>  <Extractive Summary> =  CONDITION+3%). Table 3. Our parser is competitive with state-of- When combining corpora for source and target the-art systems for English (ﬁrst line in Table 3), languages(“+dev.”inTable3),weobtainourbest with even better performance for unlabeled struc- performingsystemforEnglish,withallscoresim- ture (85.04%) and structure labeled with nuclear- proved compared to our best monolingual system ity(72.29%). Table 3. Our parser is competitive with state-of- When combining corpora for source and target the-art systems for English (ﬁrst line in Table 3), languages(“+dev.”inTable3),weobtainourbest with even better performance for unlabeled struc- performingsystemforEnglish,withallscoresim- ture (85.04%) and structure labeled with nuclear- proved compared to our best monolingual system ity(72.29%). Theseresultsshowthatusingallthe (+0.8forNuclearityand+1.3forRelation). 313695. (“+emb” in Table 3) for monolingual systems of- tenleadstoanimportantdropinperformance,es- References peciallyforRelation(from−1.1to−4.2%). This demonstrates that these embeddings do not pro- Farah Benamara and Maite Taboada. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Mapping of all the relations found in the datasets: for each class, we give the set of relation names as they appear in the corpora (removing only the possible sufﬁxes “-e”, “-s”, “-mn”). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 758> 

<Paper ID = 759>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In such way, tigated. Table 1 give an example of each of the weleveragethedatasetofDSTC-2tocreatemore questiontypepresentedbelowonadialogsample complex reasoning task than the ones present in ofDSTC-2corpus. the original dialogs of the dataset by performing Inference procedure: Concretely, the current rule-based modiﬁcation over the corpus. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: One supporting fact task : Acc. </Abstractive Summary>  <Extractive Summary> =  dialogaugmentationstrategyallowstoexhibitthe 4.3 Experimentalresults competencyoftheproposedmodelbeyondfactoid questions. Table 3 presents tracking accuracy obtained for three variables of the DSTC2 dataset formulated 4.2 TrainingDetails as Factoid Question task. We compare with two Assuggestedin(Sukhbaataretal.,2015),10%of established utterance-level discriminative neural the set was held-out to form a validation set for trackers, a Recurrent Neural Network (RNN) hyperparameter tuning. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Reasoning tasks : Acc. </Abstractive Summary>  <Extractive Summary> =  ob- approach offers several advantages compared to tainedonDSTC2testset stateoftheartmethodsoftracking. First,thepro- posed method allows to perform tracking on the As a second result, Table 4 presents the perfor- basis of segment-dialog-level annotation instead mance obtained for the four reasoning tasks. The of utterance-level one that is commonly admitted obtained results lead us to think that MemN2N are in academic datasets but tedious to produce in a a competitive alternative for the task dialog state large scale industrial environment. </Extractive Summary>  </Table ID = 4>  </Paper ID = 759> 

<Paper ID = 760>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We randomly selected speech disﬂuences intact. Demographic informa- 60 texts from this dataset, forcing only the con- tion for participants in our study is presented in dition that the number of sentences of each text Table 1. A third dataset was used in robustness sentencewasgreaterthan12. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Table3summarizestheresults. generated randomly from a gaussian distribution scaled by fan in + fan out (Glorot and Bengio, From Table 3 we can see that our approach 2010). Both embeddings matrix E and E presents better results for the Constitution dataset word tag were adjusted during training. </Extractive Summary>  </Table ID = 3>  </Paper ID = 760> 

<Paper ID = 761>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ing the RNN on the rarer types. Whilst the prob- The utility of a joint task As can be seen in lem is attenuated by the memory facility of the Table 3, the overall best performing systems on LSTM, our best system still suffers the vanish- theindividualtasksdonotreachtheresultsinany inggradientproblemforpredictinglongerrepairs relevant metric of the best performing combined with reparanda over 3 words long. Also we show system. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  WealsoinvestigatedtheuttSegdetectionerrors Incrementality Incrementally the differences and see that the networks are generally not con- betweenthearchitectureswasneglible–resultsfor fusingdisﬂuencieswithboundaries. However,our the LSTM are in Table 4. The latency for repair best system incorrectly labelled 3.6% of the ref- onsetdetectionisverylow,beingdetectedaslittle erence uttSegs as rpS (hence also affecting the as 0.196 seconds after the onset word is ﬁnished precisionoftherpS prediction)–uponinspection (or on transcripts largely directly after the word these were largely abandoned utterances, which has been consumed as TTD (word) = 0.003). </Extractive Summary>  </Table ID = 4>  </Paper ID = 761> 

<Paper ID = 762>  </Paper ID = 762> 

<Paper ID = 763>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Accuracy (%) on the 20 QA tasks for models using 1k and 10k training examples. </Abstractive Summary>  <Extractive Summary> =  4.3 Results generates a further boost in performance consis- tentlyonboththe1kand10kdatasets. Performance results on the 20 bAbI QA dataset are presented in Table 1. For comparison pur- poses,westillpresentMemN2N(Sukhbaataretal., State-of-the-art performance on both the 2015) in Table 1 but accompany it with the accu- 1k and 10k dataset. Performance results on the 20 bAbI QA dataset are presented in Table 1. For comparison pur- poses,westillpresentMemN2N(Sukhbaataretal., State-of-the-art performance on both the 2015) in Table 1 but accompany it with the accu- 1k and 10k dataset. The best performing racy obtained by our implementation of the same GMemN2N model achieves state-of-the-art perfor- modelwiththesameexperimentalsetuponv1.2of mance, an average accuracy of 87.3 on the 1k thedatasetinthecolumn“OurMemN2N”forboth dataset and 96.3 on the 10k variant. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Per-response accuracy and per-dialog accuracy (in parentheses) on the Dialog bAbI tasks. </Abstractive Summary>  <Extractive Summary> =  (OOV) test sets, are constructed separately. In addition, a supplementary dataset, task 6, is pro- 5.3 Results vided with real human-bot conversations, also in Performance results on the Dialog bAbI the restaurant domain, which is derived from the dataset are shown in Table 2, measured using secondDialogStateTrackingChallenge(Hender- both per-response accuracy and per-dialog accu- son et al., 2014). It is important to notice that the racy (given in parentheses). </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  transformgatecellvalues, Tk(uk) /d N/A N/A N/A 0.22 0.23 0.45 i i Question: Whogavethefootball? Answer: Fred,MemN2N:Mary,GMemN2N:Fred Table3: MemN2Nvs.GMemN2N-bAbIdataset-Task5-3argumentrelations 1 6 1 ops2 h 4 3 0 5 10 15 2 hops321 0.5Weight 0 0 5 10 15 2 1 4 ops2 h 3 6 0 5 10 15 0 memory position 8 Figure 2: 3 most frequently observed gate value 10 Incorrect Answers Correct Answers Tk(uk) patterns on T6 of the Dialog bAbI 126 4 2 0 2 4 6 8 10 12 dataset Figure 3: t-SNE scatter plot of the ﬂattened gate values inference. Lastly, Table 3 shows the comparison oftheattentionshiftingprocessbetweenMemN2N and GMemN2N on a story on bAbI task 5 (3 ar- sonetal.(2013)constructedanopen-domainread- gumentrelations). NotonlydoesGMemN2Nman- ing comprehension task, named MCTest. </Extractive Summary>  </Table ID = 3>  </Paper ID = 763> 

<Paper ID = 764>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Test accuracy for sentence classiﬁcation. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  among the LSTM models The hyper-parameters Bin: binary,FG:ﬁne-grained5classes. arethesameasthepreviousmodel.4 Table 1 shows the results of our models. For comparison, we include the results from the pub- berobustonthistask. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Followingpreviouswork, 2 Lastly, we evaluated NTI on the Stanford Senti- weadoptMAPandMRRastheevaluationmetrics forthistask.5 ment Treebank (SST) (Socher et al., 2013). This datasetcomeswithstandardtrain/dev/testsetsand Table 2 presents the results of our model and two subtasks: binary sentence classiﬁcation or the previous models for the task.6 The classiﬁer ﬁne-grained classiﬁcation of ﬁve classes. We withhandcraftedfeaturesisaSVMmodeltrained trainedourmodelonthetextspanscorresponding with a set of features. </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  SLSTM-LSTMmodel20%ofinputand20%/30% of input/output dropouts for binary and ﬁne- grainedsettings. 5Weusedtrec evalscripttocalculatetheevaluationmet- NTI-SLSTM-LSTM (as shown in Table 5) rics set the state-of-the-art results on both subtasks. 6Inclusionofsimplewordcountfeatureimprovestheper- formancebyaround0.15-0.3acrosstheboard OurNTI-SLSTMmodelperformedslightlyworse 17Adogmouthholdsaretrievedball. </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Inaddition, SNLI test data. As shown in Table 4, NTI seems ourmodelisabletore-orientitsattentionoverdif- to distinguish plural from singular forms (similar ferentpartsofthehypothesiswhentheexpression phrasesto”aperson”). Inaddition,NTIcaptures is more complex. </Extractive Summary>  </Table ID = 4>  </Paper ID = 764> 

<Paper ID = 765>  <Table ID = 2>  <Abstractive Summary> =  Table 2: F1 results for UD. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: F results for UD. </Abstractive Summary>  <Extractive Summary> =  average for attention. ? indicates signiﬁcantly Table 4 compares k-max pooling, attention and worsethanbestmodel(bold). two“hybrid”designs,asdescribedinSection3.3. Ink-max ﬁnaldecisioncanbeboosted. pooling, we select the k largest weighted values In summary, the CNN with external attention per dimension (per-dim in Table 4). In contrast, achievesthebestresultsoverall. ing,bothonWikipediaandbiomedicaltexts. This In Table 4, the sequence-preserving architec- demonstratesthattheCNNcanmakeeffectiveuse tures are slightly worse than standard attention of external information – a lexicon of uncertainty (i.e.,sequence-agnosticaveraging),butnotsignif- cuesinourcase. icantly: performance is different by about half a Sequence-agnostic vs. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  Table 3: F results for UD. </Abstractive Summary>  <Extractive Summary> =  The lengths have been achievesreasonableresults.8 accumulated, i.e., index 0 on the x-axis includes The results in Table 6 show the same trends the scores for all sentences of length l ∈ [0,10). as the CNN results in Table 3, suggesting that Mostsentenceshavelengthsl < 50. Inthisrange, our methods are applicable to other tasks as theCNNperformsbetterthantheRNNbutthedif- well. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Comparison of our best model with the stateoftheart pooling internal external pinrgesoefrvainsegnatettnecneticoannwvaasrythdaetptehnedisnegmoanntwichmereeaann- Alternatively,thieslansdowmaetsimeksnownasBrazil,andsormiegprhtesentthesamieslandastheBraziloffthewestcoastIrofeland. </Abstractive Summary>  <Extractive Summary> =  dardCNNs,ontheotherhand,canonlyselectthe kmaximumvaluesperdimension,i.e.,itcanpick 4.4 ComparisontoStateoftheArt atmostkuncertaintycuesperdimension. Table 5 compares our models with the state of the art on the uncertainty detection benchmark Pooling vs. Internal vs. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Accuracy on SST-2, different focus and 0 50 100 150 200 sourceofattention. </Abstractive Summary>  <Extractive Summary> =  Our model is much simpler 1 the CNN and the RNN with external attention for thanthestate-of-the-artmodelsforSST-2butstill different sentence lengths. The lengths have been achievesreasonableresults.8 accumulated, i.e., index 0 on the x-axis includes The results in Table 6 show the same trends the scores for all sentences of length l ∈ [0,10). as the CNN results in Table 3, suggesting that Mostsentenceshavelengthsl < 50. </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Accuracy on SST-2, sequence-agnostic bio: 25.3k), average number of out-of-vocabulary vs. </Abstractive Summary>  <Extractive Summary> =  Inthisrange, our methods are applicable to other tasks as theCNNperformsbetterthantheRNNbutthedif- well. Table 7 shows that the beneﬁt of sequence- ference is small. For longer sentences, however, preservingattentionisindeedtaskdependent. </Extractive Summary>  </Table ID = 7>  </Paper ID = 765> 

<Paper ID = 766>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Illegal activities dataset classes (A por- not take into considerations the words order, they tionofDUTAdataset) are simple, computationally efﬁcient and compat- iblewithmediumdatasetsizes. </Abstractive Summary>  <Extractive Summary> =  the eight classes text representation techniques. A) Bag-of-Words plus the Others one ( Table 2). After the text (BOW)isawell-knownmodelfortextrepresenta- pre-processing, we got 5,002 sample split it into tionthatextractsthefeaturesfromthetextcorpus a training set that contains 3,501 samples and a by counting the words frequency. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: A comparison between the classiﬁcation a single value, we calculated the macro, micro pipelineswithrespectto10foldscross-validation and weighted average for each class as Table 3 accuracy (CV), precision (P), recall (R) and F1 shows. </Abstractive Summary>  <Extractive Summary> =  SVM R 0,882 0,971 0,971 +/-0,011 F1 0,924 0,971 0,970 P 0,865 0,941 0,943 BOW 0,924 5.2 ResultsandDiscussion R 0,790 0,941 0,941 NB +/-0,009 F1 0,812 0,941 0,940 Sinceweareworkingonanunbalancedmulticlass P 0,530 0,885 0,855 TFIDF 0,863 R 0,425 0,885 0,885 problem, every class has a precision, a recall, and NB +/-0,012 F1 0,460 0,885 0,860 an F1 score. To combine these three values into Table 3: A comparison between the classiﬁcation a single value, we calculated the macro, micro pipelineswithrespectto10foldscross-validation and weighted average for each class as Table 3 accuracy (CV), precision (P), recall (R) and F1 shows. We can see that the pipeline of TF-IDF score metrics for micro, macro and weighted av- with LR achieves the highest value with a macro eraging. </Extractive Summary>  </Table ID = 3>  </Paper ID = 766> 

<Paper ID = 767>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Datasets for main tasks (above) and auxiliary tasks (below) with their number of sentences, tokens,type-tokenratio,sizeoflabelinventory,proportionofOlabels,kurtosisofthelabeldistribution, entropyofthelabeldistribution,andentropyofthelabeldistributionwithouttheOlabel. </Abstractive Summary>  <Extractive Summary> =  2.3 Dataproperties Inthenextsection,wedepictallmainandaux- iliarytasksconsideredinthispaper. Table 1 lists the datasets used in this paper, both to train main tasks and auxiliary tasks. For each 2.1 Maintasks dataset we list the following metrics: number of sentences, number of tokens, token-type ratio We use the following main tasks, aimed to repre- (TTR),thesizeofthelabelinventorycountingB- sentavarietyofsemanticsequencelabelingtasks. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Baseline (BL) and best system per- 1. </Abstractive Summary>  <Extractive Summary> =  Giventhesizeofthespaceofpossibletaskcombi- An interesting observation from the BIO task nations for MTL, we only report the baseline and analysisisthatwhilethestandardbi-LSTMmodel the results of the best system. Table 2 presents used here does not have a Viterbi-style decoding theresultsforallmainsemantictasks, comparing like more complex systems (Ma and Hovy, 2016; the results of the best system with the baseline. Lample et al., 2016), we have found very few in- The last column indicates the amount of systems validBIOsequences. Thisisthecaseforallsetups. Noneofthe from distinct sources to address data paucity, like results is better than the best systems in Table 2, done recently (Kshirsagar et al., 2015; Braud et and the effective number of systems that outper- al.,2016;Plank,2016). form the baseline are fewer (FRAMES 0, MPQA: Sutton et al. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Comparison different POS variants (data haveapreferencefortheinnerlayerhavinghigher source/taggranularity): Baseline(BL)andthedif- performance, which is consistent with the results ferenceinperformanceonthe+POSsystemwhen forPOSin(SøgaardandGoldberg,2016). </Abstractive Summary>  <Extractive Summary> =  we use POS for CHUNK (cf. Table 3), note that even though the language in WSJ is closer to the 4.2 Auxiliarytaskcontribution language in the training corpora for CHUNK and As follows from the results so far, the bi-LSTM NER, it is not the best auxiliary POS source for will not beneﬁt from auxiliary loss if there are eithertask. many labels and entropy is too high. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Label inventory size (|Y|), FREQBIN- whichroundsuptozero. </Abstractive Summary>  <Extractive Summary> =  the POS sequence. Table 4 lists all datasets with the size of their A high R2 indicates there is a high proportion of label inventory for reference (|Y|), as well as the thevarianceoflogfrequencyexplainedbythela- absolute difference in performance between the beltrigram. Weuselinearregressionimplemented FREQBIN-UNIFORMsystemandthebaseline(∆). </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Comparison default hierarchical systems corpussize. </Abstractive Summary>  <Extractive Summary> =  Infact,ifwedisablethechar- for POS, chunks and NER, and observe only im- acter features, making the system only depend on provements in chunking (similar to our ﬁndings, word information (cf. Table 5), we observe that cf. Section 4.2), however, did not investigate data two of the tasks (albeit the ones with the overall propertiesofthesetasks. </Extractive Summary>  </Table ID = 5>  </Paper ID = 767> 

<Paper ID = 768>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Results of Experiment 1; evaluation on speciﬁcsubsets,andinzero-shotlearning allphrasesfromHeiPLAS-Test attributes,and(ii)itspredictivecapacityinazero- In comparison to the best full additive model, shotlearningscenario. </Abstractive Summary>  <Extractive Summary> =  Results of Experiment 1 are shown in by linguistic curators. The data is separated into Table 2. The upper part of the table contains developmentandtestset(comprising869and729 theresultsbasedonwordembeddings(comprising triples, respectively, which correspond to a to- non-parametric, parametric, dilation and trainable tal of 254 target attributes). </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The target attributes compositionmodels);thecount-basedC-LDAand aresubdividedintovarioussemanticallyhomoge- L-LDAbaselinesaredisplayedbelow. neous subsets, as shown in Table 1. Due to cov- Focussing on the non-parametric models ﬁrst, erage issues in the pre-trained word2vec embed- weﬁndthatrelyingontheadjectiveembeddingas dings(Mikolovetal.,2013a),someadjectivesand a surrogate of a composed representation already nounsfromHeiPLAScannotbeprojectedintothe outperformsbothcountmodelsbyawidemargin. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Results of Experiment 3 (Spearman’s ρ 0.1 betweenhumanjudgmentsandmodelpredictions) 0 >0 >1 >2 >3 >4 >5 >6 Similarity Rating Level Figure 2: ASTA-5 scores over different levels of humansimilarityratings(cf.Experiment4) Withrespecttoweightedaddition,allresultsre- ported in Table 3 are based on the weighting pa- rameters (α=0.88; β=0.12) that have been found as optimal by Mitchell and Lapata (2010). </Abstractive Summary>  <Extractive Summary> =  interesting alternative to previous distributional Results. As shown in Table 3, the best cor- modelswhichexplicitlyencodeattributemeaning relation scores between human similarity judg- inwordvectorsandrelyonvectormixtureopera- ments and model predictions are achieved by our tionsinordertocomposethemintoattribute-based model that is built upon word embeddings and a phraserepresentations,and(ii)bearsthepotential trained full additive composition function based of being used as a generalized attribute extraction onweightingadjectiveandnounvectors(ρ=0.50). modelonvariousdomainsofapplicationsthatde- This model outperforms all distributional base- mandfordifferentattributeinventories. 7Ascoreof1expresseslowsimilaritybetweenphrases,7 indicateshighsimilarity. 60UnderlyingWord Weighted Full (cid:12) ⊕ Representation Addition Additive 0.5 Full Additive Weighted Addition word2vec 0.36 0.48 0.42 0.50 0.4 Tensor Product Adjective Dilation Noun Dilation MM&&LL--TBoopWic 00..2456 00..3367 00..3448 nn//aa 5 score 0.3 C-LDA 0.28 0.19 n/a n/a A- ST 0.2 A Table 3: Results of Experiment 3 (Spearman’s ρ 0.1 betweenhumanjudgmentsandmodelpredictions) 0 >0 >1 >2 >3 >4 >5 >6 Similarity Rating Level Figure 2: ASTA-5 scores over different levels of humansimilarityratings(cf.Experiment4) Withrespecttoweightedaddition,allresultsre- ported in Table 3 are based on the weighting pa- rameters (α=0.88; β=0.12) that have been found as optimal by Mitchell and Lapata (2010). Based on a grid search, we ﬁnd α=0.60 and β=0.40 to are compared wrt. </Extractive Summary>  </Table ID = 3>  </Paper ID = 768> 

<Paper ID = 769>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  For such relation, for each dataset, we the holonym and y is the meronym. For consis- ranked the measures by their AP@100 score, se- tencywithEVALution,weswitchedthosepairsin lecting those with score ≥ 0.8.6 Table 3 displays BLESS,placingthemeronyminthexslotandthe theintersectionofthedatasets’bestmeasures. holonyminthey slot. </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  catisatypeofanimal). ity measures computed on syntactic contexts suc- Table 2 shows that SLQS performs well in this ceed to discriminate between hypernyms and at- task on BLESS. This is contrary to previous ﬁnd- tributes. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (2015b). The large perfor- Table 4 displays the performance of the best mancegapsreportedbyLevyetal.(2015b)might classiﬁeroneachdataset,inahypernymvs. asin- beattributedtothesizeoftheirtrainingsets. </Extractive Summary>  </Table ID = 4>  </Paper ID = 769> 

<Paper ID = 770>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The ﬁnal beddings. Inthiswork,theauthorsappliedanun- datasetcontainsthenumberofwordpairsaccord- supervised algorithm for the automatic extraction ing to word classes described in Table 1. More- of symmetric patterns from plain text. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  More- of symmetric patterns from plain text. The sym- over, Table 2 shows the average number of pat- metric patterns were deﬁned as a sequence of 3-5 ternsforeachwordpairinourdataset. tokens consisting of exactly two wildcards and 1- 3words. </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Moreover, we demon- and compare the effects of the GloVe word em- strated that the distance feature outperformed a beddings and the dLCE word embeddings on the previously suggested direction feature, and that performanceofthetwoproposedmodels. our embeddings outperformed the state-of-the-art Table 5 illustrates the performance of our two GloVe embeddings. Last but not least, our two models on all word classes. </Extractive Summary>  </Table ID = 5>  </Paper ID = 770> 

<Paper ID = 771>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  and F-score of this combined model outperforms 4.1.2 DiscussionofResults that of the dependency-based model with feature expansionbyalargemargin. The results of the TWSI evaluation are presented in Table 1. In accordance with prior art in word Figure 3 illustrates how granularity of the in- sense disambiguation, the most frequent sense duced sense inventory inﬂuences WSD perfor- (MFS) proved to be a strong baseline, reaching mance. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: WSD performance of the best conﬁguration of our method identiﬁed on the TWSI dataset as comparedtoparticipantsoftheSemEval2013Task13andtwosystemsbasedonwordsenseembeddings (AdaGramandSenseGram). </Abstractive Summary>  <Extractive Summary> =  sensesper 4.2.2 DiscussionofResults word). Our method outperforms the knowledge- Table 3 presents results of evaluation of the based system of La Sapienza according to two of best conﬁguration of our approach trained on the three metrics metrics and the SenseGram system ukWaC corpus. We compare our approach to based on sense embeddings according to four of fourSemEvalparticipantsandtwostate-of-the-art ﬁve metrics. </Extractive Summary>  </Table ID = 3>  </Paper ID = 771> 

<Paper ID = 772>  <Table ID = 1>  <Abstractive Summary> =  Table 1: number of documents (#Docs), sentences (#Sents), tokens (#Tokens), sense anno- 2010). </Abstractive Summary>  <Extractive Summary> =  • SemEval-15 task 13 (Moro and Navigli, 4.3 Statistics 2015). This is the most recent WSD dataset Table 1 shows some statistics6 of the WSD available to date, annotated with WordNet datasets and training corpora which we use in the 3.0. It consists of 1022 sense annotations evaluation framework. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  al.,2014,Lesk +emb)14. ext 5.2 Results • UKB (Agirre and Soroa, 2009; Agirre et al., Table 2 shows the F-Measure performance of all 2014) is a graph-based WSD system which comparison systems on the ﬁve all-words WSD makes use of random walks over a seman- datasets. Since not all test word instances are tic network (WordNet graph in this case). </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thisresulted fact that in many cases the main disambiguation in a large evaluation dataset of 7,253 instances to clueisgivenbytheimmediatelocalcontext. This disambiguate (see Table 3). Table 4 shows the F- is particularly problematic for knowledge-based Measure performance of all comparison systems systems, as they take equally into account all the on the concatenation of all ﬁve WSD evaluation words within a sentence (or document in the case datasets, divided by PoS tag. Corpus System Nouns Verbs Adjectives Adverbs All IMS 70.4 56.1 75.6 82.9 68.4 IMS+emb 71.8 55.4 76.1 82.7 69.1 SemCor IMS +emb 71.9 56.9 75.9 84.7 69.6 -s Context2Vec 71.0 57.6 75.2 82.7 69.0 MFS 67.6 49.6 73.1 80.5 64.8 Supervised Ceiling 89.6 95.1 91.5 96.4 91.5 IMS 70.5 56.9 76.8 82.9 68.8 IMS+emb 71.0 53.3 77.1 82.7 68.3 SemCor+ IMS +emb 72.0 56.5 76.6 84.7 69.7 -s OMSTI Context2Vec 71.7 55.8 77.2 82.7 69.4 MFS 65.8 45.9 72.7 80.5 62.9 Ceiling 90.4 95.8 91.8 96.4 92.1 Lesk 54.1 27.9 54.6 60.3 48.7 ext Lesk +emb 69.8 51.2 51.7 80.6 63.7 ext UKB 56.7 39.3 63.9 44.0 53.2 Knowledge - UKB gloss 62.1 38.3 66.8 66.2 57.5 Babelfy 68.6 49.9 73.2 79.8 65.5 WN1st sense 67.6 50.3 74.3 80.9 65.2 Table4: F-MeasurepercentageofdifferentmodelsontheconcatenationofallﬁveWSDdatasets. Table 3), considerably greater than the ambiguity In this sentence, state is annotated with its ad- onotherPoStags,e.g.,4.8innouns. Nonetheless, ministrative districts of a nation sense in the gold supervised systems manage to comfortably out- standard. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  This disambiguate (see Table 3). Table 4 shows the F- is particularly problematic for knowledge-based Measure performance of all comparison systems systems, as they take equally into account all the on the concatenation of all ﬁve WSD evaluation words within a sentence (or document in the case datasets, divided by PoS tag. IMS +emb trained -s of Babelfy). </Extractive Summary>  </Table ID = 4>  </Paper ID = 772> 

<Paper ID = 773>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Oneisbasedonentitysearchtechnique(IR and manually classiﬁed the left into two parts: approach) and the other is based on a text entail- EQs and SQs. The number of different kinds of mentapproachwherewespeciﬁcallyemploydeep questions are listed in Table 1. The examples of neural networks (NN approach). </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  SQ bestforSQs. WetesttheweightsonEQsandSQs ofGKHMCwiththeircorrespondingweights,and Sentence Encoder: We experimented several re- result is shown in Table 3. As we can see, with current neural networks with different structures theseweights,weachievepromisingresult. </Extractive Summary>  </Table ID = 3>  </Paper ID = 773> 

<Paper ID = 774>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Each question in this markersincludeby, as, because, but, and, for and dataset contains at least four user-generated an- of – the full list can be found in Appendix B in swers. Some examples can be found in Table 1. (Marcu,1998). </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: The systems results versus the base- frozen on their best development epoch, the test lines. </Abstractive Summary>  <Extractive Summary> =  Weexperimentallyevaluatethefollowingmodels: The model described in Section 3 is regular- 7http://nlp.sista.arizona.edu/ ized with L2-regularization and dropout. The de- releases/acl2014/ velopment sets are used solely for early stopping 8http://webscope.sandbox.yahoo.com/ 126• MLP-discourse: The discourse features are Yahoo!Answers extracted as described in Section 4, an MLP Model P@1 MRR isusedtoproducetheranking; RandomBaseline 15.74 37.40 CRBaseline 22.63 47.17 • GRU-MLP: The system described in Sec- Jansenetal.(2014) 30.49 51.89 tion 3 without the interaction matrix S and BogdanovaandFoster(2016) 37.17 56.82 anyotherexternalfeatures(xext inSection3 MLP-Discourse 32.72* 53.54* andinFigure1); GRU-MLP 36.12* 56.63* • GRU-MLP-Sim: The system described in GRU-MLP-Sim 37.13* 57.56* Section 3 with the interaction matrix S and GRU-MLP-Sim-Discourse 38.74* 58.37* noexternalfeatures; AskUbuntu Model P@1 MRR • GRU-MLP-Sim-Discourse: Thesystemde- RandomBaseline 26.60 53.64 scribed in Section 3 with the interaction ma- CRBaseline 35.36 60.17 trixS andthediscoursefeaturesastheexter- MLP-Discourse 37.80* 61.75* nalfeaturesx ; ext GRU-MLP 38.56* 62.53* GRU-MLP-Sim 39.28* 62.64* Table 3 reports the answer reranking P@1 and GRU-MLP-Sim-Discourse 41.40* 64.42* MRR of the described models along with the re- sults of the baseline systems. The models were Table 3: The systems results versus the base- frozen on their best development epoch, the test lines. * The improvements over the CR and Ran- set had been used neither for model selection nor dombaselinesarestatisticallysigniﬁcantwithp < forparametertuning.9 0.05. All signiﬁcance tests are performed with Table 3 shows that the discourse features on one-tailed bootstrap resampling with 10,000 iter- their own with an MLP (MLP-Discourse) outper- ations. form the random and the CR baselines for both datasets. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Pre- alwaysreliable. Inmanycasestheuserstend viousstudiesinthisareahaveeitherbeenfeature- to select as the best those answers that are based or purely neural approaches that require no most sympathetic (see (Q1) in Table 4) or manual feature engineering. We show that these funny (see (Q2) and (Q3) in Table 4), rather twoapproachescanbesuccessfullycombined. Inmanycasestheuserstend viousstudiesinthisareahaveeitherbeenfeature- to select as the best those answers that are based or purely neural approaches that require no most sympathetic (see (Q1) in Table 4) or manual feature engineering. We show that these funny (see (Q2) and (Q3) in Table 4), rather twoapproachescanbesuccessfullycombined. We thantheonesprovidingmoreusefulinforma- propose a novel neural architecture whereby the tion. </Extractive Summary>  </Table ID = 4>  </Paper ID = 774> 

<Paper ID = 775>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Several highly probable clauses learnt lakantan et al. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We 1 Mean Top-k alsocollecttheentitytypeinformationfromFree- s s0.9 o base. Table 2 summarizes some important statis- L g 0.8 n tics. ForthePathQAexperiment,weusethesame ni ai0.7 train/dev/testsplitofWordNetdatasetreleasedby Tr 0.6 Guuetal.(2015)andhenceourresultsaredirectly 0.5 comparabletothem. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We use Adam (Kingma and SumExp. The results are shown in the second Ba,2014)foroptimizationforallourexperiments section of Table 3. It is not surprising to see withthedefaulthyperparametersettings(learning that the Single-Model, which leverages parame- rate = 1e´3, β1 “ 0.9, β2 “ 0.999, (cid:15) “ 1e´8). Thelast Avg. pooling performs the worst, which shows section of Table 3 lists the performance gain ob- that it is also important to weigh the paths scores tainedbyinjectinginformationaboutentities. We accordingtotheirvalues. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Model performance when trained with a butthisdidnotyieldsigniﬁcantimprovements. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 775> 

<Paper ID = 776>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Statistics for the CADEC corpus (See section2). </Abstractive Summary>  <Extractive Summary> =  While this changes the embedding of a small subset of the words,thesearemeaningfulandfrequentlyoccur- The motivation for using external knowledge ringinoursetting(seedetailsinSection6). bases in our case stems from the relatively small Figure1showsthecompletearchitectureofour size of the CADEC corpus (see Table 1), in com- model, including the RNN transducer LSTM and parisonwithotherneuralmodelstrainingcorpora. the pretrained word embeddings augmented with For example, The Penn Treebank (Marcus et al., DBpedia entity embeddings. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  1http://www.askapatient.com 143In order to allow ML algorithms to make use This task depends heavily upon context, as the of the information encapsulated in such graphs, a same word span can appear as an ADR in one recent line of work (Bordes et al., 2011; Bordes text, and as a Symptom in another. For exam- et al., 2013; Wang et al., 2014; Ji et al., 2016) ple, the ﬁrst entry in Table 2 mentions several has compellingly suggested to embed entities as ADRs (“made me gain 30 lbs”, “made my BP d dimensional dense real vectors, and relations as go up so high”, “gave me more anxiety”) asso- two projection matrices: Rlhs and Rrhs. Simi- ciating each with a different drug (“Klonopin”, larly to word embedding techniques (Collobert et “Lexapro”, etc.), while the second entry in the ta- al., 2011; Mikolov et al., 2013), a deep learning bleusessomeofthesamesurfaceformstoreferto modelistrainedtodifferentiatebetweenobserved an addressed Symptoms (e.g., “It helped both my (positive) and non-observed (negative) triples, by anxietyandIBS”). This This improvement does have a cost, however; is due to the fact that most of the text in the Ask RASCAL only allows a single annotation type at a Patient forum describes background situa- a time, so the annotation of two predeﬁned types tionandisnotdirectlyrelatedtoanADR(see,for (e.g., Drug and ADR) requires two passes. Sec- example, the ﬁrst entry in Table 2). This poses ond, RASCAL does not support non-contiguous a problem for training accuracy oriented models, spanannotations. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Performance of the different baselines model by testing both in a supervised scenario as by training from RASCAL annotations (top) vs. </Abstractive Summary>  <Extractive Summary> =  6.3 Erroranalysis In analyzing the RASCAL model, we ﬁnd that it External knowledge improves performance in relativelylacksinrecall. Thisisduetoourlimited both scenarios - As can be seen from the abla- annotation effort having predictably limited cov- tion test in Table 3, in both supervised and anno- erage. Examining our annotations, we ﬁnd 449 tatordevelopmentsettings,ourpretrainedembed- uniqueADRsannotatedinRASCALoutoftheto- dings improve performance by at least 13 points tal 3685 unique ADR phrases in the full CADEC annotation. </Extractive Summary>  </Table ID = 3>  </Paper ID = 776> 

<Paper ID = 777>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The ﬁ- false positive rate (FPR) against the true positive nal dataset contains 9,611 users in total, with an rate(TPR).Thisgivesusareceiveroperatingchar- average of 3521 tweets per user. The number of acteristics(ROC)curve,allowingustoinspectthe users with each condition is included in Table 1. performance of each model on a speciﬁc task at Users in this joined dataset may be tagged with anylevelofFPR. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  factors,though,suchasfeaturesparsity. We also found that feature counts have a pro- Table 3 shows parameters sweeps with hidden nouncedeffectonthelosscurves: Relativefeature layer width 256, training the MTL model on the frequencies yield models that are much easier to socialmediadatawithcharactertrigramsasinput trainthanrawfeaturecounts. features. </Extractive Summary>  </Table ID = 3>  </Paper ID = 777> 

<Paper ID = 778>  </Paper ID = 778> 

<Paper ID = 779>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  totle’slogosdimensionwithcogency,whichbetter ﬁtsreal-worldargumentation. Similarly,weomit 3.1 OverviewoftheTheory-basedTaxonomy those dimensions from Table 1 in the taxonomy Ourobjectiveisnottocomeupwithanewtheory, thathaveuncleardeﬁnitions, suchasstrength, or but to provide a uniﬁed view of existing theories thatarecoveredbyothers,suchaswell-formedness, thatissuitableforqualityassessment. Weaimfor which merely reﬁnes the acceptability part of co- acommonunderstandingofthedimensionsthataf- gency(Govier,2010). </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thethreemostagreeingannotatorsfor bythethreeannotatorswas“cannotjudge”(forglo- eachdimensionachievedmuchhigherα-valuesbe- balrelevance),suggestingthatourguidelineswere tween.23(clarity)and.60(credibility),though.5 comprehensive. Regarding agreement, we see in Thestudyresultswerediscussedbyallannota- Table 3(b) that the α-values of all logical and di- tors, leading to a considerably reﬁned version of alecticalqualitydimensionsexceptforglobalsufﬁ- theguidelines. Wethenselectedthreeannotators ciencylieabove0.4forthemostagreeingannotator forthecorpusannotationbasedontheiravailability. </Extractive Summary>  </Table ID = 3>  </Paper ID = 779> 

<Paper ID = 780>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Accuracy and differences between POS NN→NE 9.26 taggers ADV→ADJD 6.09 KOUS→PWAV 4.06 by roughly 6 points in accuracy, but almost dou- Changederrors: 8.15%(218/2674) blesthenumbersofcorrectlytaggedsentences. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Token-based label changes comparing from the response of Clevertagger (9.41). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Coreference resolution evaluation (F1) K butoneinS. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  <Table ID = 9>  <Abstractive Summary> =  Table 9: POS tagging upper bounds and accuracy (HOTCoref→Stanford) (highest scores in green; lowest in red; middle in yellow) We see that less than 50% of the changes that The Stanford tagger, despite performing the the Stanford system introduces are corrections lowest with respect to overall accuracy, achieves (44.62%). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 9>  <Table ID = 1>  <Abstractive Summary> =  Table 11: Mention-based coreference perfor- spectively. Table 10: Parsing accuracy (LAS) and upper puts. Table 12: Largest accuracy differences between TreeTagger (TT) and Clevertagger(CT); number oftokenwithPOStaginthetestset(#tok) A.2 Coreferenceresolution SincetheARCSframeworkisrelativelyunknown and not widely used, we revisit the connection of our diff metric to accuracy and F1 outlined in section 2 in order to use one of the corefer- ence metrics to establish the differences between the outputs. </Abstractive Summary>  <Extractive Summary> =  The resulting F1 score can then be usedasanagreementvalue,which,however,does not provide any detailed analysis of the nature of the differences compared to the ARCS approach. Table 13 shows the F1 scores when using one re- sponseasthekeyandthesecondasresponse. Note that switching the key and the response role pro- vides the same F1 scores for two responses; the only effect is that the recall and precision values areswitched. </Extractive Summary>  </Table ID = 1>  </Paper ID = 780> 

<Paper ID = 781>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  2 EvaluationMetrics METEOR addresses several deﬁciencies of BLEU such as recall evaluation and the lack of explicit A summary of the metrics investigated in our wordmatching. n-grambasedmeasuresworkrea- study is given in Table 1. All these metrics ex- sonably well when there is a signiﬁcant overlap 200between reference and candidate sentences; how- dog ever they fail to spot semantic similarity when mouth stick in the common words are scarce. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Seetextfordetails. Description BLEU METEOR ROUGE CIDEr SPICE WMD original amanwearingaredlifejacketissittinginacanoe 1 1 1 10 1 1 onalake candidate amanwearingalifejacketisinasmallboatona 0.45 0.28 0.67 2.19 0.40 0.19 lake synonyms aguywearingalifevestisinasmallboatona 0.20 0.17 0.57 0.65 0.00 0.10 lake redundancy amanwearingalifejacketisinasmallboatona 0.45 0.28 0.66 2.01 0.36 0.18 lakeatsunset wordorder inasmallboatonalakeamaniswearingalife 0.26 0.26 0.38 1.32 0.40 0.19 jacket ple case in Table 2. In this table, an original cap- Candidate1 A guy with a red jacket is standing on a boat . </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  tioning models in (Aditya et al., 2015; Karpathy Williams test (Williams, 1959) calculates the andFei-Fei,2015). statisticalsigniﬁcanceofdifferencesindependent Table 3 shows Pearson’s, Spearman’s and correlations,andformulatedastestingwhetherthe Kendall’s correlation of the metrics with the hu- populationcorrelationbetweenX andX equals 1 3 man judgements in FLICKR-8K and COMPOSITE thepopulationcorrelationbetweenX andX : 2 3 datasets. For FLICKR-8K, we follow the method- p ology in (Elliott and Keller, 2014) and compute (r −r ) (n−1)(1+r ) t(n−3) = q 13 23 12 correlationswiththehumanexpertscores. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ferentlevelofdifﬁculty. In Table 4, we present caption-level classiﬁca- ABSTRACT-50S(Vedantametal.,2015)dataset tion accuracy scores of automatic evaluation met- is a subset of the Abstract Scenes Dataset (Zit- rics at matching human consensus scores. On nick and Parikh, 2013), which includes 500 im- agescontainingclipartobjectsineverydayscenes. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In addition, racy score. In Table 5, we present the classiﬁca- weshouldaddthatthisuniﬁedmetricsigniﬁcantly tion accuracies of the evaluation metrics for each outperforms the individual metrics according to distraction type. As can be seen, the WMD met- Williamstest(p < 0.01). </Extractive Summary>  </Table ID = 5>  </Paper ID = 781> 

<Paper ID = 782>  <Table ID = 1>  <Abstractive Summary> =  Table 1: WMT15 Test Data Statistics grouped by mance of other MT quality evaluation metrics by source languages. </Abstractive Summary>  <Extractive Summary> =  of news texts. Table 1 shows the statistics for the HT texts) are selected from the WMT15 dataset test data. The target language is English for all basedonthelowest(5)rankingsbyhumanjudges. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Domains of the test data are the same for all languages except for French. 3.2 Features ThetestdatafortheFrench-Englishlanguagepair Table 2 provides examples of MT errors in com- wasfetchedfromanewsdiscussionforuminstead parison to HT. All example translations (MT vs. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Pearson’s r correlation between Trueskill scores of a metric and human judgments with the corresponding95%conﬁdenceintervalsareshown. </Abstractive Summary>  <Extractive Summary> =  of the features and human rankings. Therefore, we also investigate whether we can predict hu- Results Table 3 shows all the Pearson’s corre- manrankingsofMTtranslatedtextbycombining lations. Overall, Formality-RB obtains the high- these features since they capture different aspects est correlation score (80.7%) among all features. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Pearson’s r correlation between Trueskill scores of a metric and human judgments with the corresponding 95% conﬁdence intervals are shown. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 782> 

<Paper ID = 783>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Unlabeled Attachment Score on the UD ce Hindi n test data for TurboParser and Tensor-LSTM with re Persian crossentropyloss. </Abstractive Summary>  <Extractive Summary> =  Another explanation could be that Figure 5: Performance for Tensor-LSTM on En- some feature of the cross entropy loss function glishtestdatawith0-40%oftheedgescoresarti- makesitespeciallywellsuitedforLatinlanguages ﬁciallymaintainedat0. – as seen in Table 1, French and Spanish are alsotwoofthelanguagesforwhichTensor-LSTM As can be clearly seen, performance drops yieldsthehighestperformanceimprovement. faster with the percentage of deleted labels for To compare the effect of missing edge scores the cross entropy model. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Unlabeled attachment scores for the various systems. </Abstractive Summary>  <Extractive Summary> =  French and Spanish, however, 50 do not follow the same trend, with cross entropy S loss outperforming mean squared loss despite the A U highnumberofmissinglabels. 48 In Table 2, performance on French and Span- 46 ish for both systems can be seen to be very high. It may be the case that Indo-European target lan- 44 guages are not as affected by missing labels as 0 10 20 30 40 mostofthesourcelanguagesarethemselvesIndo- %blankout European. This conﬁrms our in- uponperformancewithoutinﬂuencefromlinguis- tuition that the initially lower performance us- tic factors such as language similarity, we repeat ing mean squared loss compared to cross entropy thecross-lingualexperimentononelanguagewith loss is mitigated by a greater robustness towards respectively10%,20%,30%,and40%ofthepro- missing labels, gained by circumventing the de- jected and averaged edge scores artiﬁcially set to coding step in the training process. In Table 2, 0,simulatingmissingdata. WechoosetheEnglish thisisreﬂectedasdramaticperformanceincreases data for this experiment, as the English projected using mean squared error for Finnish, Persian, data has the lowest percentage of missing labels Hindi, and Hebrew – the four languages furthest 226removed from the predominantly Indo-European mine and ﬁlter out high-uncertainty trees. </Extractive Summary>  </Table ID = 2>  </Paper ID = 783> 

<Paper ID = 784>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Matrix representation of the directed words of the input test section receive the FUNC- graphforthewordsinthesentence. </Abstractive Summary>  <Extractive Summary> =  6. A function κ(h,d) that returns whether the dependency (h,d) has a valid attachment di- Table 4 shows the directed multigraph used for rectiongiventhePOSofthed(cf. Sec.3.2). </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  thecanonicalUDtestsets. Table 3 shows a trace of the algorithm, with C = hhad,connection,extremists,speciali and 5.1 Baseline F = {They,also,a,to,some}. We compare our UDP system with the perfor- it word h H mance of a rule-based baseline that uses the head 1 had root ∅ 2 connection had {had} rules in Table 5. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: UAS for baseline with gold POS (BL ) G wouldbedoubled,aseitherawronglytaggedhead with direction (L/R) for backoff attachments, or dependent would break the dependency rules. </Abstractive Summary>  <Extractive Summary> =  Table 3 shows a trace of the algorithm, with C = hhad,connection,extremists,speciali and 5.1 Baseline F = {They,also,a,to,some}. We compare our UDP system with the perfor- it word h H mance of a rule-based baseline that uses the head 1 had root ∅ 2 connection had {had} rules in Table 5. The baseline identiﬁes the ﬁrst 3 extremists had {had,connection} verb(orﬁrstcontentwordiftherearenoverbs)as 4 special connection {had,connection,extremists} themainpredicate,andassignsheadstoallwords 5 They had {had,connection,extremists,special} 6 also had ... bor,accordingtotheestimatedruntimeparameter. We report the best head direction and its score The ﬁrst four iterations calculate the head of for each language in Table 5. This baseline ﬁnds contentwordsfollowingtheirPR,andthefollow- the head of each token based on its closest possi- ing iterations attach the function words in F. This gives French 47.1R 64.5 72.7 70.6 62.1 36.3 German 48.2R 60.6 66.9 62.5 57.0 24.2 usanindicationhowoursystemcomparestostan- Gothic 50.2L 57.5 61.7 59.2 55.8 34.1 Greek 45.7R 58.5 68.0 66.4 57.0 29.3 dardcross-lingualparsers. Hebrew 41.8R 55.4 62.0 58.6 52.8 35.7 Hindi 43.9R 46.3 34.6 34.5 45.7 27.0 Hungarian 53.1R 56.7 58.4 56.8 54.8 22.7 5.3 Results Indonesian 44.6L 60.6 63.6 61.0 58.4 35.3 Irish 47.5R 56.6 62.5 61.3 53.9 35.8 Table 5 shows that UDP is a competitive system; Italian 50.6R 69.4 77.1 75.2 67.9 37.6 because UDP is remarkably close to the super- Latin 49.4L 56.2 59.8 54.9 52.4 37.1 G Norwegian 49.1R 61.7 70.8 67.3 58.6 29.8 vised MSD system, with an average difference Persian 37.8L 55.7 57.8 55.6 53.6 33.9 G Polish 60.8R 68.4 75.6 71.7 65.7 34.6 of 6.4%. Notably, UDP even outperforms MSD Portuguese 45.8R 65.7 72.8 71.4 64.9 33.5 ononelanguage(Hindi). Table 7: Average language-wise domain evalua- However, if we regard the results for UDP in tion. We report average UAS and standard devi- G Table 5, we can see that there are 24 languages ation per language. The bottom row provides the (out of 32) for which the parser performs better averagestandarddeviationforeachsystem. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Evaluation across domains. </Abstractive Summary>  <Extractive Summary> =  Ingeneral,therule-basedUPD Cross-domain test sets To further assess the is less sensitive to domain shifts than the data- cross-domainrobustness,weretrievedthedomain driven MSD counterpart, conﬁrming earlier ﬁnd- (genre)splitsfromthetestsectionsoftheUDtree- ings(PlankandvanNoord,2010). banks where the domain information is available Table 6 gives the detailed scores per language as sentence metadata: from Bulgarian, Croatian, and domain. From the scores we can see that andItalian. </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Note these three pervised parsers require around 100 sentences to small datasets are not included in the results on reach UDP-comparable performance, namely a thecanonicaltestsectionsinTable5. mean of 300 sentences and a median of 100 sen- Table 7 summarizes the per-language average tences, with Bulgarian (3k), Czech (1k), and Ger- scoreandstandarddeviation,aswellasthemacro- man (1.5k) as outliers. The difference between averaged standard deviation across languages. </Extractive Summary>  </Table ID = 7>  </Paper ID = 784> 

<Paper ID = 785>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Whenseveral corporaareavailableforalanguage,wepickedthe 3Spectral-based dimension reduction such as PCA are standard one. Table 1 provides some basic statis- limitedbythenumberofeigenvectorsofthematrixtobere- tics on the language datasets. Also note that our duced.Forexampleamatrixofsize200×500canatmostbe reducedintoa200×200matrixviaPCA.Whenthenumber experiments follow the train/dev/test split as pro- ofeigenvectorsissmallerthan500,weusethatvalueinstead. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Best UAS scores in cross-lingual setting. </Abstractive Summary>  <Extractive Summary> =  Attachment Scores (UAS) measured on the test 4.5 Cross-lingualExperiments sets ignoring the punctuation marks. As a base- linecomparisonweuseourimplementationofthe Table 3 summarizes the UAS scores achieved us- MSTparser without morpho-syntactic attributes ing delexicalized embeddings learned on several representation of any kind. We computed the sig- languages. </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Best UAS scores for each embedding type in monolingual setting. </Abstractive Summary>  <Extractive Summary> =  guages (en, eu, hu, ro) in the cross-lingual set- ting compared to the best monolingual setting. 4.4 MonolingualExperiments Whilethemultilingualembeddingsdonotoutper- Table 2 displays UAS scores for the monolin- form the monolingual ones for the other four lan- gual setting. Except for French and Romanian guages,theystilldeliverparsingperformancethat that do not show real improvement, the six other are better than with the baseline MST parser for languagesshowsubstantialperformanceincreases alllanguages(butGothic). </Extractive Summary>  </Table ID = 2>  </Paper ID = 785> 

<Paper ID = 786>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  arguments. However, as illustrated in Table 1, Weassumethatcincludesaclaimtargetx ,de- claims typically refer to the pros and cons of the c ﬁned as a phrase about which c makes a positive topic target, but do not entail or contradict the or a negative assertion. Speciﬁcally, it is deﬁned topic. Stance(c,t) = sc×R(xc,xt)×st (1) where Stance(c,t) ∈ {−1,1}, 1 indicates Pro 3 TheClaimPolarityDataset and −1 indicates Con. Rows 1-8 in Table 1 show examples for x , s , x , s and R(x ,x ). It is The IBM argumentative structure dataset pub- c c t t c t easytoverifythatthemodelcorrectlypredictsthe lished by Aharoni et al. We required absolute majority agreement (≥3)fors andR(x ,x ),otherwisetheclaimwas c c t We assessed the validity and applicability of the labeledasincompatiblewithourmodel. proposedmodelthroughmanualannotationofthe Rows 1-8 in Table 1 show some examples of IBM dataset.5 The labeled data was also used to annotated claims in our dataset. Row 9 is an ex- trainandassesssub-componentsinthemodelim- ampleofaclaimthatwasfoundincompatiblewith plementation. Here we address a Sentimentmatching: Positiveandnegativeterms more general problem of open domain, generic fromthesentimentlexiconofHuandLiu(2004a) target identiﬁcation. Table 1 illustrates the diver- arematchedintheclaim. sityandcomplexityofclaimtargets. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Features extracted for a target candidate presents in more detail our novel contrast detec- xinaclaimc. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ployed here a simpliﬁed version of the sentiment analyzer (cf. Section 6.2), in which target identi- 8.2 Results,AnalysisandDiscussion ﬁcationisnotperformed,andsentimenttermsare The results are shown in Table 3. Comparing the weighted uniformly. </Extractive Summary>  </Table ID = 3>  </Paper ID = 786> 

<Paper ID = 787>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Five are more likely to be of the tweet only. </Abstractive Summary>  <Extractive Summary> =  The irony occurs because tic literature. Several categories have been pro- thewriterbelievesthathisaudiencecandetectthe posed, as shown in the ﬁrst column of Table 2. disparity between P and P on the basis of con- 1 2 Since all these categories have been found in a textualknowledgeorcommonbackgroundshared speciﬁc genre (literary texts), the ﬁrst step was with the writer. −→ P2: Serge Dassault is involved and has been sentencedinmanycourtcases. 3.2 Ironycategories 3.3 Ironymarkers Both explicit and implicit activation types can be expressed in different ways which we call irony As shown in Table 2, linguistic literature consid- categories. After a thorough inspection of how ers other forms of irony categories, such as sur- categories have been deﬁned in linguistic litera- prise effect, repetition, etc. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  next section). This is an important re- Table 4 gives the percentage of tweets belong- sult that shows that annotators are able to iden- ing to each category of irony split according to tify which are the textual spans that activate the explicit vs. implicit activation, when applicable. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Besides standard frequencies, nations are false assertion/other and false asser- we provide the correlations between irony activa- tion/hyperbole for both English and French; and tiontypesandmarkersandbetweencategoriesand analogy/otherforEnglish4. markersinordertobringoutfeaturesthatcouldbe Table 5 provides the percentage of tweets con- usedinaperspectiveofautomaticironydetection. tainingmarkersforironic(explicitorimplicit)and In each corpus, all the frequencies presented here non ironic tweets (row in gray). </Extractive Summary>  </Table ID = 5>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In French, in- arestatisticallysigniﬁcantfromwhatwouldbeex- tensiﬁers, punctuationmarksandinterjectionsare pectedbychanceusingtheχ2 test(p < 0.05). morefrequentinironictweetswhereasquotations Table 3 gives the total number of annotated are more frequent in non ironic tweets. In En- tweets and the activation type for ironic tweets. </Extractive Summary>  </Table ID = 3>  </Paper ID = 787> 

<Paper ID = 788>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  On the other hand, being only a NLP expert hashtags, can be particularly helpful when per- 276formingaspect-basedsentimentanalysis. An analogous consideration can be drawn for Regarding emojis, the most frequent ones are the emojii distribution (see Table 2). It turns out (asexpected)sentiment-driven,i.e. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  lated to the context where they appear: their con- tribution in terms of conveyed sentiment (or con- veyedtopic)strictlydependsonthedomainwhere they are used. In Table 3, we report a compari- Table1: Labeldistributionperannotator sonbetweenthelabeldistributionoftwoemojisin A A A 1 2 3 Subjective 0.671 0.748 0.657 ourcorpusandthecorrespondingdistributionina Objective 0.330 0.252 0.343 state of the art emoji sentiment lexicon (Novak et None 0.330 0.252 0.343 al.,2015). Positive 0.509 0.476 0.426 In the proposed corpus, the ﬁre emoji has been Neutral 0.038 0.146 0.131 Negative 0.123 0.126 0.100 mainly labelled as positive because it represents None 0.330 0.252 0.343 the word “hot”, whose meaning is intended as Explicit 0.254 0.512 0.416 something beautiful and trendy. </Extractive Summary>  </Table ID = 3>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  As conclusion, any emoji should be Anger 0.014 0.034 0.028 not considered as independent on the context and Trust 0.008 0.034 0.022 thereforeevaluatedaccordingtoitssemantic. Fear 0.001 0.001 0.002 5.1 AgreementMeasures In Table 1, we report some statistics that sum- marize behaviours of the involved annotators. By The kappa coefﬁcient (Cohen, 1960) is the most analysingthedistributions,wecanobservediffer- usedstatisticformeasuringthedegreeofreliabil- entattitudes: A isinclinedtolabelmorepostsas ity between annotators. </Extractive Summary>  </Table ID = 1>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Inordertoestimatetheuncertaintyofthe annotation of each labeller, we sampled a portion Amorereliablemeasureforestimatingtheagree- of tweets to be annotated twice by the same an- ment among annotators is PABAK-OS (Parker et notator. We report in Table 5 the self-agreement al., 2011), which controls for chance agreement. measure,thatisavalidindextoquantifythequal- PABAK-OSaimstoavoidthepeculiar,unintuitive ityofthelabellingprocedure. </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Theannotatorscanbeconsid- lenceofagivenlabel). eredmoderatelyreliableforimplicit/explicitanno- We report in Table 4, the inter-agreement be- tationsandveryaccuratefortheremaininglabels. tweencouplesofannotatorsdistinguishedforeach label. </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ing only the ironic messages. From the results, We described the construction of the corpus, to- reported in Table 6, we can conﬁrm that A and getherwithannotationschema,statisticsandsome 1 A annotators are more willing to interpret irony interestingremarks. Theproposedcorpusisaimed 2 similarly(asalreadystatedinTable4). </Extractive Summary>  </Table ID = 6>  </Paper ID = 788> 

<Paper ID = 789>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Our best feedforward variant signiﬁ- tunethenumberofhiddenlayersandhiddenunits cantly outperforms the systems with surface fea- on the development set. </Abstractive Summary>  <Extractive Summary> =  The test set is not yet available 3,000BrownclustersontheGigawordcorpus. at the time of submission, so the system is eval- Table 5 shows the results for the models which uated based on the average accuracy over 7-fold arebesttunedonthenumberofhiddenunits,hid- cross-validation on the combined set of training den layers, and the types of word vectors. The anddevelopmentsets. </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Comparing various systems on the CoNLL 2016 Shared Task standard datasets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 789> 

<Paper ID = 790>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Number of documents (#Doc), trees (#Trees, less than #Doc when we were unable to parse a document,seeSection4.2),words(#Words,seeSection6),relations(#Rel,originally),labels(#Lab,re- lationandnuclearity),EDUs(#EDU,max/min/avgnumberofEDUsperdocument),andCDUs(#CDU). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Dictionary coverage for each dataset on could also contain discourse connectives, adverbs thetrainsetwhenavailable,onthedevsetelse. </Abstractive Summary>  <Extractive Summary> =  Position and length Other features are used to The word features for the non-English datasets representthepositionoftheEDUinthedocument aretranslatedusingavailablebilingualWiktionar- anditslengthintokens. Weusethresholdstodis- ies19withoutdisambiguation,thecoverageofeach tinguishbetweenverylong(lengthl > 25tokens), dictionary is given in Table 2. We also look for a long(l > 15),short(l > 5)andveryshort(l ≤ 5) translation of the lemma (and of the stems for the EDUs. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Performance of our monolingual and cross-lingual systems for Span (Sp), Nuclearity (Nuc) and Relation (Rel). </Abstractive Summary>  <Extractive Summary> =  CONDITION+3%). Table 3. Our parser is competitive with state-of- When combining corpora for source and target the-art systems for English (ﬁrst line in Table 3), languages(“+dev.”inTable3),weobtainourbest with even better performance for unlabeled struc- performingsystemforEnglish,withallscoresim- ture (85.04%) and structure labeled with nuclear- proved compared to our best monolingual system ity(72.29%). Table 3. Our parser is competitive with state-of- When combining corpora for source and target the-art systems for English (ﬁrst line in Table 3), languages(“+dev.”inTable3),weobtainourbest with even better performance for unlabeled struc- performingsystemforEnglish,withallscoresim- ture (85.04%) and structure labeled with nuclear- proved compared to our best monolingual system ity(72.29%). Theseresultsshowthatusingallthe (+0.8forNuclearityand+1.3forRelation). 313695. (“+emb” in Table 3) for monolingual systems of- tenleadstoanimportantdropinperformance,es- References peciallyforRelation(from−1.1to−4.2%). This demonstrates that these embeddings do not pro- Farah Benamara and Maite Taboada. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Mapping of all the relations found in the datasets: for each class, we give the set of relation names as they appear in the corpora (removing only the possible sufﬁxes “-e”, “-s”, “-mn”). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 790> 

<Paper ID = 791>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In such way, tigated. Table 1 give an example of each of the weleveragethedatasetofDSTC-2tocreatemore questiontypepresentedbelowonadialogsample complex reasoning task than the ones present in ofDSTC-2corpus. the original dialogs of the dataset by performing Inference procedure: Concretely, the current rule-based modiﬁcation over the corpus. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: One supporting fact task : Acc. </Abstractive Summary>  <Extractive Summary> =  dialogaugmentationstrategyallowstoexhibitthe 4.3 Experimentalresults competencyoftheproposedmodelbeyondfactoid questions. Table 3 presents tracking accuracy obtained for three variables of the DSTC2 dataset formulated 4.2 TrainingDetails as Factoid Question task. We compare with two Assuggestedin(Sukhbaataretal.,2015),10%of established utterance-level discriminative neural the set was held-out to form a validation set for trackers, a Recurrent Neural Network (RNN) hyperparameter tuning. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Reasoning tasks : Acc. </Abstractive Summary>  <Extractive Summary> =  ob- approach offers several advantages compared to tainedonDSTC2testset stateoftheartmethodsoftracking. First,thepro- posed method allows to perform tracking on the As a second result, Table 4 presents the perfor- basis of segment-dialog-level annotation instead mance obtained for the four reasoning tasks. The of utterance-level one that is commonly admitted obtained results lead us to think that MemN2N are in academic datasets but tedious to produce in a a competitive alternative for the task dialog state large scale industrial environment. </Extractive Summary>  </Table ID = 4>  </Paper ID = 791> 

<Paper ID = 792>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We randomly selected speech disﬂuences intact. Demographic informa- 60 texts from this dataset, forcing only the con- tion for participants in our study is presented in dition that the number of sentences of each text Table 1. A third dataset was used in robustness sentencewasgreaterthan12. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Table3summarizestheresults. generated randomly from a gaussian distribution scaled by fan in + fan out (Glorot and Bengio, From Table 3 we can see that our approach 2010). Both embeddings matrix E and E presents better results for the Constitution dataset word tag were adjusted during training. </Extractive Summary>  </Table ID = 3>  </Paper ID = 792> 

<Paper ID = 793>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ing the RNN on the rarer types. Whilst the prob- The utility of a joint task As can be seen in lem is attenuated by the memory facility of the Table 3, the overall best performing systems on LSTM, our best system still suffers the vanish- theindividualtasksdonotreachtheresultsinany inggradientproblemforpredictinglongerrepairs relevant metric of the best performing combined with reparanda over 3 words long. Also we show system. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  WealsoinvestigatedtheuttSegdetectionerrors Incrementality Incrementally the differences and see that the networks are generally not con- betweenthearchitectureswasneglible–resultsfor fusingdisﬂuencieswithboundaries. However,our the LSTM are in Table 4. The latency for repair best system incorrectly labelled 3.6% of the ref- onsetdetectionisverylow,beingdetectedaslittle erence uttSegs as rpS (hence also affecting the as 0.196 seconds after the onset word is ﬁnished precisionoftherpS prediction)–uponinspection (or on transcripts largely directly after the word these were largely abandoned utterances, which has been consumed as TTD (word) = 0.003). </Extractive Summary>  </Table ID = 4>  </Paper ID = 793> 

<Paper ID = 794>  <Table ID = 9>  <Abstractive Summary> =  Table 9: Results on test languages. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 9>  </Paper ID = 794> 

<Paper ID = 795>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Distribution of sentences from the thattopics2,4,7,8,9,and10containmainlyGE- WSJ+GENIA data set given 2 and 10 topics NIA sentences while the remaining topics cover (showing the percentage of GENIA sentences per mainly WSJ sentences. </Abstractive Summary>  <Extractive Summary> =  HereweinvestigatewhetherLDAcanseparatethe For the 10-topic setting, the topic expert model sentences into meaningful topics. Table 1 shows outperforms the random split of the same size by thedistributionofsentencesinthetrainingandtest 0.9 percent points, which is a higher difference set into different topics when we assume 2 or 10 than for the 2-topic setting. This shows that the topics. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We also perform a soft clustering exper- createasecondbaselinebysplittingthesentences iment, in which each sentence is added to every randomly into the same number of groups as the topic,weightedbyitsprobability(seesection4.2). numberoftopics,whilemaintainingtheequaldis- The results in Table 2 show that if we assume tribution of WSJ and GENIA sentences. I.e., we a 2-topic setting, the experts perform better than assume the same number of random “topics”, all both baselines, i.e., the model trained on the full ofthesamesize. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Weusetherandomsplitcaseasthelowerbase- 5.4 AnalysisofResults linefortheseexperimentsandthefulltrainingset We now have a closer look at the results pre- as the more competitive baseline. Table 4 shows sentedfortheparsingexperimentsusinggoldPOS theresults. tags in section 5.3.1. tags in section 5.3.1. The results show that the 2-topic parsing experts outperform the general parser trained on the full training set by almost Table 4 shows that in the 2-topic setting, us- 2 percent points. We looked at the 5 sentences ing topic modeling experts on the POS level as that had the lowest LAS when we used the gen- wellasontheparsinglevelreachesthehighestre- eralparser. </Extractive Summary>  </Table ID = 4>  </Paper ID = 795> 

<Paper ID = 796>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Dependency parsing results on the Hungarian Universal Dependency dataset. </Abstractive Summary>  <Extractive Summary> =  To default parameters for both the MarMot and the empiricallyinvestigatethisinthecaseofHungar- Bohnetparser. ianUD,wecomparedtheconvertedandthemanu- Table 1 presents unlabeled (UAS) and labeled allycorrected,i.e. goldstandard,treesofthe1800 (LAS) attachment scores achieved by the parser sentences. 31 9.94 32 9.91 dencylabelsatevaluations. AdjacentN/A 15 4.81 20 6.19 Subordination 13 4.17 17 5.26 Table 1 shows the MSD outperforms UM con- Copula 14 4.49 11 3.41 sistently at each of the experiments. Although Inﬁnitive 9 2.88 15 4.64 these differences are not high, this suggests that Nominalarg. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Error analysis: the number and ratio of greatest difference when training and evaluating speciﬁcerrortypes. </Abstractive Summary>  <Extractive Summary> =  As the trainedbyusingpredicteduniversalcodesandpre- numbersinTable1show,slightlybetterresultscan dicted MSD morphological codes, respectively. be achievedboth interms ofUAS andLAS when Results are presented in Table 2. We found that trainingthemodelwithfulllabelsthanwithmain the beneﬁts of the original language-speciﬁc an- labels. </Extractive Summary>  </Table ID = 2>  </Paper ID = 796> 

<Paper ID = 797>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Re-Categorization impact on develop- be categorized into types -VERB- and -SURF-, mentset doesn’t improve the baseline system. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  There are two ex- isting AMR aligners: one is a rule-based aligner conﬁguration to the condition NO-RELATION- comingwithJAMR(Flaniganetal.,2014),which ALIGN where we only ignore the alignment be- tween sentence and AMR relations by putting an deﬁnes regular expression patterns to greedily all zero vector as the reference attention for each match between AMR graph fragment and in- output relation label. From Table 3 we see that put token spans; another one is an unsupervised simply ignoring the reference attention for rela- aligner (Pourdamghani et al., 2014) which adopts tionswouldgreatlyaffectthemodelperformance, thetraditionalwordalignmentmethodinmachine and how we effectively represent the reference translation. Although evaluated on different set alignmentforrelationsiscrucialforthesupervised of manual alignment test sentences, both aligners attentionmodel. </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Our model can also be re- garded as a word-level model; however, by uti- Table2: Supervisedattentionimpactondevelop- lizingcarefullydesignedcategorizationandsuper- mentset visedattention,oursystemoutperformsboththeir resultsbyalargemargin. As shown in Table 2, the supervised attention modelisabletofurtherimprovetheSmatchscore System P R F byanother2points,whicharemainlycontributed Oursystem 0.55 0.50 0.52 by 3 points increase in recall. Since the refer- BarzdinsandGosko(2016)† - - 0.37 BarzdinsandGosko(2016)? - - 0.43 ence/externalalignmentismostlybetweenthein- put tokens and AMR graph concepts, we believe Table 4: Compare to other sequence-to-sequence thatthesupervisedattentionmodelisabletocon- AMR parser. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Compare to other sequence-to-sequence thatthesupervisedattentionmodelisabletocon- AMR parser. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  parser. System P R F SuperAttn+Cate(n=50) 0.56 0.49 0.52 Table 5 gives the comparison of our system NO-RELATION-ALIGN 0.46 0.40 0.43 to some of the teams participating in SemEval16 Task 8. Since a large portion of the teams ex- Table3: Supervisedattentionimpactondevelop- tend on the state-of-the-art system CAMR (Wang mentset et al., 2015b; Wang et al., 2015a; Wang et al., 2016), here we just pick typical teams that rep- Because we have relations in the AMR graph, resent different approaches. </Extractive Summary>  </Table ID = 5>  </Paper ID = 797> 

<Paper ID = 798>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We propose an approach †ThisworkwasdonewhiletheauthorwasapartofIBM ResearchIndia 1https://www.freebase.com/ 376 Proceedingsofthe15thConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics:Volume1,LongPapers,pages376–385, Valencia,Spain,April3-7,2017.(cid:13)c2017AssociationforComputationalLinguisticsPredicate CEO a meaningful question. For example, for the set Subject SundarPichai of keywords shown in Table 1, it is not possible Object Google tousethesubset{person,designation}toforman ParentPredicate designation interestingquestion. Hence,weneedtoautomati- Domain person callyidentifytherightsetofkeywordsthatshould Range organization be used to form the question such that the answer CEO,designation, alsoliesintheset. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Examples of 5-tuples (subject, domain, pair. </Abstractive Summary>  <Extractive Summary> =  Inthiswork,weconsideronlyasingle 5-tuple to generate a QKA pair. For example, we Figure 1: Triples with the entity London in a cangenerateQKApairlike({Capital,City,United knowledgegraph Kingdom}, London) using Column A of Table 2. ButwewillnotgenerateQKApairlike({Capital, since London is the answer to the above ques- City, United Kingdom, Birth Place, Stephen Wol- tion, ({Capital, City, United Kingdom}, London) fram},London)usingbothColumnA&BofTable together will form a Question Keyword and An- 2. If p is not unique for sub(p ), then Domain Country Person Location i i i Predicate Capital BirthPlace Contains there could be multiple possible answers to Object London London BuckinghamPalace thegeneratedquestionincludingobj(p ),and Range City Location Location i therefore we do not generate such a QKA Table 2: Examples of 5-tuples (subject, domain, pair. When this is applied to Column A of predicate,object,range)fortheentityLondon Table 2, we generate ({Capital, City, United Kingdom}, London) as a QKA pair. There Inordertoretrieveinformationaboutthegiven is no QKA pair generated for Column C us- entity E, we need to ﬁrst identify the node n that ing this rule as London contains many loca- represents the entity E in the KG. allpredicateswhosesubjectorobjectisn. Figure Similar to unique forward relation, this rule 1 shows the entity London with three neighbours can be applied to Column A of Table 2 and United Kingdom, Stephen Wolfram and Bucking- cannotbeappliedtoColumnB&C. ham Palace. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Automatic Evaluation (column 2): The considered a part of the output vocabulary. </Abstractive Summary>  <Extractive Summary> =  The results are presented pairsextractedfromWikiAnswers. in Table 3. Both K2Q-RNN and K2Q-PBSMT clearly outperform the template based method 4.3 Evaluationmetrics whichshowsthatthereismeritinformulatingthis We evaluate the performance of K2Q RNN with problemasasequencetosequencelearningprob- other baselines to compare the K2Q approaches, lem. </Extractive Summary>  </Table ID = 3>  <Table ID = 6>  <Abstractive Summary> =  Table 6: PARASEMPRE Evaluation: when dealing with triples such as ({also known WQ=WebQuestions, GQA=Generated QA pairs as, Andre Agassi}, Agassi). </Abstractive Summary>  <Extractive Summary> =  the training set). Table 6 then compares the same Automation of question generation from sentences. system trained on the following different training InProceedingsofQG2010: TheThirdWorkshopon sets: (i) Only Web Questions (WQ) dataset (ii) QuestionGeneration,pages58–67. </Extractive Summary>  </Table ID = 6>  </Paper ID = 798> 

<Paper ID = 799>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Then we continue the depth-ﬁrst search by τ fortheiroraclesummaries. calling the procedure the FINDORACLE (lines 15 Since the systems in Table 2 were developed twoe1d7o).nIfotRu\OpUdGatEenτ(Ran,dVO)isblaurtgreeretnhtaenrτth(ecadseept3h)-, over many years, we compared the ROUGEn τ scores of the oracle summaries with those of the ﬁrst search by calling the procedure again (lines current state-of-the-art systems using the DUC- 18 to 19). If neither case 1 nor case 3 is true, we 2004corpusandobtainedsummariesgeneratedby delete the last visited sentence from V and return differentsystemsfromapublicrepository1 (Hong tothetopoftherecurrence. Since the ROUGEn scores are summarization,toovercomethelimitation. based on the unweighted counting of n-grams, whenmanysentenceshavesimilarmeanings, i.e., 6.2.2 ROUGEScoresofSummariesObtained many redundant sentences, the number of oracle fromGreedyAlgorithm summariesthathavethesame ROUGEn scoresin- Table 2 also shows the ROUGE1,2 scores of the creases. The source documents of multiple docu- summaries obtained from the greedy algorithm mentsummarizationtasksarepronetohavemany (greedy summaries). </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The repository includes summaries produced by the following seven state-of-the-art 6 Experiments summarization systems: CLASSY04 (Conroy et al.,2004),CLASSY11(Conroyetal.,2011),Sub- 6.1 ExperimentalSetting modular (Lin and Bilmes, 2012), DPP (Kulesza We conducted experiments on the corpora devel- and Tasker, 2011), RegSum (Hong and Nenkova, opedforamultipledocumentsummarizationtask 2014), OCCAMS V (Davie et al., 2012; Conroy in DUC 2001 to 2007. Table 1 show the statistics et al., 2013), and ICSISumm (Gillick and Favre, of the data. In particular, the DUC-2005 to -2007 2009; Gillick et al., 2009). </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In particular, the DUC-2005 to -2007 2009; Gillick et al., 2009). Table 3 shows the re- datasetsnotonlyhaveverylargenumbersofsen- sults. tencesandwordsbutalsoalongtargetlength(the Based on the results, RegSum (Hong referencesummarylength)of250words. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Jaccard Index between both oracle and CLASSY11 .286 .0919 greedysummaries Submodular .300 .0933 DPP .309 .0960 RegSum .331 .0974 scores of the oracle summaries and greedy sum- OCCAMS V .300 .0974 maries,thoseobtainedfromthegreedysummaries ICSISumm .310 .0980 achieved near optimal scores, i.e., approximation Table3: ROUGE1,2scoresforstate-of-the-artsum- ratio of them are close to 0.9. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thedataset’scompressionrate was only 1 to 2%. Thus, under tight length con- Table 5 shows the median number of oracle sum- straints,extractivesummarizationbasicallyfailsto maries and the rates of the reference summaries cover large numbers of n-grams in the reference thathavemultipleoraclesummariesforeachdata summary. Thisrevealsthelimitationoftheextrac- set. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Correlation coefﬁcients between auto- maticevaluationsandhumanjudgmentsonDUC- Table7: Mediannumberofsummariescheckedby 2004corpus eachsearchmethod 6.2.4 SearchEfﬁciency ROUGE2, respectively. </Abstractive Summary>  <Extractive Summary> =  tences. Since both the oracle and system sum- Table 6 shows Pearson’s r and Spearman’s ρ. In maries are sets of sentences, it is easy to check thetable,“F-measure(R )”and“F-measure(R )” 1 2 whether each sentence in the system summary is indicate the F-measures calculated using oracle contained in one of the oracle summaries. </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  However, to enumerate oracle summaries, with the number of summaries that were checked weneededseveralweeksforsometopicsinDUCs inoursearchalgorithm. 2005 to 2007 since they hold a huge number of Table 7 shows the median number of feasible sourcesentences. solutions and checked summaries yielded by our method for each data set (in the case of “sin- 7 Conclusions gle”). </Extractive Summary>  </Table ID = 7>  </Paper ID = 799> 

<Paper ID = 800>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  embeddingsareﬁxedduringtraining. Theembed- We evaluated three different variations of NSE dingsforout-of-vocabularywordsweresettozero show in Table 1. The NSE model encodes each vector. This task tests sentation. the ability of a model to reason about the seman- Table 1 shows the results of our models along ticrelationshipbetweentwosentences. Inorderto withtheresultsofpublishedmethodsforthetask. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  tained85.4%accuracyscore. Thebestperforming Table 2 presents the results of our model and model for this task performs tree matching with the previous models for the task.7 The classiﬁer attentionmechanismandLSTM. withhandcraftedfeaturesisaSVMmodeltrained withasetoffeatures. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Test accuracy for sentence classiﬁcation. </Abstractive Summary>  <Extractive Summary> =  Thewrite/readneuralnetsandthelast 6Weusedtrec evalscripttocalculatetheevaluationmet- linearlayerwereregularizedby50%dropouts. rics Table 3 compares the result of our model with the state-of-the-art methods on the two subtasks. Model MAP MRR Mostbestperformingmethodsexploitedtheparse Classiﬁerwithfeatures(2013) 0.5993 0.6068 ParagraphVector(2014) 0.5110 0.5160 tree provided in the treebank on this task with Bigram-CNN(2014) 0.6190 0.6281 theexceptionoftheDMN.TheDynamicMemory 3-layerLSTM(2016) 0.6552 0.6747 Network (DMN) model is a memory-augmented 3-layerLSTMattention(2016) 0.6639 0.6828 NASM(2016) 0.6705 0.6914 network. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: BLEU scores for English-German trans- movie reviews and 10 different classes and Yelp lationtask. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  duced translation. For an efﬁcient encoding, the Table 4 shows our results. We report two per- attention-based NTM was introduced (Bahdanau formance metrics: accuracy and MSE. </Extractive Summary>  </Table ID = 4>  </Paper ID = 800> 

<Paper ID = 801>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The general trend is that using α ≥ .95,i.e.δ = 0.05. the Gaussian model in TS/PETS requires signif- T,a Table 1 summarises the average number of icantly more queries compared to the hierarchi- queries and the success rates of TS and PETS in cal model as well as the baselines. Needing more combinationwithourhierarchicalBayesianmodel queries compared to the baselines highlights the for F-measure across different margins. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  4.3 SentimentAnalysis We then ask the question whether the number of competing systems is important. Table 2 We consider the task of sentence level sentiment summarisestheaveragenumberofqueriesandthe prediction for medical documents. The aim is to success rate of each algorithm on the competing benchmark systems according to how well they conﬁgurations for the margin 0.05 for varying can predict the polarity of sentences contained in number of systems K ∈ {5,10,20}. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Sentiment classiﬁcation for biomedical age/birthday, gender were removed. </Abstractive Summary>  <Extractive Summary> =  speaking, the model couples the sentiment of the sentences contained in a report with the overall Results. Table 3 presents the results. As seen sentiment of the report. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In International conference on Algorithmic Results. Table 4 presents the results. As seen the learningtheory,pages23–37.Springer. </Extractive Summary>  </Table ID = 4>  </Paper ID = 801> 

<Paper ID = 802>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Perplexities for the baseline models. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Test perplexity results for the best Largebaselinew475 modelsonPTB.Baselineperplexitiesareforsizes Largebaselinew650 w175/w200forasmallmodelandw475/w650for LargeCWc10forwardorder a large model. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Relative change in validation perplexity 62 for models to which random information is added, w.r.t. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 802> 

<Paper ID = 803>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  TheMapTaskcor- pushas13DAtags,includingthe“unclassiﬁable” where the matrix WWW(out), vector bbb(out) and the tag. Table 1(b) shows percentages of the seven parameters of the conversation-level network most frequent tags in the data. We randomly split RNN arelearnedduringthetraining. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  per-conversationlabels). The results in Table 3 show that our model Switchboard. We compare our model’s perfor- outperforms these baselines (statistically signiﬁ- mance with that of the following strong base- cant). </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  arethereforeexcludedfromourcomparison. 4Two studies on MapTask DA classiﬁcation were per- The results in Table 2 show that our model formedunderexperimentalsetupsthatdifferfromours: Ju- outperforms these baselines.3 The higher ac- lia et al. (2010) employed HMM+SVM on text transcrip- tions and audio signals, obtaining an accuracy of 55.4% 2github.com/clab/cnn. </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  negative log-likelihood) on the Switchboard de- velopment set, but small accuracy increases (and Does an RNN at the utterance level help? To negativelog-likelihooddrops)ontheSwitchboard answerthisquestion, wecreateavariant, denoted trainingset—anindicationofover-ﬁtting. Incon- woUttRNN, where attentional coefﬁcients are ap- trast, as seen in Table 5, both models yield a neg- plied directly to the token embeddings. Thus, ligibleornodropinaccuracyontheMapTaskde- Equation4ischangedtoEquation9: velopmentset,whilebothyieldadropinaccuracy X onthetrainingset. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  inTable6. Figure3showshowtheattentionalvectorhigh- The results in Table 6 show that exposure bias lights the most important tokens in sample utter- hasdifferenteffectsonthedifferentvariantsofour ances in the context of the DA-classiﬁcation task. model. </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Sample DA-speciﬁc high-focus tokens anattentionmechanismtofocusonsalienttokens forMapTask. </Abstractive Summary>  <Extractive Summary> =  ThemodelsthatemployaDAconnection fromtheattentionmechanism. to compute the attention signal (HA-RNN, woUt- Table 7 shows the most attended tokens for tRNN, woHid2Attn, woConvRNN) show a slight four classes of DA in MapTask. We compiled improvement in accuracy when using the correct theselistsbycomputingtheaverageattentionthat DA as input, instead of the predicted DA. In con- a token received for all the utterances in a DA trast, wDA2DA shows large improvements when class (we excluded tokens that appear less than 5 using the correct DA (3.5% on Switchboard and times). As shown in Table 7, both important to- 6.8%onMapTask),becomingthebest-performing kens“move”and“yes”inFigure3appearintheir model for both datasets. This improvement may respectiveDAcolumns. </Extractive Summary>  </Table ID = 7>  </Paper ID = 803> 

<Paper ID = 804>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  lstm 89.70 30.60 0.1769 0.1799 cnn rnn-cnn lstm 88.82 58.52 0.2354 0.2429 Fullmodelw/differentdecodingstrategy lstm rnn-cnn lstm 86.34 75.16 0.2184 0.2313 lstm rnn-cnn +weighted 86.04 78.40 0.2222 0.2280 lstm rnn-cnn +att. 90.88 80.02 0.2286 0.2388 lstm rnn-cnn +att.+weighted 90.88 83.82 0.2304 0.2369 Table 2 shows the result of the corpus-based decoding strategy does not provide a signiﬁcant evaluation averaging over 5 randomly initialised improvement in BLEU score but it does greatly networks. TheBaselineblockshowstwobaseline improve task success rate (∼ 3%). </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: A comparison of the NN system with a ratingforcomprehension/naturalnessarebothout rule-basedmodularsystem(HDC). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 804> 

<Paper ID = 805>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Notethatanitemisdeﬁnedasarowinthe second set as ASR-dev and ASR-test. Table 2 lists structureddatarecordandtypicallyincludesmul- the statistics of the data sets. Note that the audio tiple ﬁelds. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Note that the audio tiple ﬁelds. Using Table 1 as an example, there ofaconversationwascollectedasasingleﬁleand arethreeitemstobescored. Onlywhenthemodel then automatically segmented into turns for ASR produces an item that is exactly the same as the decoding. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  4.4 Baselinesystems 4.5 Results First we discuss the performance of our models Wecomparetheperformanceofourneuralmodel onmanuallytranscribeddataandthenexaminethe with baseline models that employ information re- results on ASR recognized data. Table 3 lists the trieval (IR) and phrase-based machine translation experiment results on manually transcribed dev (PBMT)approaches. and test sets. </Extractive Summary>  </Table ID = 3>  </Paper ID = 805> 

<Paper ID = 806>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Breakdown of our dataset by novel and type of quote (uncollapsed). </Abstractive Summary>  <Extractive Summary> =  notation tool developed by the authors. Previ- 3.3 Statistics ously developed tools were either not designed Table 3 shows the statistics of our annotated cor- for the task (BRAT (Stenetorp et al., 2012), pus. Unlike He et al. (2013), we provide the col- annotators were shown short snippets that lacked lapsed statistics as well. As Table 3 shows, we the context to determine the speaker and no char- haveroughlythesamenumberofannotatedquotes acter list. We designed our tool to provide con- for Pride and Prejudice as He et al. </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  A summary of the an- tions and form coreference chains. We ﬁnd that notations included in this dataset and comparison 57.8% of the quotes in this corpus either i) have tothepreviousdatathatwedrawfromisdescribed no speaker label (48.1%) or ii) the speaker can- in Table 2. Our ﬁnal dataset is described in Table not be linked to a known character entity (9.7%). </Extractive Summary>  </Table ID = 2>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Comparison with previous work. </Abstractive Summary>  <Extractive Summary> =  ourcharacterlist,labelthequotewiththatspeaker. 6.2 ComparisontoPreviousWork Coreference Disambiguation If the mention is apronoun,weattempttodisambiguateittoaspe- Table 7 shows a direct comparison of our work ciﬁc character using the coreference labels pro- versus the previous systems. We replicate the test videdbyBookNLP(Bammanetal.,2014). </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  us to create a high precision system that might be Thissievegoesjustbeforetheconversationpat- more appropriate for real-world applications that 467valueprecisionoverrecall. evaluatehowwelloursystemgeneralizestoother The results in Table 8 conﬁrm that the subset novels and also allow us to train better models. oftestquotesfromEmmaandTheSteppeusedin Another interesting direction is to eliminate the previous work were an easier subset of the whole use of predeﬁned character lists by automatically set of quotations. </Extractive Summary>  </Table ID = 8>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  the whole set of quotations, we lose 0.2 and 11.1 points of accuracy for Emma and The Steppe, re- Acknowledgments spectively. As we show in Table 4, The Steppe Thismaterialisbaseduponworksupportedbythe ismissingasigniﬁcantportion(50.9%)ofthean- National Science Foundation Graduate Research notations whereas Emma is missing 28.6%. Our Fellowship Program under Grant No. </Extractive Summary>  </Table ID = 4>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The authors inspecting the accuracy of our system by quote thanktheiranonymousreviewersandtheStanford type. As seen in Table 9, the main challenge lies NLPgroupfortheirhelpfulfeedback. inidentifyinganaphoricandimplicitspeakers. </Extractive Summary>  </Table ID = 9>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 806> 

<Paper ID = 807>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (2015), we report SRL results as identify the correct frame label for each predi- averagesoverthethreeannotations(TW-av). cate, if any, and then to identify the role spans Table 1 shows statistics on these datasets. For asargumentsandadjunctsoftheframe,andtola- TW, it displays the statistics for each annotator. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Training data coverage of test sets in %. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: In-domain system comparison on das- classiﬁeritselfisagnostictothepredicate’spartof test,*denotesresultsfromHermannetal.(2014); speechandexactlemmaandonlyreliesontheword ambig: evaluation on ambiguous predicates; no- representationsfromthevsm. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Out-of-domain frameId, total accuracy. </Abstractive Summary>  <Extractive Summary> =  For domain setting. Table 5 summarizes the results. example, the predicate window was assigned Eachofthesystemswastrainedondas-trainand the frame Connecting architecture instead of testedonavarietyoftestsets. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Error sources for NN+Dep; unk is the largeroomforimprovement. </Abstractive Summary>  <Extractive Summary> =  Althoughchoosingamongframesdeﬁnedin ing observation is that our systems perform al- the lexicon provides a quality boost, it also ren- most as well in the no-lexicon setting as the dersmanyinstancesintractableforthesystem,if DataBaseline,whichhasaccesstothelexicon, the lexicon coverage is incomplete. As Table 6 inthetotalsetting. Tooursurprise,theWSABIE- shows, unknown and unlinked predicates are al- basedframeidentiﬁcationdidnotyieldaconsistent mostnon-presentinthein-domaincase,butarea improvementin-domain, comparedtothesimple major source of errors in the out-of-domain case NN-basedapproach. </Extractive Summary>  </Table ID = 6>  </Paper ID = 807> 

<Paper ID = 808>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Individual c(s ), c(s ) and joint target topic Data set: We evaluate and compare our pro- c(s ,s )distributionsofsentiments target topic posed system to the state-of-the-art baselines on a benchmarking corpus (Dong et al., 2014) that 4 Developingastate-of-the-artapproach hasbeenusedbyseveralpreviousstudies(Voand fortarget-speciﬁcsentiment Zhang, 2015; Tang et al., 2016a; Zhang et al., 2016). </Abstractive Summary>  <Extractive Summary> =  (2016). Table 1 reports the individ- As many as 3,713 tweets have more than a sin- ual c(s ), c(s ) and joint c(s ,s ) target topic target topic gleentitymention(target)pertweet,whichmakes distributions of the target/entity s and topic target the task different from 2015 Semeval 10 subtask s sentiment. While s and s report topic target topic C (Rosenthal et al., 2015) and a target-dependent how often each sentiment category occurs in the benchmarkingdatasetofDongetal.(2014)where dataset,thejointdistributionc(s ,s )(the target topic eachtweethasonlyonetargetannotatedandthus inner portions of the table) shows the discrepan- cies between target and topic sentiments. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Performance comparison on the bench- models proposed by Zhang et al. </Abstractive Summary>  <Extractive Summary> =  ’,’ ’.’ ’?’ ’!’). Em- withotherbaselines bedding features are extracted from each sub- sentence and pooling functions are applied to We report our experimental results in Table 2 combine word vectors. Naive-seg extends it on the single-target benchmarking corpus (Dong by adding features extracted from the left- et al., 2014), with three model categories: 1) target-right contexts, while Naive-seg+ ex- tweet-level target-independent models, 2) target- tendsNaive-segbyaddinglexiconﬁlteredsen- dependentmodelswithoutconsideringthe‘same- timentfeatures. Clausal features with features extracted from left-target- segmentationoftweetsorsentencescanprovidea right contexts and Target-dep+ that adds target- simple approximation to parse-tree based models dependent sentiment features on top of Target- (Li et al., 2015). In Table 2 we can see our dep, we see that our method beats both of these, naive tweet segmentation models Naive-seg and withoutusingfulltweetfeatures9. TDParse+also Naive-seg+alsoachievecompetitiveperformance outperformsthestate-of-the-artTC-LSTM. TDParse+also Naive-seg+alsoachievecompetitiveperformance outperformsthestate-of-the-artTC-LSTM. suggesting to some extent that such simple When considering the ‘same-target-multi- parse-tree approximation preserves the semantic appearance’scenario,ourbestmodel-TDParse+ structure of text and that useful target-speciﬁc improves its performance further (shown as TD- information can be drawn from each segment or Parse+ (m) in Table 2). Even though TDParse clauseratherthantheentiretweet. Micro3-class-F1 45.17 51.66 52.05 Results: Overall the models perform much Macro2-class-F1 37.05 39.75 40.92 poorer than for the single-target benchmarking S3 Semeval-best Target-dep+ TDParse Macro3-class-F1 35.08 42.83 51.26 corpus, especially in 2-class F score, indicat- 1 Micro3-class-F1 38.16 46.05 53.07 ing the challenge of the multi-target-speciﬁc sen- Macro2-class-F1 35.17 40.53 50.14 timent recognition. As seen in Table 3 though Table4: PerformanceanalysisinS1,S2andS3 the feature-rich tweet-level model Semeval-best gives a reasonably strong baseline performance (same as in Table 2), both it and Target-ind target-right contexts originally proposed by Vo perform worse than the target-dependent baseline andZhang(2015)andinascenarioofmultipletar- models Target-dep/Target-dep+ (Vo and Zhang, getspertweet. Ourclausal-segmentationbaseline 2015), indicating the need to capture and utilise - Naive-seg models approximate such parse-trees target-dependent signals in the sentiment classiﬁ- by identifying segments of the tweet relevant to cationmodel. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Micro3-class-F1 45.17 51.66 52.05 Results: Overall the models perform much Macro2-class-F1 37.05 39.75 40.92 poorer than for the single-target benchmarking S3 Semeval-best Target-dep+ TDParse Macro3-class-F1 35.08 42.83 51.26 corpus, especially in 2-class F score, indicat- 1 Micro3-class-F1 38.16 46.05 53.07 ing the challenge of the multi-target-speciﬁc sen- Macro2-class-F1 35.17 40.53 50.14 timent recognition. As seen in Table 3 though Table4: PerformanceanalysisinS1,S2andS3 the feature-rich tweet-level model Semeval-best gives a reasonably strong baseline performance (same as in Table 2), both it and Target-ind target-right contexts originally proposed by Vo perform worse than the target-dependent baseline andZhang(2015)andinascenarioofmultipletar- models Target-dep/Target-dep+ (Vo and Zhang, getspertweet. Ourclausal-segmentationbaseline 2015), indicating the need to capture and utilise - Naive-seg models approximate such parse-trees target-dependent signals in the sentiment classiﬁ- by identifying segments of the tweet relevant to cationmodel. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  teractivepresentationsessions, COLING-ACL’06, pages 69–72, Stroudsburg, PA, USA. Association Table 4 shows results achieved by the tweet- forComputationalLinguistics. level target-independent model - Semeval-best, the state-of-the-art target-dependent baseline William Boag, Peter Potash, and Anna Rumshisky. </Extractive Summary>  </Table ID = 4>  </Paper ID = 808> 

<Paper ID = 809>  <Table ID = 1>  <Abstractive Summary> =  Table 1: The symbols we used in the paper. </Abstractive Summary>  <Extractive Summary> =  derivationforevaluation. Inthispaper,weaddress Structure of Derivation The word problem in these difﬁculties and for the ﬁrst time propose to Table 1 shows our notation, where our proposed evaluatethesolversusingderivationaccuracy. To annotationsareshowninbold. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ments. Statistics comparing the datasets is shown in Table 2. In total, our experiments involve over 4.2 OurSolver 2300wordproblems. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  However,ifwereconciled beingclearlysuperiortoKAZB. all templates and then used the new equations for 5.3 CaseStudy training (called TE∗ setting in Table 4), we were We discuss some interesting examples from the able to see improvements from training on more datasets, to show the limitations of existing met- data. Wesuspectdifferenceinannotationstyleled rics,whichderivationaccuracyovercomes. to several equivalent templates in the combined dataset, which got resolved in TE∗. Therefore, in Correct Solution, Incorrect Equation In the Table4,wecompareTE∗ andTDsettings.7 following example from the DOLPHIN-L dataset, In Table 4, a trend similar to Table 3 can be by choosing the correct template and the wrong observed – solution accuracy assigns a small im- alignments, the solver arrived at the correct solu- provement to TD over TE∗. Derivation accuracy tions,andgetsrewardedbysolutionaccuracy. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  to several equivalent templates in the combined dataset, which got resolved in TE∗. Therefore, in Correct Solution, Incorrect Equation In the Table4,wecompareTE∗ andTDsettings.7 following example from the DOLPHIN-L dataset, In Table 4, a trend similar to Table 3 can be by choosing the correct template and the wrong observed – solution accuracy assigns a small im- alignments, the solver arrived at the correct solu- provement to TD over TE∗. Derivation accuracy tions,andgetsrewardedbysolutionaccuracy. </Extractive Summary>  </Table ID = 3>  </Paper ID = 809> 

<Paper ID = 810>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The corpus higher TTR value indicates higher morphological statistics for the different languages can be found complexity. in Table 1. The chosen languages are from dif- ferent language families: Balto-Slavic (Bulgar- 1http://dependencies.org/ 2https://ufal.mff.cuni.cz/pdt3.0 5The sentences are all taken from the Wiki dumps 3http://www.ims.uni-stuttgart.de/ on http://linguatools.org/tools/corpora/ forschung/ressourcen/korpora/tiger.html wikipedia-monolingual-corpora/ and https: 4http://dokufarm.phil.hhu.de/ //archive.org/details/wikipediadumps? spmrl2013/?animal=spmrl2013 &sort=-downloads&page=2. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thehyper-parametersaresetto • CNNHighway: the large setup from (Kim et Table2: Tagstatistics,TTRstandsforType/Token al.,2016),i.e.,charactervectorsize=15,ﬁl- Ratio ter widths ranging from one to seven, num- ber of ﬁlters as a function of the ﬁlter width Language #Tags Entropy TTR(%) min{200,50·ﬁlterwidth},twohighwaylay- Arabic/UD 320 32.5 12 ers Bulgarian/UD 448 49.5 12 Czech/PDT 878 77.7 11 • LSTM: character vector size = 128, two lay- UD 1418 97.7 11 erswith1024and256nodes English/UD 119 27.9 7 Estonian/UD 787 57.3 13 The BLSTM modeling the context of words in a Finnish/UD 1593 76.1 17 sentence(Fig.1(a))consistsoftwohiddenlayers, French/UD 197 34.1 8 eachwith256hiddennodes. German/TIGER 681 97.7 13 Thesehyper-parametersweretunedontheGer- Hindi/UD 922 56.9 7 manTIGERdevelopmentdataandareoptimalon Hungarian/UD 652 64.5 14 a”besteffortbasis.”Germanisgoodforthehyper- Korean/SPMRL 1976 119.4 20 parametertuningasitisarelativelyhardtask(see Romanian/UD 444 65.8 7 Table 2) and shows morphological effects both Russian/UD 434 54.6 16 withinandacrosswords. Furthermore,theTIGER Turkish/UD 987 73.0 10 corpus is relatively large, which reduces statisti- calﬂuctuationsintrainingandtesting. Incontrast,wedonotobserveagainofLSTM- BLSTM over MarMoT for English and French. 5 EmpiricalEvaluation Both languages are considered to be morpholog- We empirically evaluate an LSTM-based and a ically poor, as supported by the tag statistics in CNN-based architecture for character-based mor- Table 2. This may be because of the low mor- phological tagging (Section 2) and compare them phological complexity, i.e., a character represen- against MarMoT, a state-of-the-art morphological tation does not add much information to a word tagger (Mueller et al., 2013). </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  2015. Finding function in form: This appendix contains Table 3 with the raw re- Compositional character models for open vocabu- sults used in this paper. When available, the best larywordrepresentation. </Extractive Summary>  </Table ID = 3>  </Paper ID = 810> 

<Paper ID = 811>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  tematically across other experimental conditions. We create our new dataset by uniformly sam- Table 2 gives an overview of the number of dif- pling three additional word forms from the ferent source forms per language in our dataset. paradigmofeachsourceformintheoriginaldata. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ForArabic,Ger- themulti-sourcemodelisbetterthanofthesingle- man and Turkish, we run an additional set of ex- source model. Table 3 demonstrates that the two perimentstotesttwoadditionalarchitecturalcon- conﬁgurationsweidentiﬁedaspromisingformulti- ﬁgurations of multi-source encoder-decoders: (i) sourceMRI, SINGLEFORM and MULTIFORM,oc- In addition to the default conﬁguration in which curfrequentlyenoughtoboosttheperformancefor allencodersshareparameters,wealsotesttheop- sevenoftheeightlanguages,withthelargestgains tion of each encoder learning its own set of pa- observed for Arabic (7.3%) and Russian (3.5%) rameters(sharedpar’s: yesvs.noinTable4). (ii) andthesmallestforSpanish(0.9%)andGeorgian Another way of realizing a multi-source system (1.3%)(comparingusingsourceform1withusing is to concatenate all sources and give this to an sourceforms1–4). Error Analysis. We compare errors of single- 4.2 Results sourceandmulti-sourcemodelsforGermanonde- Table 3 shows the results of the MRI experiment velopmentdata. Mostmistakesofthemulti-source on test data. Findings of the 2016 conference single-source and by how much? For example, onmachinetranslation. InProceedingsoftheFirst ConferenceonMachineTranslation,pages131–198, the numbers we compare in Table 3 are matched Berlin,Germany,August.AssociationforComputa- withrespecttothenumberoftargetforms,butnot tionalLinguistics. withrespecttothenumberofsourceforms: multi- Kyunghyun Cho, Bart van Merrie¨nboer, Dzmitry Bah- 8http://cistern.cis.lmu.de danau,andYoshuaBengio. </Extractive Summary>  </Table ID = 3>  </Paper ID = 811> 

<Paper ID = 812>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Precision is the The Autodesk training and development sets ratio of the number of sentences an APE system consist of 12,238, and 1,948 segments respec- improves (with respect to the MT output) over all tively, while the WMT16 data contains 12,000, the sentences it modiﬁes.9 Higher precision indi- and 1,000 segments. Table 1 provides additional cates that the APE system is able to improve the statistics of the source (mt#src) and target (pe) quality of most of the sentences it changed. The training sets, the repetition rate (RR) to measure statistical signiﬁcance of BLEU results is com- the repetitiveness inside a text (Bertoldi et al., puted using paired bootstrap resampling (Koehn, 2013),andtheaverageTERscoreforboththedata 2004). </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Results on the mixed data. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Weaddressed ence (REF). Table 4 seems to conﬁrm our intu- this problem incrementally, ﬁrst by proposing an ition that learning from the most similar exam- instanceselectiontechniquethatlearnsrulesfrom ples yields better translation quality. An interest- contexts that are similar to the MT segment to be ing counter example is shown in Table 5, where post-edited. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Table 4 seems to conﬁrm our intu- this problem incrementally, ﬁrst by proposing an ition that learning from the most similar exam- instanceselectiontechniquethatlearnsrulesfrom ples yields better translation quality. An interest- contexts that are similar to the MT segment to be ing counter example is shown in Table 5, where post-edited. The gains achieved by this solution despite having access to a training sample (MT- over the existing batch APE methods were fur- Top1 and PE-Top1) that is exactly the same as therincreasedbytheadditionofadynamicknowl- the MT segment to be post-edited, our system edgebasethatstoresmorereliablestatisticsabout deteriorates the translation by selecting a transla- thelearnedtranslationoptions,alsoimprovingthe tionoption(“zuplatzieren”→“zumPlatzieren”) computation time. </Extractive Summary>  </Table ID = 5>  </Paper ID = 812> 

<Paper ID = 813>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Statistics for non-projectivity and reen- Figure 3: AMR’s edges for the sentence “I beg trancies in 200 AMR manually aligned with the you to excuse me.” mapped back to the sentence, associatedsentences.5 according to the alignment. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  539with only the ◦ node. The transitions required to standardgraphG = (V ,A ),theoracleisdeﬁned g g parsethesentenceTheboyandthegirlareshown as follows, where we test the conditions in the in Table 2, where the ﬁrst line shows the initial given order and apply the action associated with conﬁguration and the last line shows the terminal theﬁrstmatch: conﬁguration. 1. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Informally, static and Stanford parser dependencies, one-hot vec- meansthatiftheactualconﬁgurationoftheparser tors for named entities and additional sparse fea- has no mistakes, the oracle provides a transition tures, extracted from the current conﬁguration of thatdoesnotintroduceanymistake. Shorteststack the transition system; this is reported in more de- meansthattheoraclepreferstransitionswherethe tails in Table 3. The embeddings for words and numberofitemsinthestackisminimized. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Features used in edge labeling. </Abstractive Summary>  <Extractive Summary> =  Berlusconi gave Lucio Stanca his current role of We use a feed-forward neural network similar to modernizing Italy’s bureaucracy in Figure 5. At the one we trained for the transition classier, with the top, we show the output of a parser (Parse 1) features shown in Table 4. The accuracy of this that is not able to deal with named entities. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Evaluation of the two parses in Figure 5 :ARG0 (p4 / person :wiki− :name (n5 / name :op1 ”Lucio” :op2 ”Stanca”))) withtheproposedevaluationsuite. </Abstractive Summary>  <Extractive Summary> =  We therefore implement a ﬁndstrengthsandweaknessofeachparser, hence test for it (Reentrancy), where we compute the speeding up the research in this area. Table 5 re- Smatchscoreonlyonreentrantedges. ports the scores for the two parses of Figure 5, Concept identiﬁcation is another critical com- where we see that Parse 1 gets a good score for ponent of the parsing process and we therefore semantic role labeling while Parse 2 is optimal compute the F-score on the list of predicted con- for named entity recognition. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Results on test split of LDC2015E86 for make the correct predictions. </Abstractive Summary>  <Extractive Summary> =  In JAMR, the concept identiﬁcation step is quadratic and the relation identiﬁcation step is 7 Conclusion O(|V|2log|V|),with|V|beingthesetofnodesin theAMRgraph. WepresentedatransitionsystemthatbuildsAMR Table 6 shows the results obtained by the graphs in linear time by processing the sentences parsers on all metrics previously introduced. On left-to-right. </Extractive Summary>  </Table ID = 6>  </Paper ID = 813> 

<Paper ID = 814>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  training pairs. For example, from the conversa- The dialog is considered a success if the ﬁnal tion in Table 1, such training examples would be actiontakeniscorrectandafailureotherwise. The generatedbypairingtheresponses“bringthecof- useralsohastheabilitytoprematurelyendthedi- feetodrmorgan”,“takethecoffeetodrmorgan” alog,andanyconversationterminatedinthisman- and “give the coffee cup to dr morgan” with the nerisalsoconsideredafailure. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  However, there in previous work (Young et al., 2010; Gasˇic´ and are a large number of possible values for g and Young, 2014). Table 2 contains the features used we use the idea of partitions (Young et al., 2010) tolearnthepolicy. Also,thepolicyislearnedover to track their probabilities in a tractable manner. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Performance metrics for dialog agents proves, its top hypotheses are more likely to be tested. </Abstractive Summary>  <Extractive Summary> =  of this can be seen in tables 4 and 5. The learned policy results in a shorter dialog in the same sit- Table 3 gives the agents’ performance on these uation because it allows the agent to act upon a metrics. All differences in dialog success and the hypothesis oflower probability. </Extractive Summary>  </Table ID = 3>  <Table ID = 6>  <Abstractive Summary> =  Table 6: An example where a stochastic policy helpswhentherobotisunabletounderstandafull In this work, we have demonstrated that continu- command. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Comparison of an agent using only the Onlydialog 0.564 0.623 top hypothesis from the semantic parser and an- Parser&dialog-full 0.588 0.647ˆ Parser & dialog - 0.576 0.670* other using the top 10 parses. </Abstractive Summary>  <Extractive Summary> =  All differences are batchwise statisticallysigniﬁcantaccordingtoanunpairedt- testwithp < 0.05. Table 7: Comparison of performance of initial parser and parsers after updating various compo- Table 8 shows the usefulness of considering nents, on paired commands and semantic forms. multiple hypothesesfrom thesemantic parser. </Extractive Summary>  </Table ID = 8>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  </Paper ID = 814> 

<Paper ID = 815>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The different types of represent the dependency nodes that are aligned c rules, e.g., word, lemma, numbers, and date, etc., by c and cp, respectively. Parse tree path is the and their proportional applicability to both AMR concatenation of all dependency tree and direc- concepts and leaves are listed in Table 1. For ex- tion labels through the tree path between dc and ample, the rule “Date” type aligns concept “11” dcp. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: The data split of the LDC DEFT AMR dependency parse pairs. </Abstractive Summary>  <Extractive Summary> =  development/test set. Table 2 presents the statis- We then compare our aligner with three other ticsfortheexperimentaldata.3 aligners: JAMR, another version of unsupervised alignment(Chen,2015),andISI.Tomakethemﬁt 5.2 ExperimentResults ourtestdata,wedesignaheuristicmethodtoforce We run EM for 50 iterations and ensure the EM every unaligned concept (e.g., named entity and model converges. Afterwards, we use our decod- “‘:polarity -”’ concepts) to align to a dependency ingalgorithmtoﬁndthealignmentsthatmaximize wordnodeaccordingtorule-basedandglobalfea- thelikelihood. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Incremental Feature Contributions for entity tags automatically on the generated de- differentfeatures: L: lemma;R: relation;N: NE; pendency parses. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 815> 

<Paper ID = 816>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We label the sense ages. We show an example of case frames for the ofthoseunseenpredicatesusingthedefaultsense, verb ‘谢’ in Table 1, which has multiple mean- whichis‘01’inourwork. ings. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  labeled corpus (a treebank), a part of which was These features are described in the upper part of used to train a base dependency parser. Another Table 2. We use binned values (i.e., high, mid- part of the labeled corpus was used to apply au- dle and low) for all of the feature values calcu- tomatic dependency parsing. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Automatically lar to the method described in Section 4, we also obtainedmorphologicalandsyntacticinformation proposeasetoffeaturesextractedfromdeepcase (the columns begin with “P”) was used. PD and frames which are listed in Table 3. The ﬁrst four AI, AC step are regarded as multi-class classiﬁ- featuresdonotconcernthepredicatesense. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  surfacecaseframefeaturesgainsaslightimprove- mentcomparedtothebasicfeatures. 8 Conclusion&futurework Table 4 shows the experimental results of SRL using surface and deep case frames as additional Weproposedamethodforusingadditionalknowl- features. Knowledge (n%) indicates that the top edgetoimproveChineseSRL.Toaddressthecase n% (according to the classiﬁer) of the automat- ambiguity problem in the surface case represen- ically extracted knowledge was used. </Extractive Summary>  </Table ID = 4>  </Paper ID = 816> 

<Paper ID = 817>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Hyperparameters of different models. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Type macro aver- 5 CLR(NSL) .164 .484 .464 .157 .470 .443 .173 .483 .472 age F on test for all, head 6 CLR(CNN) .177 .494 .468 .171 .484 .450 .187 .489 .474 1 7 BOW .113 .346 .379 .109 .323 .353 .120 .356 .396 and tail types. </Abstractive Summary>  <Extractive Summary> =  Ourbestmodel, ELR+SWLR+CLR+TC (line 22), which we refer to as MuLR in the other tables, beats our initial Figure 4: t-SNE result of entity-level representa- baselines (ELR and AGG-FIGER) by large mar- tions gins, e.g., in tail entities improvements are more than8%inmicroF1. Table 3 shows type macro F for MuLR 1–13. Thisshowsthepowerofentity-levelembed- 1 (ELR+SWLR+CLR+TC) and two baselines. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Micro F on test of 19 ELR+SWLR .558 .820 .796 .584 .846 .829 .480 .751 .714 1 20 ELR+WWLR+CLR .568 .823 .798 .590 .847 .829 .491 .755 .716 character, word level models 21 ELR+SWLR+CLR .569 .824 .801 .590 .849 .831 .497 .760 .724 for all, known (“known? yes”) 22 ELR+WWLR+CLR+TC .572 .824 .801 .594 .849 .831 .499 .759 .722 and unknown (“known? no”) 23 ELR+SWLR+CLR+TC .575 .826 .802 .597 .851 .831 .508 .762 .727 entities. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Table2: Accuracy(acc),micro(mic)andmacro(mac)F 1 ontestforall,headandtailentities. 4.2 Results – is more robust than hand engineered feature basedNSL.WeshowmoredetailedresultsinSec- Table 2 gives results on the test entities for all tion4.3. (about 60,000 entities), head (frequency > 100; about 12,200) and tail (frequency < 5; about Word-level models are on lines 7-10. </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Micro average F results of MuLR and beingsthatarenothumansbelongtothistype. </Abstractive Summary>  <Extractive Summary> =  thatareindicativeofPERSONandMUSIC. Results are shown in Table 5. While for head ELR incorrectly assigns “Zumpango” (CITY) entities,MuLRworksmarginallybetter,thediffer- and“LakeKasumigaura”(LOCATION)toLIVING- enceisverysmallintailentities. </Extractive Summary>  </Table ID = 5>  </Paper ID = 817> 

<Paper ID = 818>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  bridalignedresource. Noise within hypernymy graphs can be further Table 1 shows the proto-conceptualization entries classiﬁed into: i) noisy nodes, the concepts that forthepolysemoustermsbridgeandlink,namely donotbelongtoaspeciﬁctargetvocabulary, e.g., their ﬁgurative (“bridge:NN:2” and “link:NN:1”) domain concepts for domain-speciﬁc KBs, such and concrete ‘infrastructure’ (“bridge:NN:3” and asJaguar Carswithinazoologicaltaxonomy; “link:NN:0”) senses, respectively. JoBimText ii)noisyedges,thewrongly-acquiredrelationsbe- models provide sense distinctions that are only tween unrelated concepts or out-of-domain rela- partiallydisambiguated: thelistofsimilarandhy- tions, e.g., Jaguar Cars isa Feline; iii) cy- pernyms terms of each sense, in fact, does not clesofhypernymyrelations,suchasthosederived carry sense information. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  C (x) = C (s)}; ii) the datasets, respectively). Table 2 shows some of T T 595senses polysemy hypernyms links hypernymygraph dataset # avg. max # avg. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  dard” information of the companion KB, and its 4.1.3 Resultsanddiscussion linking to the NKG, which act as a source of su- pervision. Consequently, we address the research Table 3 summarizes the performance of Con- question of how much (pseudo-)supervision our trastMedium on the four automatically acquired method needs in terms of KB links, and whether NKGs. The results show that the pruning impact it can be used to improve the state-of-the-art on of our approach is lower than that of the baseline thetaskoftaxonomyinduction. </Extractive Summary>  </Table ID = 3>  </Paper ID = 818> 

<Paper ID = 819>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  are correct when when sufﬁciently justiﬁed in the 6.2 ResultsandDiscussion sourcecorpus. The answersin Y (hop-1) are cor- Table 2 compares our full system (KBC+E+R) rect only if both the element of X that generated to the top performing system in TAC Cold Start the response is correct and the response in Y is 2015 using three different approaches to post-hoc justiﬁedinthetext. scoring. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: CS-SF scores (string-match) for differ- offset-basedmatch,string-matchmatch,andas- ent priors: KBC (baseline: no world knowledge), sessinwhichweapplytheoffset-basedmetricus- KBC+E(onlyentity-basedfactors),KBC+R(only ing additional internally performed assessments. </Abstractive Summary>  <Extractive Summary> =  Table2compares KBC+E+R 49 31 38.4 32 19 24 the TAC top-ranked system to our full conﬁgura- tion using three post-hoc scoring strategies: strict Table 3: CS-SF scores (string-match) for differ- offset-basedmatch,string-matchmatch,andas- ent priors: KBC (baseline: no world knowledge), sessinwhichweapplytheoffset-basedmetricus- KBC+E(onlyentity-basedfactors),KBC+R(only ing additional internally performed assessments. relation-based factors), KBC+E+R(both sets of For the ablation study in Table 3, we use the of- factors). Numbers in parentheses indicate the op- ﬁcial scorer’s string-match mode. match mode. We further account for these re- Table 3 ablates each type of world knowledge sponses by re-estimating precision for hop-0 and to show the impact of entity and relation-based hop-1 assuming that the precision of the ignored factorsindependentlywhencomparedtoaversion responsesathop-1isthesameasthehop-0preci- of our system without world knowledge. As ex- sion13. </Extractive Summary>  </Table ID = 3>  </Paper ID = 819> 

<Paper ID = 820>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  617Model MAP highly varied relations. For example, in the ﬁrst row EntityEmbeddings 54.81 in Table 3, for the query /baseball/baseball player MeanPool 39.47 the model needs to correctly focus on aspects like MaxPool 32.59 /sports/pro athlete and ignore evidence information Attention 55.66 like/tv/tv actor. Amodelthatcreatesasinglequery- MaxRelation 55.37 independent centroid will be forced to try and merge thesedisparatepiecesofinformationtogether. </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The precision(MAP). entity pair model used a batch size 1024, ‘ = 1e- Table 2a shows the results of this experiment. We 2 8, (cid:15) = 1e-4, and learning rate 0.01. representations. The Max Relation model performs competitively with the Attention model which is not Table 2b shows the results of the experiment with entirely surprising as it is a simpliﬁed version of the unseen entities. There is very little performance drop Attentionmodel. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: The percentage of positive triples ranked in less model generalizes to all entities and entity pairs, thetop10amongsttheirnegativesaswellasthemean whetherseenattraintimeornot.Wecancombinethese reciprocal rank (MRR) scaled by 100 on a subset of two approaches together to make an universal schema theFB15K-237dataset. </Abstractive Summary>  <Extractive Summary> =  experiment,werandomlyselect14000entitiesandtake all(entity,type)pairscontainingthoseentities. Were- Our results are shown in Table 4a. We can see that move these pairs from our training set and use them the models with query speciﬁc aggregation functions aspositivesamplesinourtestset. Weremovealltraining ticularlyimportantforrowsthatarenotdescribedeas- examplesthatcontainapositiveentitypairineitherour ily by a single centroid such as an entity with several validation or test set. We use the same validation and very different careers or an entity pair with multiple test set as in Table 4a. The entity pair model predicts 618Query ObservedColumns /baseball/baseball player /sports/pro athlete, /sports/sports award winner, /tv/tv actor, /people/measured person, /award/award winner,/people/person /architecture/engineer engineer,/book/author,/projects/project focus,/people/person,sir /baseball/baseball player baseman,/sports/pro athlete,/people/measured person,/people/person,dodgers,coach /computer/computer scientist /education/academic,/music/group member,/music/artist,/people/person /business/board member /organization/organization founder,/award/award winner,/computer/computer scientist, /people/person,president,scientist /education/academic /astronomy/astronomer,/book/author Table3:Eachrowcorrespondstoatruequeryentitytype(leftcolumn)andtheobservedentitytypes(rightcolumn) foraparticularentity. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: The percentage of positive triples ranked in SIGMOD international conference on Management thetop10amongsttheirnegativesaswellasthemean ofdata,pages1247–1250.ACM. </Abstractive Summary>  <Extractive Summary> =  unseen entity pairs. The query-independent aggrega- The results of this experiment with observed rows tion functions, mean pool and max pool, perform bet- are shown in Table 5a. While both the MRR and terthanmodelswithexplicitentitypairrepresentations. </Extractive Summary>  </Table ID = 5>  </Paper ID = 820> 

<Paper ID = 821>  <Table ID = 2>  <Abstractive Summary> =  Table 2: We manually annotate some polarity la- NN-pr 1.53 22.44 bels (positive or negative) for generated reviews NN-ur 3.61 26.37 and compute accuracy by comparing them with Att2Seq 4.51 30.24 the input ratings. </Abstractive Summary>  <Extractive Summary> =  This Nearest Neighbor based method same labels by two annotators. Table 2 shows retrievesthereviewsthathavethesameproductID our method signiﬁcantly outperforms others (p < andratingastheinputattributesintheTRAINset. 0.05). </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  actersinthisbook’sreviews. 4.4 TheAttentionMechanism 4.5 GeneratedExamples As described in Section 3.3, the attention mecha- nism learns soft alignment scores between gener- As shown in Table 4, we sample products and ated words and input attributes. These scores are userstogeneratesomeexampleswithdifferentrat- used to obtain encoder-side context vectors that ings. </Extractive Summary>  </Table ID = 4>  </Paper ID = 821> 

<Paper ID = 822>  <Table ID = 1>  <Abstractive Summary> =  Table 1: The top ﬁfteen slots across entities used DEV and 50,030 TEST instances. </Abstractive Summary>  <Extractive Summary> =  This criteria PLACE OF BIRTH 298,374 25 covers 72.8% of all facts. Table 1 shows the dis- EDUCATED AT 141,334 32 tributionoffactslotsinthestructureddataandthe SPORTS TEAM 108,222 29 percentage of time tokens from a fact value occur PLACE OF DEATH 107,188 17 inthecorrespondingWikipediasummary. POSITION HELD 87,656 75 Additionally,someWikidataentitiesremainun- PARICIPANT OF 77,795 23 derpopulatedanddonotcontainsufﬁcientfactsto POLITICAL PARTY 74,371 49 reconstruct a text summary. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Language model perplexity across tem- plateddatasets. </Abstractive Summary>  <Extractive Summary> =  textpairs. Thesummarysentencescanbelongand Table 2 lists perplexity numbers for the themostfrequentlengthis21tokens. Weﬁlterto benchmark LM models with different templating 635Templates DEV None 29.8 Title 14.5 Full 10.1 Table 2: Language model perplexity across tem- plateddatasets. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: BLEU scores for each hypothesis against Table 4: Percentage of entities for which human theWikipediareference judgespreferredtherowsystemtothecolumnsys- tem. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Percentage of entities for which human theWikipediareference judgespreferredtherowsystemtothecolumnsys- tem. </Abstractive Summary>  <Extractive Summary> =  tertext. 7.2 Examplegeneratedtext 6.2 Humanpreferenceevaluation Table 5 shows some DEV entities and their sum- Table 4 shows the results of our human evalua- maries. The model learns interesting mappings: tionover82entitiessampledfromTEST. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  tertext. 7.2 Examplegeneratedtext 6.2 Humanpreferenceevaluation Table 5 shows some DEV entities and their sum- Table 4 shows the results of our human evalua- maries. The model learns interesting mappings: tionover82entitiessampledfromTEST. </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Hallucination results phrased as preci- put facts are expressed in the reference sum- sion, recall and F1 of systems with respect to the mary from Wikipedia. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  put Wikidata relations (Table 7). Our best model Do systems select the same facts found in the achieves a precision of 0.93 with respect to Wiki- reference summaries? Table 6 shows content data input. Notably, the template-driven baseline selection scores for systems with respect to the maintains aprecision of 1.0 as itis constrained to Wikipedia text as reference. </Extractive Summary>  </Table ID = 6>  </Paper ID = 822> 

<Paper ID = 823>  <Table ID = 1>  <Abstractive Summary> =  Table 10: Deep type training instance from Sur- 7 SHIFTSUBTREE(i,ρ) faceRealisationSharedTask2011. Table 11: Average F-measure for function word eventuallybegintalkingdirectlytotheanc predictionfordevelopmentset. </Abstractive Summary>  <Extractive Summary> =  Therearethreetran- thenode. sitionactions: Table 1 shows the feature templates for classi- ﬁcationoftoinﬁnitivesandthatcomplementizers • SHIFT-Word-POS – shifts Word from ρ, as- signs POS to it and pushes it to top of stack and Table 2 shows the feature templates for pre- asS ; dicting the count of comma child nodes for each 0 non-leaf node in the graph. These feature tem- • LEFTARC-LABEL – constructs dependency LABEL plates are a subset of features used in the joint arc S1 ←−−−−− S0 and pops out second ele- model(Section4)withtheexceptionsbeingword mentfromtopofstackS1; orderfeatures,whicharenotavailablehereforthe • RIGHTARC-LABEL – constructs depen- LABEL pipeline system, since earlier stages cannot lever- dency arc S −−−−−→ S and pops out top 1 0 age features in subsequent outcomes. economists who read september ’s low level offactoryjobgrowthasasignofaslowdown 6.2 PipelinevsJointModel Pipeline september’slowleveloffactoryjobgrowth whoasasignofaslowdownreadseconomists Wecomparetheresultsofthejointmodelwiththe TBDIL economists who read september ’s low level pipeline baseline system. Table 11 shows the de- offactoryjobgrowthasasignofaslowdown velopmentresultsoffunctionwordprediction,and Table14: Exampleoutputs. Table 12 shows the overall development results. Table 11 shows the de- offactoryjobgrowthasasignofaslowdown velopmentresultsoffunctionwordprediction,and Table14: Exampleoutputs. Table 12 shows the overall development results. Our joint model of Transition-Based Deep Input instance, because of incorrect linearization, there Linearization (TBDIL) achieves an improvement is error propagation to morphological generation of5BLEUpointsoverthepipelineusingthesame inthepipelinesystem. Our system gave the highest scores reported 8 Analysis fortheNLG2011sharedtaskonDeepInputLin- earization(Belzetal.,2011). Table 14 shows sample outputs from the Pipeline systemandthecorrespondingoutputfromTBDIL. Acknowledgments Intheﬁrstinstance, thefunctionwordtoisincor- rectlypredictedinthearcbetweennodesdoesand WethankLittonKurisinkelforhelpfuldiscussions yield in the pipeline system. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Feature templates for the comma predic- tion(Qianetal.,2015),syntacticlinearizationand tion system. </Abstractive Summary>  <Extractive Summary> =  Therearethreetran- thenode. sitionactions: Table 1 shows the feature templates for classi- ﬁcationoftoinﬁnitivesandthatcomplementizers • SHIFT-Word-POS – shifts Word from ρ, as- signs POS to it and pushes it to top of stack and Table 2 shows the feature templates for pre- asS ; dicting the count of comma child nodes for each 0 non-leaf node in the graph. These feature tem- • LEFTARC-LABEL – constructs dependency LABEL plates are a subset of features used in the joint arc S1 ←−−−−− S0 and pops out second ele- model(Section4)withtheexceptionsbeingword mentfromtopofstackS1; orderfeatures,whicharenotavailablehereforthe • RIGHTARC-LABEL – constructs depen- LABEL pipeline system, since earlier stages cannot lever- dency arc S −−−−−→ S and pops out top 1 0 age features in subsequent outcomes. A subset is shown 7 candidates←TOP-K(agenda) 8 agenda←∅ here. For the full feature set, refer to Table 2 of 9 best←BEST(candidates) Puduppully et al. (2016). </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Transition action sequence for lineariz- then ing the graph in Figure 3. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Baseline linearization feature templates. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Lookahead linearization feature tem- plates for the word L to shift. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  portionaltoρ. GivenaconﬁgurationC, thescore ofapossibleactionaiscalculatedas: Score(a) = θ~·Φ(C~,a), similartothatusedinSongetal.(2014), listedin Table 7, to generate a candidate set of inﬂections. whereθ~isthemodelparametervectorandΦ(C~,a) An averaged perceptron classiﬁer (Collins, 2002) denotes a feature vector consisting of conﬁgura- is trained for each lemma. </Extractive Summary>  </Table ID = 7>  <Table ID = 9>  <Abstractive Summary> =  Table 9: Transition action sequence for lineariz- think have think to have C-A1 INF C-A1 ing the sentence in Figure 2. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 9>  </Paper ID = 823> 

<Paper ID = 824>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thus,weconditionedthechoiceof All extracted proper names were automatically a speciﬁc proper name form by a set of discourse annotated with their syntactic position (subject, features that describe the reference as well as to object or genitive noun phrase in a sentence) and thepersontobementioned. referential statuses in the text (discourse-new or Table 1 depicts the discourse features used to discourse-old) and in the sentence (sentence-new describe the proper name references. We choose or sentence-old). </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  a total of 28 possible ones. Table 2 de- presence of ﬁrst, middle and last names, a Proper pictsthemostfrequentones. Thecompletelistcan Name Knowledge Base was extracted from DB- be found at the webpage that describes the REG- pedia (Bizer et al., 2009) with all the names of namescorpus3. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  From the 1,000 peo- 4.1. ple in the corpus, we ﬁrst ﬁltered the ones whose 6.4 Results birth names were not mentioned, or for whom the values of the DBpedia’s attributes foaf:name, Table 3 summarizes the accuracy-scores of the foaf:givenName and foaf:surname were missing. modelsinthepredictionofthepropernameforms. </Extractive Summary>  </Table ID = 3>  </Paper ID = 824> 

<Paper ID = 825>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Projective statistics on four datasets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Results on English dataset (PTB with ingsharedtask. </Abstractive Summary>  <Extractive Summary> =  Accuracy results for all four theCTBexperiments,wetrained300dimensional parsersarereportedinWeissetal.(2015). 5http://stp.lingfil.uu.se/˜nivre/research/ The second block in Table 2 presents re- Penn2Malt.html sultsobtainedfromneuralnetwork-basedparsers. 6We make the number of sentences in the development C&M14(ChenandManning,2014)isatransition- andtestsetscomparable. Ascanbeseen algorithminstead. in Table 2 (DENSE-Pei), these features are less Unlike all models above, DENSE does not use effective compared to LSTM-based ones and the any kind of transition- or graph-based algorithm contributionoftheMSTalgorithm(Eisner)during during training and inference. Nonetheless, it ob- decoding is more pronounced (DENSE-Pei+E). </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Results on Chinese dataset (CTB). </Abstractive Summary>  <Extractive Summary> =  When we post-process the Results on CTB follow a similar pattern. As remaining13%ofnon-projectiveoutputswiththe shown in Table 3, DENSE outperforms all previ- Eisner algorithm (DENSE+E), we obtain a slight ous neural models (see the test set columns) on improvementonUAS(94.10%). UAS and LAS. plexhighorderdecodingalgorithminvolvingcube BeforeMST AfterMST pruning and strategies for encouraging diversity. Dataset #Sent Tree Proj Tree Proj Post-processing the output of the parser with the PTB 1,700 95.1 86.6 100.0 100.0 Eisneralgorithmgenerallyimprovesperformance CTB 803 87.0 73.1 100.0 100.0 (by 0.21%; see last row in Table 3). Again we Czech 374 87.7 65.5 100.0 72.7 observe that 1-order-atomic features (Lei et al., German 367 96.7 67.3 100.0 68.1 2014a) are inferior compared to the LSTM. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Our results are summarized LAS) when using a MST algorithm. On German, in Table 5. We compare DENSE against three DENSE is comparable with the best third-order non-projective graph-based dependency parsers: parser (Turbo-3rd), while on Czech it lags behind the MST parser (McDonald et al., 2005b), the Turbo-3rd and RBG-3rd. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Percentage of trees and projective trees (UEM) in Table 4 for English and Chinese. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Again we Czech 374 87.7 65.5 100.0 72.7 observe that 1-order-atomic features (Lei et al., German 367 96.7 67.3 100.0 68.1 2014a) are inferior compared to the LSTM. Ta- ble4reportsunlabeledsentencelevelexactmatch Table 6: Percentage of trees and projective trees (UEM) in Table 4 for English and Chinese. In- on the development set before and after DENSE terestingly, even when using the greedy inference usesaMSTalgorithm. </Extractive Summary>  </Table ID = 4>  </Paper ID = 825> 

<Paper ID = 826>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Comparing training methods on PENN ComputeGradients(OracleSeq); Treebank (arc-standard and arc-eager) and Ger- elseifRL-Randomthen manpartofSPMRL-2014(swap-standard). </Abstractive Summary>  <Extractive Summary> =  The ComputeGradients(m); Stanford dependency scorer5 was used for evalu- end ation. Performonegradientdescentstep; 4.2 EffectivenessofReinforcementLearning end Algorithm1:Trainingadependencyparserwith Table 2 displays the performance of different ap- approximatepolicygradient. proaches to training dependency parsers. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Parsing accuracy of RL-RANDOM (arc- arcsBig→theandon→Bigarethusalltheresult standard)withdifferentsamplesizescomparedto ofasingledecisionerror. </Abstractive Summary>  <Extractive Summary> =  RIGHTl] at each step. We use the term arc error Table 3 shows the importance of samples for to refer to an erroneous arc in the resulting tree. RL-RANDOM. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Overview of average impact of decision cess,wetalkoferrorpropagation. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 826> 

<Paper ID = 827>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Toy grammar used to demonstrate verb cleaningservice had sentoverwas cleaningev- 3 3 2 forgetting. </Abstractive Summary>  <Extractive Summary> =  s ≈ 0, since its relative clauses are obligatorily verb-ﬁnal. When verb-ﬁnal relative clauses have 3.1 ModelofVerbForgetting higher prior probability, a doubly-nested RC pre- ﬁx NCNCVV is more likely to be preserved by a Table 1 presents a toy probabilistic context-free rationalnoisy-channelcomprehender. grammar for the constructions involved in verb forgetting. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Mutual information over wordforms in vide evidence below, dependency locality effects different dependency relations in the Syntactic n- canbeseenasaspecialcaseofinformationlocal- gram corpus. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 827> 

<Paper ID = 828>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Hyperparameters. </Abstractive Summary>  <Extractive Summary> =  2015), with a ﬁrst momentum coefﬁcient of 0.9 4.2.1 FeatureVector andasecondmomentumcoefﬁcientof0.999,2 L 2 regularizationandDiversityRegularization(Xieet The ﬁnal feature vector as input of classiﬁer con- al., 2015). Table 1 shows the values of the hyper- tainsthreeparts: rep,simi,extra. parameters,tunedondev. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Results on WikiQA. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Results on SICK. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 828> 

<Paper ID = 829>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  creases by 1.83% with respect to the No axioms baseline. This system had higher performance 5.3 Results than the other two best logic systems The Mean- Table 2 shows the results of our experimentation. ing Factory and LangPro (82.97% vs. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  stand and run). In the rest of the examples, an ideal axiom injection 5.4 PositiveExamplesandErroranalysis mechanismwouldneedaccesstostringsimilarity Table 3 shows some positive and negative exam- methods (i.e. “on a stage” → “onstage”) and to ples of performance of our system on the trial a knowledge base to understand that a machine is split of the SICK dataset. </Extractive Summary>  </Table ID = 3>  </Paper ID = 829> 

<Paper ID = 830>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  As described above, we compute the KL divergence on a per-item ba- Our model up to this point has only been trained sis, and report the mean over all items in the test onshortphrases,sinceconditionalprobabilitiesin set. the denotation graph are only reliable for phrases Table 1 shows the performance of our trained that occur with multiple images (see Figure 5 model on unseen test data. The full test data for the distribution of phrase lengths in the train- consists of 4.6 million phrase pairs, all of which ing data). </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  dictory sentence pairs have conditional probabil- itiesapproaching0(median0.19). Results Table 2 contains our results on SNLI. 8 Predictingtextualentailment Our baseline LSTM achieves the same 77.2% ac- curacyreportedbyBowmanetal.(2015),whereas In Section 7.2, we trained our probability model aclassiﬁerthatcombinestheoutputofthisLSTM on both short phrase pairs for which we had gold with only a single feature from the output of our probabilities and longer SNLI sentence pairs for probabilitymodelimprovesto78.2%accuracy. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Go 1ld and predicted conditional proba-  1 limitedvocabularycomparedtothefullSNLIdata bilities from the denotational phrase development (thereare5263wordtypesinthedenotationgraph data. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  However, our model occurrence in example 13. Table 5 demonstrates does correctly predict low probabilities for some similarpatternsforpairswherebothphraseswere contradictory examples that have reasonably high unseen. word overlap, as in example 13. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  word overlap, as in example 13. Finally, exam- ple 14 shows that our model can correctly predict Table 6 has examples of predicted conditional verylowconditionalprobabilityforsentencesthat probabilitiesforsentencepairsfromtheSNLIde- sharenocommonsubjectmatter. velopment data. </Extractive Summary>  </Table ID = 6>  </Paper ID = 830> 

<Paper ID = 831>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  UndergraduatestudentsoftwoIn- ship,whichmeansthatanyonehavingaPowerori- dianinstitutes(NIT,Agartala,TripuraandIIIT,Sri entation can have the Achievement orientation as City,AndhraPradesh)werecontactedforthedata well. To understand this notion vividly, we have reported the fuzzy membership statistics from the 5http://twitter4j.org/ Twitter data in Table 3 (due to space limitations, 6http://www.statista.com/statistics/264810/number-of- monthly-active-facebook-users-worldwide/ statistics for the other corpora are not reported). 735FeatureAblation AC BE CO HE PO SE SD ST TR UN BeforeAblation 65.84 56.06 64.02 58.02 58.80 53.06 60.89 56.58 64.28 65.58 AfterAblation 65.84 58.54 64.80 58.93 59.58 55.80 61.53 56.84 65.06 66.10 Numberoffeatures 52 37 65 38 54 47 65 53 39 48 Table 4: Best LIWC feature selection (by accuracy) for each of Schwartz’ ten personality value types. Thevaluesinthe‘BeforeFeatureAblation’rowarebasedonthefullfeatureset(69features). The statistics in Table 3 show how the ten val- theequation: xˆ = (x−µ)/σ,wherexisthe‘raw ues are interconnected and inﬂuence each other, frequency count’, µ and σ are respectively the supporting the basic assumption of the Schwartz mean and standard deviation of a particular fea- model that the borders between value classes are ture. After normalizing, each feature vector value artiﬁcialandthatonevalueﬂowsintothenext. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Best LIWC feature selection (by accuracy) for each of Schwartz’ ten personality value types. </Abstractive Summary>  <Extractive Summary> =  Ten different classiﬁers were trained, each times). Table 4 contains detailed categorical fea- foraparticularvaluetype. Eachclassiﬁerpredicts turesforeachvaluetypefortheSMOmodel,and, whetherthepersonconcernedispositivelyorneg- e.g., shows that the same accuracy (65.84%) for ativelyinclinedtowardsthegivenSchwartzvalue. </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  fold cross validation. Automatic speech act clas- Table 6 reports that our models outperformed the siﬁcation of social media conversations is a sepa- majoritybaselinesbysigniﬁcantmarginsof+5.05, rateresearchproblem,whichisoutofthescopeof +7.20, +9.83 respectively on the Essay, Twitter the current study. However, although the speech and Facebook corpora. </Extractive Summary>  </Table ID = 6>  </Paper ID = 831> 

<Paper ID = 832>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ple, user-generated content might be more engag- Andindeed,about90%ofthepeopleinonlinefo- ing and persuasive than traditional media, due to rumsareso-calledlurkers(Whittaker,1996;Non- theprevalenceofemotionallanguage,socialafﬁl- necke and Preece, 2000; Preece et al., 2004), and iation, conversational argument structure and au- do not post, suggesting that they are in fact read- dienceinvolvement. Moreover,particulartypesof ing opinionated dialogs such as those in Table 1 people may be more or less convinced by partic- forinterestorentertainment. ularstylesofargument, e.g. Ourresearchquestionsare: The ARGUMENT includes the content and its • Can we mine social media to ﬁnd arguments presentation, e.g. whether it is a monolog or a thatchangepeople’sbeliefs? dialog, or whether it is factual or emotional as il- lustrated in Table 1 and Table 2. The AUDIENCE • Do different argument types have different factor models people’s prior beliefs and social af- effectsonbeliefchange? ﬁliations as well as innate individual differences • Do personality and prior beliefs affect belief that affect their susceptibility to particular argu- change? ments or types of arguments (Anderson, 1971; • Aredifferentpersonalitytypesdifferentlyaf- Davies, 1998; Devine et al., 2000; Petty et al., fectedbyfactualvs. responses with an average ≥ 4 annotation R5: No Christian sin??? Other then the intent to kill were considered factual, and those whose anno- andthendoingso:p tation averaged ≤ −4 were considered emotional Emotional:GayMarriage on a scale of -5 to 5. Table 1 illustrates both fac- Q6: Didanyoneelseexpectanythingless? Theseevil tual (R1) and emotional (R2) arguments, with ad- fundiechristianistscanhaveaffairs,2,3,4,oreven 5marriagesyetgaypeopleareathreattomarriage ditionalexamplesforothertopicsinTable3. bywantingtogetmarried. (1.0-1.5)or(4.5-5.0), ticipants with one of the three different argument indicatinganextremeinitialview. types to test their affect on belief change: a Cu- We tested whether people who were more en- rated Monolog (MONO) (Table 2), an emotional trenched initially showed less change than those argument (EMOT) (R2 in Table 1), or a factual ar- who were initially more neutral. We conducted gument (FACT) (R1 in Table 1). types to test their affect on belief change: a Cu- We tested whether people who were more en- rated Monolog (MONO) (Table 2), an emotional trenched initially showed less change than those argument (EMOT) (R2 in Table 1), or a factual ar- who were initially more neutral. We conducted gument (FACT) (R1 in Table 1). After each per- a 2 Initial Belief (Entrenched/Neutral) X 3 Ar- son read one of these three types of arguments, gument Type (MONO/EMOT/FACT) ANOVA, with we retested their reactions to the original stance Belief Change as the dependent variable, and Ini- question, while viewing the argument. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Traditional balanced summary of the been neglected in computational work, which has deathpenaltyissuefromProCon.org. </Abstractive Summary>  <Extractive Summary> =  Ourresearchquestionsare: The ARGUMENT includes the content and its • Can we mine social media to ﬁnd arguments presentation, e.g. whether it is a monolog or a thatchangepeople’sbeliefs? dialog, or whether it is factual or emotional as il- lustrated in Table 1 and Table 2. The AUDIENCE • Do different argument types have different factor models people’s prior beliefs and social af- effectsonbeliefchange? ﬁliations as well as innate individual differences • Do personality and prior beliefs affect belief that affect their susceptibility to particular argu- change? ments or types of arguments (Anderson, 1971; • Aredifferentpersonalitytypesdifferentlyaf- Davies, 1998; Devine et al., 2000; Petty et al., fectedbyfactualvs. arelesslikelytochangetheirmindwhenprovided We selected the balanced, monologic, argument new information about that issue. Table 6 shows summaries from the website ProCon.org (in the relationship between initial beliefs and extent Table 2 with an additional example in Table 5). of belief change. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: TIPI σ and mean from our personality whole, by comparing the means and standard de- surveycomparedtothenormaldistribution viationsofoursampleof637Turkerswiththena- 7453.3 PriorBeliefsandBeliefChange N Meanchange σchange MONOentrenched 1826 0.50 1.09 Previous research suggests that people who are MONOneutral 1359 0.62 0.71 entrenched about an issue are unlikely to change FACTentrenched 258 0.27 0.79 FACTneutral 202 0.39 0.55 theirmind(Anderson,1971;Davies,1998;Devine EMOTentrenched 213 0.35 0.87 et al., 2000), so we wanted to establish the base- EMOTneutral 187 0.37 0.54 line beliefs of our pre-qualiﬁed Turkers before ALLentrenched 2951 0.43 1.00 ALLneutral 2234 0.51 0.65 they had been exposed to any arguments about a topic. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Theyarguethatpregnantwomenwillre- Our ﬁrst question is whether our method changed sorttounsafeillegalabortionsifthereisnolegal participant’s beliefs. Table 6 shows belief option. change as a function of argument type: monologs CON: Opponents,identifyingthemselvesaspro-life,as- sert that personhood begins at conception, and (MONO), factual (FACT) and emotional (EMOT). arelesslikelytochangetheirmindwhenprovided We selected the balanced, monologic, argument new information about that issue. Table 6 shows summaries from the website ProCon.org (in the relationship between initial beliefs and extent Table 2 with an additional example in Table 5). of belief change. The interpretation of what and FACT (both p <0.0001), but no differences stance the Low and High bins represent is strictly between EMOT and FACT overall across all sub- topic dependent. The Medium bin consists of z- jects (See Table 6). Finally there was no inter- transformationvaluesbetween-1and1. </Extractive Summary>  </Table ID = 6>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Traditional balanced summary of cial media can be mined for persuasive materials. </Abstractive Summary>  <Extractive Summary> =  arelesslikelytochangetheirmindwhenprovided We selected the balanced, monologic, argument new information about that issue. Table 6 shows summaries from the website ProCon.org (in the relationship between initial beliefs and extent Table 2 with an additional example in Table 5). of belief change. </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  impactforbeliefchangeprediction. Resultsonthedevelopmentsetshowedthatthe Table 7 summarizes our key results, reporting z-transformation scores for prior belief and per- accuracy, precision, recall, and F1 for predicting sonality performed better than the raw scores and beliefchangeasadiscretebin,Low,Medium,and bins. Ontheotherhand, thebeliefchangefeature High. Weshowallofourresults,butfocusourdis- EMOT datasets have speciﬁc information in terms cussion below on statistically signiﬁcance differ- ofscalarvaluesabouttheirdegreeoffactualityor ences in F1. We boldface personality feature sets emotionality,onascaleof-5,+5andafeaturewith in Table 7 that are statistically signiﬁcant when thisvalueiscreatedforthesedatasetsderivedfrom comparing{MONO, FACT, EMOT}+Nonewiththe thecrowdsourcedTurkerjudgmentsaboutthede- otherfeaturesetsinthegroup. gree of Fact/Emotion in a Q/R pair, as described Theeffectofargumentalone(withoutpersonal- earlier. </Extractive Summary>  </Table ID = 7>  </Paper ID = 832> 

<Paper ID = 833>  </Paper ID = 833> 

<Paper ID = 834>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Bil- BOWA(Gouwsetal.,2015)alsousesasimilarset 2homepages.inf.ed.ac.uk/s0787820/ bible/ offeatures,butrestrictsthesourcelanguagewords 3www.isi.edu/natural-language/ to those that appeared within a certain distance download/hansard/ 767Inthewordalignmentbenchmark,eachwordin tuninghyperparametersfortrulylow-resourcelan- a given source language sentence is aligned with guages. themostsimilartargetlanguagewordfromthetar- get language sentence – this is exactly the same 3.3 Results greedydecodingalgorithmthatisimplementedin Table 1 shows that the two algorithms based on IBMModel-1(Brownetal.,1993). Ifasourcelan- thesentence-IDfeaturespaceperformconsistently guage word is out of vocabulary, it is not aligned better than those using source+target words. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  768Source+TargetWords SentenceIDs BWE Bilingual Inverted BilBOWA SkipGram Autoencoders Index en fr .3653 .3538 .4376 .3499 fr en .3264 .3676 .4488 .3995 en es .2723 .3156 .5000 .3443 ) GRAC¸A es en .2953 .3740 .5076 .4545 R E en pt .3716 .3983 .4449 .3263 A pt en .3949 .4272 .4474 .3902 - 1 ( en fr .3189 .3109 .4083 .3336 nt HANSARDS fr en .3206 .3314 .4218 .3749 e m en es .1576 .1897 .2960 .2268 gn LAMBERT es en .1617 .2073 .2905 .2696 Ali en ro .1621 .1848 .2366 .1951 d MIHALCEA ro en .1598 .2042 .2545 .2133 r o W en sv .2092 .2373 .2746 .2357 HOLMQVIST sv en .2121 .2853 .2994 .2881 en tr .1302 .1547 .2256 .1731 CAKMAK tr en .1479 .1571 .2661 .2665 en fr .1096 .2176 .2475 .3125 fr en .1305 .2358 .2762 .3466 en es .0630 .1246 .2738 .3135 1) es en .0650 .1399 .3012 .3574 @ en pt .1384 .3869 .3281 .3866 P ( pt en .1573 .4119 .3661 .4190 n o en ar .0385 .1364 .0995 .1364 ti c ar en .0722 .2408 .1958 .2825 u d WIKTIONARY en ﬁ .0213 .1280 .0887 .1367 n I ﬁ en .0527 .1877 .1597 .2477 y ar en he .0418 .1403 .0985 .1284 n o he en .0761 .1791 .1701 .2179 cti en hu .0533 .2299 .1679 .2182 Di hu en .0810 .2759 .2234 .3204 en tr .0567 .2207 .1770 .2245 tr en .0851 .2598 .2069 .2835 Average* .1640 .2505 .2856 .2867 Top1 0 3.5 15 13.5 Table1:Theperformanceoffourstate-of-the-artcross-lingualembeddingmethods.*Averagesacrosstwodifferentmetrics. From Table 2 we learn that the existing em- manceModel-1andothersentence-IDmethods. bedding methods are not really better than IBM Model-1. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We propose using a third vari- w and a target-language word w – are each ant, sentence-ID SGNS (SID-SGNS), which sim- s t other’stranslation: ply applies SGNS (Mikolov et al., 2013b) to the word/sentence-IDmatrix(see§5.1). Dice(w ,w ) = 2·S(ws,wt) (4) s t S(ws,∗)·S(∗,wt) Table 3 compares its performance (Bilingual SID-SGNS) to the other methods, and shows that whereS(·,·)isthenumberofalignedsentencesin indeed, this algorithm behaves similarly to other thedatawherebothargumentsoccurred. sentence-ID-based methods. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 834> 

<Paper ID = 835>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Number of labels, samples, vocabu- Table2: Out-of-vocabularyratesfornon-hapax larysizes(incl. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Accuracy results on our ﬁve datasets for the Re-embedding Passive-Aggressive (RPA) against standardPA-II(PA).Foreachtask,thebestresultsforeachembeddingmethod(acrossdifferentdimen- sions)hasbeengreyedout,whiletheoverallhighestaccuracyscorehasbeenunderlined. </Abstractive Summary>  <Extractive Summary> =  learnedfromrandomvectorsachieveperformance thatareoftenonaparorhigherthanthosegivenby 5.5 Results HLBL, HPCA or CnW initializations. They actu- Table 3 summarizes accuracy results for the RPA allyyieldthebestperformanceforthetworemain- against those obtained by a PA trained with ﬁxed ing datasets: comp and religion. On these tasks, pre-trainedembeddings. </Extractive Summary>  </Table ID = 3>  </Paper ID = 835> 

<Paper ID = 836>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We randomly generate a ble1),wecomputetheequivalenceclassandthen permutation π on the alphabet and learn a trans- replace the learned operations with ones that re- duction τ (details below). In Table 2 (left), the place a character by the canonical member of its columns “method”, π and τ indicate the method equivalenceclass(e.g.,2→1,3→1). used (W2V or FTX) and whether experiments in Permutation premise. The two uppercase/lowercase conversions shown 7882→1 /r →r ‡@ →‡ “Harrison Ford” is a mention of type “actor”. We substitution 3:;E→→→→1..e predeletion @@@@‡‡HI →→→→HI‡‡ postdeletion elmm@@@l →→→→elmm tt3uh.4neseethRthreeressushhlotolsdldssoonndteevstt.ooptimizeF1andthenuse C→c Results are presented in Table 2 (left). Overall Table 1: String operations that on average do not performance of FTX is higher than W2V in all change meaning. todayintuitasoneword. 3.5 Analysisofngramembeddings 4 Utilization: Tokenization-free Table 2 (right) shows nearest neighbors of ten representationoftext character ngrams, for the A-RANDOM space. 4.1 Methodology Querieswerechosentocontainonlyalphanumeric characters. Table 6: Cosine similarity of ngrams that cross 4.2 Experiments word boundaries and disambiguate polysemous We again use the embeddings corresponding to words. The tables show three disambiguating A-RANDOM in Table 2. We randomly selected ngrams for “exchange” and “rates” that have dif- 2,000,000 contexts of size 40 characters from ferent meanings as indicated by low cosine sim- Wikipedia. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  Table 1: String operations that on average do not performance of FTX is higher than W2V in all change meaning. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Nearest ngram embeddings (rank r ∈ [1,5]) of the position embeddings for “POS”, the posi- tions2/3(best),15/16(monthly),23/24(comic),29/30(book)and34/35(publications)intheWikipedia excerpt“best-sellingmonthlycomicbookpublicationssoldinNorthAmerica” k = 3, k = 9, this means that the position exchange@f ic@exchang ing@exchan min maxP (inexchangefor) (manycontexts) (manycontexts) is the sum of ( k) ngram embeddings (if exchange@f 1.000 0.008 -0.056 3≤k≤9 ic@exchang 0.008 1.000 0.108 all of these ngrams have embeddings, which gen- ing@exchan -0.056 0.108 1.000 erally will be true for some, but not for most po- xchange@ra ival@rates rime@rates (exchangerates) (survivalrates) (crimerates) sitions). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  A.3 Sparsenessintokenization-free Another alternative is to choose one ﬁxed size, approaches e.g., 4 or 5 (similar to (Schu¨tze, 1992)). Many Nonsymbolicrepresentationlearningdoesnotpre- of the nice disambiguation effects we see in Ta- processthetrainingcorpusbymeansoftokeniza- ble 2 (right) and in Table 6 would not be possi- tionandconsidersmanyngramsthatwouldbeig- ble with short ngrams. On the other hand, a ﬁxed nored in tokenized approaches because they span ngram size that is larger, e.g., 10, would make it token boundaries. </Extractive Summary>  </Table ID = 6>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Illustration of the result in Table 4. </Abstractive Summary>  <Extractive Summary> =  We did this in two ngramrepresentations. differentconditions: forabag-of-ngramrepresen- Table 5 shows an example of a context in tationofthecontext(sumofallcharacterngrams) which position embeddings did better than bag- and for the concatenation of 11 position embed- of-ngrams, demonstrating that sequence informa- dings, those between 15 and 25. Our evaluation tion is lost by bag-of-ngram representations, in measureismeanreciprocalrankofthecleancon- thiscasetheexactpositionof“Seahawks”. </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Mean reciprocal rank of text denoising experiment for bag-of-ngram text representation Single vs. </Abstractive Summary>  <Extractive Summary> =  Thissimu- Table3givesfurtherintuitionaboutthetypeof latesatextdenoisingexperiment: ifthecleancon- information position embeddings contain, show- texthasrank1,thenthenoisycontextcanbecor- ingthengramembeddingsclosesttoselectedposi- rected. tionembeddings;e.g.,“estseller”(theﬁrst9-gram Table 4 shows that sequence-preserving po- on the line numbered 3 in the table) is closest to sition embeddings perform better than bag-of- theembeddingofposition3(correspondingtothe ﬁrst“s”of“best-selling”). ThekNNsearchspace bag-of-ngram positionembeddings isrestrictedtoalphanumericngrams. multiple segmentation. The motiva- andpositionembeddingtextrepresentation tion for multiple segmentation is exhaustive cov- 791rep.space similarity r leftcontext center rightcontext 1 correct ks and Seattle Skeahawks thkat led to publik 2 noise(query) ks and Seattle Skeahawks t kat led to publik 3 position-emb .761 1 ks and Seattle Skeahawks thkat led to publik 4 bag-of-ngram .904 1 karted 15 games kfsr the Sekahawks, leadingk 5 bag-of-ngram .864 6 ks and Seattle Skeahawks thkat led to publik Table 5: Illustration of the result in Table 4. “rep. </Extractive Summary>  </Table ID = 4>  </Paper ID = 836> 

<Paper ID = 837>  <Table ID = 3>  <Abstractive Summary> =  Table 3: 20 randomly sampled entity mentions helpsinthecaseoflabelsofmixedtypes. </Abstractive Summary>  <Extractive Summary> =  Examples of such mentions are 100 Feature level transfer learning analysis: We people, It, the director, etc. Table 3 shows 20 observed 4.5% performance increase in micro- randomly sampled entity mentions from test set F1 score of AFET on BBN dataset, after replac- of OntoNotes datasets. Some of these mentions ing hand-crafted features with feature representa- are very generic and likely to be dependent on tions generated by the proposed model. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Performance analysis of the proposed tic Web Conference, ISWC’07/ASWC’07, pages model and AFET on top 10 (in terms of type fre- 722–735,Berlin,Heidelberg.Springer-Verlag. </Abstractive Summary>  <Extractive Summary> =  For this, we look Future work could analyse the effect of label at type-wise performance for the top-10 most fre- noisereductiontechniquesontheproposedmodel, quenttypesintheOntoNotestestdataset. Results revisiting the deﬁnition of clean and noisy labels are shown in Table 4. Compared to AFET, the and modeling label-label correlation in a princi- proposed model performs better in all types ex- pled way that is not dependent on dataset speciﬁc ceptother inthetop-10frequenttypes. </Extractive Summary>  </Table ID = 4>  </Paper ID = 837> 

<Paper ID = 838>  <Table ID = 1>  <Abstractive Summary> =  Table 1: An example of several tweets describ- ing the same event about “Space shuttle Atlantis To extract structured representations of landed at Kennedy Space Center in Florida on newsworthyeventsfromTwitter,unsuper- 2011/07/08’’. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Comparison of the performance of event anIBM3850X5Linuxserverequippedwith1.86 extractiononthethreedatasets. </Abstractive Summary>  <Extractive Summary> =  For example, more than 20% named entities in Dataset I can not be found in the word The performance comparison of event extrac- vocabularyconstructedbasedonDatasetIII. tion results is presented in Table 2. It can be ob- served that the proposed DPEMM achieves better Examples of events extracted by DPEMM and performanceonallthethreedatasetscomparedto DPEMM-WE are shown in Table 3. Extracting city trafﬁc WE gives better extraction results overall com- events from social streams. ACM Transactions on pared to DPEMM as shown in Table 2, it returns IntelligentSystemsandTechnology,9(10):e110206. lower purity results because of some noisy infor- Edward Benson, Aria Haghighi, and Regina Barzi- mationintroducedthroughwordembeddings. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  tion results is presented in Table 2. It can be ob- served that the proposed DPEMM achieves better Examples of events extracted by DPEMM and performanceonallthethreedatasetscomparedto DPEMM-WE are shown in Table 3. It can be ob- thebaselineapproach,withtheimprovementinF- served that the extracted results from DPEMM- measure being 6.1% and 7.7% on Dataset I and WE contain more detailed and accurate infor- II, respectively. </Extractive Summary>  </Table ID = 3>  </Paper ID = 838> 

<Paper ID = 839>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Expected output of end-to-end relation Table 2: Expected output of end-to-end relation extractionsystemforentitymentions extractionsystemforrelations 3 AllWordPairsModel(AWP-NN) thepowersofNeuralNetworksandMarkovLogic Networkstojointlyaddressallthethreesub-tasks We propose a single, joint model for addressing of end-to-end relation extraction. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Expected output of end-to-end relation extractionsystemforentitymentions extractionsystemforrelations 3 AllWordPairsModel(AWP-NN) thepowersofNeuralNetworksandMarkovLogic Networkstojointlyaddressallthethreesub-tasks We propose a single, joint model for addressing of end-to-end relation extraction. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The above rules state that if there is WEM pairs where a word is paired with any subsequent relation between two words x and y then at least word in the sentence, are considered as relation one of them should have label OTH and at least type predictions. Table 4 describes the domains one of them should have entity type label, i.e. a andpredicatesrequiredforgeneratinganMLNfor labelfromdomainetypeotherthanOTH. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Wefollowthesame 1) was set to be 4, hence all word pairs having assumptions made by (Chan and Roth, 2011; length of dependency path more than 4 were as- Li and Ji, 2014; Pawar et al., 2016), which are sumedtohaveNULLlabel. - ignore the DISC relation, do not consider im- plicitrelations(resultingduetointra-sentenceco- 5.3 Results references) as false positives and use coarse-level Table 5 shows the comparative performances entityandrelationtypes. (in terms of micro-F1 measure) for various ap- Direction of Relations: Out of 6 coarse-level proaches. </Extractive Summary>  </Table ID = 5>  </Paper ID = 839> 

<Paper ID = 840>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Freebase predicates for querying geo- example, entities mentioned in a text about world coordinatesoflocations,geo-politicalentities,and politics will be geographically more distant than organizations. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Freebase predicates for querying the be- withabegin,i.e. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: KB coverage of our proposed global co- Smith”) to one entity, and a coreferent surname- herence features. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Results on CoNLL and TAC15 test sets. </Abstractive Summary>  <Extractive Summary> =  ument into sets of mentions that appear near each other. The partitioning is motivated by the in- 3.2 ResultsandDiscussion tuition that a given mention’s immediate context Evaluation results are shown in Table 5. Our provides the most salient information for disam- method improves the linking performance of all biguation,anddrasticallyreducesthesearchspace evaluated EL systems. </Extractive Summary>  </Table ID = 5>  </Paper ID = 840> 

<Paper ID = 841>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Entity extraction results for dependence manceoninﬂuenceterms,whichcontributetothe relations. </Abstractive Summary>  <Extractive Summary> =  keepthesamefeaturesacrossourexperiments. In addition to the baseline CRF we test three 5.2 EntityExtractionforDependence constraint settings: (i) Default refers to the de- Relations coding with the CRF driven enforcement of con- Weﬁrstlookattheentityextractionresultsforde- straints described in Section 3.2; (ii) Parallel pendence relations in Table 1. There is consistent referstotheapplicationoftheParallelﬂowwhich improvement in F scores for variables A and B feeds the sentences into the CRF to identify en- 1 as we reﬁne the application of constraints. While 1 increasesbyabout11%and15%(absolute). Parallel recall for A and B is slightly below the Table 1 shows Parallel results with p1 = 0.5 numbers reached in Default, the 9% absolute in- and p2 = 0.5 and Sequential resultswith p = 0.5 crease in precision for both variables A and B (see Figure 2). We chose those parameter values leads to F improvements for both. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: F scores across p threshold values for Parallel 39.01% 25.44% 30.80 1 Sequentialﬂow. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 841> 

<Paper ID = 842>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Number of samples in training, valdiation, and, test samples in the MCTest-160, MCTest-500, CNN-11K,CNN-22K,CNN-55K,and,Dailymail-55Kdatasets;alongwiththesizeofvocabulary. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Train, validation and test percentage accuracy on CNN-11/22/55K datasets. </Abstractive Summary>  <Extractive Summary> =  levelofperformancewithoutknowledgetransfer. As baselines for knowledge transfer Previously,Chenetal.(2016)annotatedasam- from the Dailymail-55K dataset to CNN- pleof100questionsonCNNstoriesbasedonthe 11/22/55K datasets, Table 2 presents results type of capabilities required to answer the ques- for SW+Dailymail, memory network initialized tion. Wereportresultsforall6speciﬁccategories with word2vec (MemNN+W2V), memory net- in Table 3. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Question-speciﬁc category analysis of percentage test accuracy with only learning and knowl- edgetransfermethodsonCNN-11Kdataset. </Abstractive Summary>  <Extractive Summary> =  As baselines for knowledge transfer Previously,Chenetal.(2016)annotatedasam- from the Dailymail-55K dataset to CNN- pleof100questionsonCNNstoriesbasedonthe 11/22/55K datasets, Table 2 presents results type of capabilities required to answer the ques- for SW+Dailymail, memory network initialized tion. Wereportresultsforall6speciﬁccategories with word2vec (MemNN+W2V), memory net- in Table 3. Even with CNN-11K and Dailymail- work trained on Dailymail (MemNN+SrcOnly), 55K which is roughly 20% of the complete CNN memory network initialized with pre-trained dataset,theproposedmethodsachievesimilarper- 856Model+TrainingMethods Exact Para. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Knowledge transfer results on MCTest-160 and numberofmillionupdateswhiletrain- MCTest-500 datasets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 842> 

<Paper ID = 843>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Fi- Additionally, we skip negations if the negated to- nally,wegeneratepotentialinterpretationsusinga kenisthenounone,asscoringtheirpotentialpos- battery of deterministic rules. Table 1 shows the itive interpretations is straightforward. Out of the outputofeachstepwithtwosamplenegations. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Corpus analysis. </Abstractive Summary>  <Extractive Summary> =  This strategy to selectpotential focihasa fewexceptions toavoid 5 CorpusAnalysis focithatyieldmeaninglessinterpretations. Specif- Table 2 shows basic counts and statistics of the ically,wediscardpotentialfoci: annotated potential positive interpretations. De- • whose root has dependency aux, auxpass, pendency indicates the syntactic dependency be- cop,poss,dep,prt orpunct; tween a token within the potential focus and a to- ken outside the potential focus. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  5. ns pobj 70 9.0 3.99 1.31 u o amod 50 6.4 4.72 0.8 N ccomp 45 5.7 3.62 1.34 other 212 27.3 3.73 1.49 5.1 AnnotationExamples total 777 100.0 3.86 1.38 nsubj 57 28.5 4.12 0.80 Table 3 presents two complete annotation exam- s root 50 25.0 4.82 0.48 e v ccomp 22 11.0 4.91 0.29 ples when the negated token is a noun and adjec- djecti pxocobmj p 2112 106..50 44..1492 11..0352 tive. We show all potential interpretations gener- A atedandthemanuallyassignedscores. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  7 ExperimentalResults We perform two kinds of experiments. First, we 6.1 FeatureSelection score all potential positive interpretations auto- Table 4 lists the full feature set. We extract fea- matically generated (Section 7.1). </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Precision, Recall and F-measure obtained with the test split for instances with the highest score (5 out of 5). </Abstractive Summary>  <Extractive Summary> =  positivelabel,andotherinterpretationsreceivethe The latter evaluation is more suitable, as the ul- negative label. Table 6 presents results in the test timate goal is to identify valid positive interpre- set(Precision,RecallandF-score)forthepositive tations and discard other potential interpretations labelusingthemajoritybaselineandseveralcom- generatedwithourgenerate-and-rankapproach. 867References Association for Computational Linguistics (Volume 1: LongPapers), pages495–504, Berlin, Germany, Pranav Anand and Craig Martell. </Extractive Summary>  </Table ID = 6>  </Paper ID = 843> 

<Paper ID = 844>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (ThistoomaybeanartifactofWordNet’sﬁne granularity.) Wethereforeextendedoursub- 5.2 ResultsandAnalysis stitutesearchtotwolevelsofhypernyms. Table 2 shows the results for the state-of-the-art • TheglossesprovidedbyWordNetsometimes andna¨ıvebaselines,alongwithresultsofourtwo consistofalistofequivalenttermswhichdo basicsystemsand,asbefore,anenhancedversion notappearinthelistofsynonyms. Forexam- 4Weareawareofseveralfurtherlexicalsubstitutionsys- ple,WordNetdeﬁnesonesenseoftheadverb tems (Moon and Erk (2013), O´ Se´aghdha and Korhonen “right”as“precisely,exactly”,thoughitdoes (2014),RollerandErk(2016),SinhaandMihalcea(2011), Szarvasetal.(2013b), andThateretal.(2010)asreimple- notactuallylistthosewordsassynonyms. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  876Table3: SystemperformanceontheSemEval-2012lexicalsimpliﬁcationdataset. System TRnk R@1 R@2 R@3 D-Bees(enhanced)(originalordering) 37.5 71.6 75.5 76.4 D-Bees(enhanced)(unigramordering) 50.9 72.8 75.2 76.3 D-Bees(enhanced)(n-gramordering) 47.1 71.3 74.5 75.7 JauharandSpecia(2012) 60.2 57.5 68.9 76.9 unigramorderingbaseline 58.5 55.9 68.1 76.0 randomorderingbaseline 34.0 32.1 61.2 82.5 asupervisedsystemthatclassiﬁessubstitutesusing 6.2 ResultsandAnalysis acontext-sensitiven-gramfrequencymodel,abag- Table 3 shows the published results for our base- of-wordsmodel,andpsycholinguisticfeatures. At lines, along with the results from the enhanced SemEval-2012itachievedthebestperformancefor D-Bees–based system from §5.2 using various everymetricexceptR@3,whereitwasbeatenonly ranking methods. </Extractive Summary>  </Table ID = 3>  </Paper ID = 844> 

<Paper ID = 845>  <Table ID = 1>  <Abstractive Summary> =  Table 1: BLEU scores (WMT 2015 test set) for 0.1 SMTandNMTmodels(foreigntoEnglish(F→E) 0.0 andEnglishtoforeign(E→F)directions). Table 10: Sentences marked (a) are the input and a.Almost every day the panels in the lounge were open for (b)arePARANETparaphrases. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Correlation (Spearman ρ) of supervised logisticreg 0.589 modelsagainsthumanratingsforparaphrasepairs. </Abstractive Summary>  <Extractive Summary> =  In we hold out 200 phrases. Table 2 presents results our experiments 1,000 trees were trained to mini- for PARANET and PARASTAT usingdifferentlan- mizemeansquareerror. Theregressorwastrained guages as pivots. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Paraphrase detection results (F1) on the 4.4 ParaphraseIdentiﬁcationandSimilarity PIT-2015dataset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Semantic similarity results (Pearson) on PIT-2015 similarity dataset using the features de- thePIT-2015dataset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Results on the Semeval-2015 semantic could be in part due to Czech non-strict word or- similaritydataset. </Abstractive Summary>  <Extractive Summary> =  iBLEU pe- Again, we experimented with one, two, and three nalizesparaphraseswhicharesimilartothesource languagesaspivots,andcomparedPARANETand PARASTAT directly. Our results are summarized in Table 5. The third block in the table presents a 3http://wiki.answers.com/ 888Model PARASTAT PARANET Model Wikianswers Leagues MTC All fr 0.280 0.299 PARASTAT 2.09 2.38 2.23 2.26 de 0.282 0.295 PARANET 1.86 1.94 1.70 1.83 cz 0.280 0.291 Humans 2.17 1.81 2.0 2.0 Gold 0.599 Table 7: Mean Rankings given to paraphrases by Table6: MeaniBLEUacrossthreedatasets. </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Mean Rankings given to paraphrases by Table6: MeaniBLEUacrossthreedatasets. </Abstractive Summary>  <Extractive Summary> =  standard. Table 7 shows the mean ranks given PARANET relies on a relatively simple archi- to these systems by human subjects. An Analy- tecture which is trained end-to-end with the ob- sisofVariance(ANOVA)revealedareliableeffect jective of maximizing the likelihood of the train- of system type. </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  Table 9: Sentences marked (a) are the input and phrasing with bilingual parallel corpora. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 9>  </Paper ID = 845> 

<Paper ID = 846>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  mustpredictthetranslationinthetargetlanguage. In the following sections, we evaluate the per- We report recall at 1 and 5 for the various mod- formanceofourmultilingualwordembeddingsin els listed in Table 1. The evaluation data for comparison with bilingual word embeddings and it-en, es-en, and nl-en pairs was manually previouspublishedmultilingualwordembeddings constructed (Vulic´ and Moens, 2015). We extend (MultiCluster, MultiCCA, MultiSkip and Multi- the evaluation for nl-es pair which do not in- Trans)forthreetasks: bilinguallexiconinduction volveEnglish.7 (§6),monolingualsimilarity(§7)andcrosslingual TheBiWEresultsforpairsinvolvingEnglishin document classiﬁcation (§8). MultiCluster and Table 1 are from Duong et al. (2016), the current MultiCCA are the models proposed from Am- state of the art in this task. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (WS-de) (Finkelstein et al., 2001; Luong et al., This task exploits transfer learning, where the 2013;Luongetal.,2015). document classiﬁer is trained on the source lan- Table 2 shows the result of our multilingual guage and tested on the target language. The word embeddings with respect to several base- sourcelanguageclassiﬁeristransferredtothetar- lines. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  This is motivated gual RCV1/RCV2 corpus (Lewis et al., 2004) bythefactthatitandesareverysimilar. where each document is annotated with labels The trend observed in Table 3 is consistent from 4 categories: CCAT (Corporate/Industrial), with previous observations. Linear transforma- ECAT (Economics), GCAT (Government/Social) tion performs well. Moreover,thebestscoresforeachlanguage labels. pairsareallfrommultilingualtraining,emphasiz- Table 3 shows the accuracy for the CLDC task ingtheadvantagesoverbilingualtraining. for many pairs and models with respect to the baselines. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  For all our models, including Linear, tions still hold. Table 4 shows some examples Joint and Joint + Mapping, the embedding space of such relations where each word in the analogy isavailableformultiplelanguages;thisiswhywe queryisindifferentlanguages. can exploit different relations, such as it→es. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Performance of our model compared with MultiCluster and MultiCCA using extrinsic and intrinsicevaluationtaskson12languagesproposedinAmmaretal.(2016),allmodelsaretrainedonthe samedataset. </Abstractive Summary>  <Extractive Summary> =  lexicons which are similar to our proposed meth- (2016). Table 5 shows that our model achieved ods. Therefore, for a strict comparison8, we train competitive results, best at 4 out of 9 evaluation our best model (Joint + Mapping) using the same tasks. monolingual data and set of bilingual lexicons on thesame12languageswithMultiClusterandMul- 10 Conclusion tiCCA. Table 5 shows the performance on intrin- sic and extrinsic tasks proposed in Ammar et al. In this paper, we introduced several methods for (2016). </Extractive Summary>  </Table ID = 5>  </Paper ID = 846> 

<Paper ID = 847>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The link intensities are deﬁned for each which the edges of the deﬁnition graph are repre- type, by multiplying a manually deﬁned constant sented. Table 1 describes the link types used in link base (a model parameter) by the TF-IDF thiswork. scorecalculatedforthevocabularywithrespectto thetype. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Top closest and farthest to the term termoflengthn: “happy” by Term Deﬁnition Vector, and clos- est Word2Vec (GoogleNews corpus), and GloVe i Create empty list morph cand of mor- (Wikipedia2014+Gigaword)cosinesimilarities. </Abstractive Summary>  <Extractive Summary> =  A value closer to 1 in- (OOV),acharactern-gram-basedattemptofmor- dicates high similarity, a value closer to −1 in- phological decomposition is done and if a com- dicates opposition and a value closer to 0 indi- pletemorphemematchisfoundintheconceptdef- cates unrelatedness. Table 2 shows a compari- inition set, the matched concepts are composed son table between semantic matches obtained us- for the OOV term. This decomposition attempt is ing this method, word2vec (Mikolov et al., 2013) doneasfollows: andGloVe(Penningtonetal.,2014). </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Distribution of deﬁnition vector error representations as a machine translation resource. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 847> 

<Paper ID = 848>  <Table ID = 2>  <Abstractive Summary> =  Table 2: The F scores of an unsupervised WSD 1 approach and the extended Lesk mesure, com- Table 1: The F scores of various unsupervised 1 pared to the F scores of ShotgunWSD based state-of-the-artWSDapproaches,comparedtothe 1 on the extended Lesk measue and ShotgunWSD F scores of ShotgunWSD based on the extended 1 basedonsenseembeddings,ontheSenseval-2En- Lesk measue and ShotgunWSD based on sense glish all-words data set. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  Table 1: The F scores of various unsupervised 1 pared to the F scores of ShotgunWSD based state-of-the-artWSDapproaches,comparedtothe 1 on the extended Lesk measue and ShotgunWSD F scores of ShotgunWSD based on the extended 1 basedonsenseembeddings,ontheSenseval-2En- Lesk measue and ShotgunWSD based on sense glish all-words data set. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: The F scores of an unsupervised WSD 1 to a fair comparison with Schwab et al. </Abstractive Summary>  <Extractive Summary> =  (2015) and the ex- the extended Lesk measure (Torres and Gelbukh, tended Lesk measure (Torres and Gelbukh, 2009) 923on the Senseval-3 English all-words data set. The periments are required to make sure that the per- F scores are presented in Table 3. The empirical formanceboostisconsistentacrossdatasets. </Extractive Summary>  </Table ID = 3>  </Paper ID = 848> 

<Paper ID = 849>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thedatasetconsistsof3 guages with more than 75M speakers, the second differenttestsets,eachcontainingadifferentnum- with more than 10M speakers and the third group beroflanguages,stylesanddocumentlengthscol- containedtherest. Fortheﬁrstgroup,weallowed lected from different sources, see Table 1 for de- atmost10Mcharactersinthetrainingset,thesec- tails: ond group was capped at 5M characters and the EuroGov contains texts in Western European third group was allowed only 1M characters per languages from European government re- languageatmost.7 sources. In total, our ﬁnal training set includes 131 + 1 TCL was extracted by the Thai Computational (HTML)languages,seeFigure2. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Wikipedia are texts collected from a Wikipedia System Alllanguages Commonlanguages Langid.py .567 .912 dump. CLD2 .545 .891 Ourmodel .950 .955 Table 2 summarizes the accuracies of several algorithms on the three test sets. For some algo- Table 3: Results on our test set for short texts. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Results on our test set for short texts. </Abstractive Summary>  <Extractive Summary> =  Themost Section4. commonerrorisconfusingIndonesianwithMod- Results on short texts are reported in Table 3. ern Standard Arabic, which indicates some noise The two other systems, Langid.py and CLD2 in our training data rather than difﬁculty of sep- cover fewer languages and they were trained on arating these two languages. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The evaluation criterion is 10000 bilingual documents divided as follows: thus macro- (M) or micro- (µ) averaged precision 8000 training, 1000 development and 1000 test (P),recall(R)orF-measure(F).11 documents. Weevaluateourmodelontwoexistingtestsets The results on the 1000 test documents are in formultilingualidentiﬁcation,ALTW2010shared Table 4. For algorithms SEGLANG and LIN- task and WikipediaMulti. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  documentsforeachvalueofk. Table 5 shows that our model performs well, 8 Conclusion both when trained on the provided data and when trainedon ourtraining corpus. The model trained Wehavedevelopedalanguageidentiﬁcationalgo- on our dataset performs slightly worse in F , but rithm based on bidirectional recurrent neural net- µ if we simply prevent it from predicting languages works. </Extractive Summary>  </Table ID = 5>  </Paper ID = 849> 

<Paper ID = 850>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Perplexity of language models trained on 8k and 16k sentences for different languages. </Abstractive Summary>  <Extractive Summary> =  Na language modeling and discuss hurdles to be overcome. 4.3 OtherTargetLanguages 5.1 ExperimentalSetup In Table 1 we present results of language model experiments run with other languages used as the The phonemically transcribed corpus5 consists of low-resourcetarget. InthistableEnglishisusedin 3,039 phonemically transcribed sentences which eachcaseasthelargesourcelanguagewithwhich are a subset of a larger spoken corpus. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  isaninterpolationofMKNwithCLWE. Types Tokens 5.2 ResultsandDiscussion Tones 2,045 45,044 Table 3 shows the Na language modeling results. Notones 1,192 45,989 Pre-trained CLWEs do not signiﬁcantly outper- Table 2: Counts of types and tokens across the form that of the non-pre-trained, and MKN out- wholeNacorpus,givenoursegmentationmethod. The limited sentences (to emulate the Na case) and the size scope of this lexicon motivates lemmatization on of the lexicon afforded to the CLWE training is the English side as a normalization step, which adjusted. Thiscanonlyserveasaroughcompari- may be of some beneﬁt (see Table 3). Further- son,sincePanLexislargeandsoa1kentrysubset more, such lemmatization can be expected to re- may contain many obscure terms and few useful duce the syntactic information present in embed- ones. The Na corpus is a collection of each. However, results in Table 3 show that this spokennarrativestranscribed,whiletheWikipedia failed to show improvements. More sophisticated articles are encyclopaedic entries, which makes processingofthelexiconisrequired. </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Counts of types and tokens across the form that of the non-pre-trained, and MKN out- wholeNacorpus,givenoursegmentationmethod. </Abstractive Summary>  <Extractive Summary> =  The gained in exchange for higher polysemy, with an phonemic transcriptions include tones, so we cre- average of 4.1 English translations per Na entry atedtwopreprocessedversionsofthecorpus: with when tones are removed, as opposed to 1.9 when and without tones. Table 2 exhibits type and to- tones are present. Though this situation of poly- ken counts for these two variations. </Extractive Summary>  </Table ID = 2>  </Paper ID = 850> 

<Paper ID = 851>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (2012) used cross-sentence 949consistency features in a translation model, while from the UN Corpora, with the same features on Hardmeier (2012) designed the Docent decoder, the source side. Table 1 presents statistics about whichcanusedocument-levelfeaturestoimprove thedata. thecoherenceacrosssentencesofatranslateddoc- ument. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Prediction of the correct translation ing one as the new translation. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Prediction of the correct translation (accuracy (%) and kappa) and translation quality (BLEU) forrepeatednounsontheChinesetestset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Prediction of the correct translation (accuracy (%) and kappa) and translation quality (BLEU) forrepeatednounsontheGermantestset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Prediction of the correct translation (1st/2nd/None)forrepeatednounsinGerman,in the following BLEU improvements: for ZH/EN, from11.07to11.36,andforDE/EN,from17.10to terms of accuracy (%) and kappa scores, on the 17.67. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Effects of semantic similarity (cosSim) cally the discourse ones) are intended to disam- on classiﬁcation (10-fold c.-v.). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  appear more than twice. Using our dataset, we Table 6 shows the top ten syntactic features for identiﬁed them as noun pairs that share the same ZH/EN and for DE/EN, ranked by information word, i.e. triples of repeated nouns, to which we gain computed using Weka. </Extractive Summary>  </Table ID = 6>  </Paper ID = 851> 

<Paper ID = 852>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Weusedtheaveragelexicalsur- tion labeled as having ‘good’ alignments for our prisal and average syntactic surprisal as idealized workandassumedthat,ineverysentencepair,the measures of the channel capacity required to read SimpleEnglishsentenceshouldberankedaseas- a sentence. While this underestimates the chan- ier than the English sentence (rank=1 < rank=2 nel capacity required to process a sentence, it is in Table 1). This provides a large corpus with atleastinternallyconsistent,insofarasasentence noisy labels, as there are likely to be instances with higher average surprisal overall is likely to where the English and Simple English sentences requireahigherchannelcapacityaswell. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 852> 

<Paper ID = 853>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Dataset properties on: total number of product categories and is thus highly imbalanced. </Abstractive Summary>  <Extractive Summary> =  half of the BU2 dataset. Table 1 shows dataset We also compute the average Kullback-Leibler 971(KL) divergence, KL(p(x)|q(x)), (Cover and in the next iteration. The optimal selection of de- Thomas,1991)betweentheempiricaldistribution cision tree parameters is based on optimizing the overlistingsinbranchesforeachsubtreerootedin f (x,a,w)usingalogisticloss. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  vendor’serror. Tothisend,weremovelistingscor- From Table 2, we observe that most classiﬁers responding to such “out-of-category” merchants tendtoperformwellwhenlog(N/B)isrelatively fromalltop-levelcategories. high. From Fig. 9 and Table 2, it is havetakenhundredsofannotatorsseveralmonths clearthatGBTsarebetteronBU2. accordingtotheestimatesinSunetal.(2014). </Extractive Summary>  </Table ID = 2>  </Paper ID = 853> 

<Paper ID = 854>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Size of the ﬁnal corpus and class distri- vide one speciﬁc example as premise. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We achieve the best results by port. Table 3 shows the macro F1 scores of the combiningallfeatures. support vector machine using individual features and the results of feature ablation tests on the de- For gaining further insights into the character- velopmentsets. </Extractive Summary>  </Table ID = 3>  </Paper ID = 854> 

<Paper ID = 855>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Categories included in the evaluation For a given phrase p, let π (1) be the index of p subsets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Clustering performance assessed by correspondence measures to gold standard cate- Inthissection, weevaluatetheclusterlabels(i.e., gories. </Abstractive Summary>  <Extractive Summary> =  Asevaluationmetrics,weusethemacro-averaged F1score2,andnormalisedmutualinformation3. Given that the size of the vocabulary is very Table 2 compares the clustering performance large, computing the softmax function during achievedbyfourPVmodels(PV-DBOW-WP,PV- stochasticgradientdescentiscomputationallyex- DBOW-W,PV-DM,andPV-CAT)againsttheper- pensive. For faster training, different optimisa- formance of the baselines (i.e., the two versions tion algorithms can be used to approximate the oftheCEDLalgorithmandspectralclusteringvia log probability function. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The average precisionismaximisedwhendocumentsclosestto The PV-DBOW-W and PV-CAT methods yield theselectedphrasebelongtothegoldstandardcat- the best clustering performance on the Reuters egory. Table 3 shows the selected cluster descrip- dataset. Performancegainsoverthethreebaseline torsalignedtothegoldstandardcategories,theav- methodsrangebetween6%−12%(F1score)and erageprecisionandmeanaverageprecisionscores 8% − 9% (normalised mutual information). </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Two documents whose vector embed- (CSUR),41(3):17. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 855> 

<Paper ID = 856>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Example of morphological representation. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  measure;wenotethatourtasksdifferasdescribed Addingthericherlinguisticresourcesresultsin insection2. both improved target precision, recall, and senti- Results on blind test Table 6 shows the results ment scores, with F-measure for positive targets on unseen test data for best-linguistic using D3, reaching67.7forpositivetargetsand80fornega- D3+ATBandwithclustersusingk=8000. There- tive targets. </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Good and bad examples of output by SMARTies. </Abstractive Summary>  <Extractive Summary> =  Acknowledgments Ingeneral,ouranalysisshowedthatoursystem does well on posts where targets and subjective This work was supported in part by grant NPRP language are well formed, but that the important 6-716-1-138 from the Qatar National Research targetidentiﬁcationtaskisdifﬁcultandmademore Fund,byDARPADEFTgrantFA8750-12-2-0347 complex by the long and repetitive nature of the and by DARPA LORELEI grant HR0011-15-2- posts. Table 7 shows two examples of the trans- 0041. Theviewsexpressedarethoseoftheauthors latedoutputofSMARTies,theﬁrstonmorewell- anddonotreﬂecttheofﬁcialpolicyorpositionof formed text and the second on text that is more theDepartmentofDefenseortheU.Sgovernment. </Extractive Summary>  </Table ID = 7>  </Paper ID = 856> 

<Paper ID = 857>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Al- in order to count the number of positive and neg- though the test set has not been released with ative emoticons in each tweet and then were re- their gold annotations, the evaluation platform of moved from the text. Table 1 shows the list of the TASS competition is still open1 for registered positiveandnegativeemoticons,whichhavebeen users. Thisplatformallowsparticipantstosubmit taken from Wikipedia2. used the iSOL lexicon (Molina-Gonza´lez et al., This model is available for research community 2013), a list composed by 2,509 positive words andwasbuiltfromseveralSpanishcollectiontexts and 5,626 negative words. As described before, suchasSpanishWikipedia(2015),theOPUScor- for the emoticons we used the listed in Table 1, pora (Tiedemann and Nygaard, 2004) or the An- but also added to the positive ones the number of cora corpus (Taule´ et al., 2008), among others. laughs detected; and also, we included the num- It contains nearly 1.5 billion words (Cardellino, ber of recommendations present in the form of a 2016). </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  To train this second of the word embeddings on the performance of model of word embeddings, we used the corpus our CNN model. Table 2 compares the models provided by the TASS organizers (68,000 tweets) based on the type of word-embeddings used as aswellasaveryextensivecollectionof8,774,487 input of the network: CNN-rand, CNN-wiki and tweets, which we collected during 2014. To do CNN-Twitter. </Extractive Summary>  </Table ID = 2>  </Paper ID = 857> 

<Paper ID = 858>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Predicted conditional probabilities at ev- itself from the conditional dependence. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Sentiment classiﬁcation accuracies from calculated. </Abstractive Summary>  <Extractive Summary> =  dataset partitioning. Even though they are not di- Theproposedmodelshaveachievedbetterresults rectlycomparable,ourresultsarebetterthanother than the previously published results on the same previously published results reported in Table 2 dataset with the same partitioning. In addition, whereadifferentdatasetpartitioningisused. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Examples of the highest/lowest ranked data. </Abstractive Summary>  <Extractive Summary> =  Furthermore, no de- convergence. To show the efﬁciency of our sen- tails are provided about how the model is exactly tence ranking methodology, Table 3 shows exam- modiﬁed and how the left and right dependencies ples of the highest and lowest ranked sentences aremaintainedovertimesteps. frompositiveandnegativetrainingdata. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Sentiment classiﬁcation accuracies mea- beencompressedintoroughlyan suredontheIMDBdataset. </Abstractive Summary>  <Extractive Summary> =  Forasentence 8 ExperimentalResults withanegativesentiment,thetargetoutputsareset to zeros at all time steps. Since the sigmoid func- Table 4 shows the results of our experiments. tion provides output values in the interval [0,1], All the neural networks in this work are trained 1029andoptimizedusingourownCURRENNTtoolkit correctly classiﬁed by the discriminative BLSTM (Weninger et al., 2014). </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Examples of reviews correctly classiﬁed outperforms the use of both LSTM LM and bythecBLSTMLMclassiﬁer. </Abstractive Summary>  <Extractive Summary> =  In addition, whereadifferentdatasetpartitioningisused. indicative comparisons have been made with the For illustration, Table 5 shows two examples previously published results on the same dataset of positive and negative reviews that could not be with different partitioning. Using model combi- 1030nation, we could achieve further performance im- George E. </Extractive Summary>  </Table ID = 5>  </Paper ID = 858> 

<Paper ID = 859>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Syntacticpatternsinthesystem. </Abstractive Summary>  <Extractive Summary> =  One similar network structure to our classiﬁer (Section 3.2) and an opinion expression model is proposed in (Miwa and Bansal, 2016). classiﬁer(Section3.3) They jointly extract entities and relations using Table 1 lists the patterns used in our system. two LSTM models. Table2: Statisticsoftheopinionrelationdatabase. 4 Experiments The relations are extracted by patterns in Table 1 4.1 Conﬁgurations andnormalizedbyremovingleadingarticles,pro- nounsandcopulasofopinionexpressionsandtar- We extract opinion relations on a subset of Ama- gets. Allnumbersarein106. </Extractive Summary>  </Table ID = 1>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  It ﬁrst identiﬁes words in the sentence level biLSTM and not include depen- general purpose opinion lexicon, then ﬁnds the dencypathh andtheopinionexpressionclas- nearest noun or verb phrase to them as their D siﬁer (the conﬁguration of “NN” equals “biL- opiniontargets. STM+LOBTR” in Table 5). In order to build 3https://github.com/AntNLP/OpinionRelationCorpus 4http://nlp.stanford.edu/software/lex-parser.shtml 6Weselectthefeaturesonthedevelopmentset. tween USAGE and our dataset. For example, we Next, we test our neural network model with don’t annotate pronouns as opinion targets while various settings in Table 5. First, we com- USAGE does (e.g., (“love”, “it”) is a proper an- pare models with different conﬁgurations on notation in USAGE). </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Some inter- “NN”. esting opinion expressions added by the classi- We have several observations on Table 4. ﬁer are “not even enough”, “became extremely First,performancesof“Adjacent”arepoor,which hot”,“*just*enough”,“arriveddamaged”,“looks means that we do need some advanced linguistic coolncute”,whichshowsthattheclassiﬁercould featuresforthetask. The setting “biL- distantsupervisionsprovidenotableperformances STM+B” only uses the CNN corresponding to gainsthandirectpatternmatching. the words in B (i.e., the words between O and T); “biLSTM+OBT” uses three CNN on words 8Bothbaselinesdon’treportend-to-endperformances.As in B,O,T; “biLSTM+LOBTR” involves all ﬁve areference,in(JebbaraandCimiano,2016),theF1ofopin- CNNs (equals to “NN” in Table 4). We see ionexpressionandtargetextractionare50%and67%. </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Results on USAGE corpus. </Abstractive Summary>  <Extractive Summary> =  thenewlyaddedwordsbringalotofnoiseintothe opinion lexicon, which affect the accuracy nega- 4.3 ResultsonUSAGECorpus tively. Third, while “Pattern” has the highest pre- cision in all systems, distantly supervised meth- In order to compare with fully supervised meth- ods (“LR” and “NN”) help to improve recall and ods,weevaluateourmodelsonUSAGEcorpusin achieve better F1 values (except on the Pet do- Table 6. To build the distantly supervised mod- main). </Extractive Summary>  </Table ID = 6>  </Paper ID = 859> 

<Paper ID = 860>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thememoryhierarchyonthedeviceislaidout to maximize the data throughput. Table 1 shows 2.1 Architecture theamountofcoresavailableforexecutionaswell as the amount of memory available on a Kepler CUDAcores(alsoknownasscalarprocessors)are basedGPU.Registersarethefastesttypeofmem- grouped into diﬀerent Streaming Multiprocessors ory on the device, and this memory is private to (SM) on the graphics card. The number of cores eachthreadrunningonablock. If the kernel is successfully launched, the device. Table 1 shows the number of stream- each block in the grid will get assigned to a ing multiprocessors and the number of cores per SM. Each SM will execute 32 threads at a time multiprocessoronaK40GPU.Multipleblocksin (also called a warp) in its assigned block. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  This is because 0.04 thelatterneedtoaddlog-probabilities(lines6and 13ofAlgorithm2),whichinvolvesexpensivecalls 0.02 totranscendentalfunctions. 0 0 10 20 Nu 3m0 ber of 40input w o50rds 60 70 80 7.4 ComparisonacrossGPUarchitectures Table 2 compares the performance of the Kepler- Figure4: Viterbidecodingtimesfor1000individ- basedK40,wherewedidmostofourexperiments, ualtestsentencescomparedforourserial,parallel, with the Maxwell-based Titan X and the Pascal- andcuSPARSEimplementations(TitanX). basedTeslaP100. Our parallel implementations addinganotherlayerofcomplexitytotheprocess. were compiled using architecture speciﬁc ﬂags The timings for OpenFST and Carmel on Table 2 (-arch=compute_XX) to take full advantage of includecomposition the architectural enhancements described in this section. 7.3 Results 7.5 Comparisonagainstamulti-core Table 2 shows the overall performance of our implementation Viterbialgorithmandthebaselinealgorithms. were compiled using architecture speciﬁc ﬂags The timings for OpenFST and Carmel on Table 2 (-arch=compute_XX) to take full advantage of includecomposition the architectural enhancements described in this section. 7.3 Results 7.5 Comparisonagainstamulti-core Table 2 shows the overall performance of our implementation Viterbialgorithmandthebaselinealgorithms. Our parallel implementation does worse than our se- Table 2 shows how our parallel implementation rial implementation when the transducer used is on a GPU compares against a multi-core version small (presumably due to the overhead of kernel of our serial Viterbi algorithm implemented in launches and memory copies), but the speedups MPI. 7.3 Results 7.5 Comparisonagainstamulti-core Table 2 shows the overall performance of our implementation Viterbialgorithmandthebaselinealgorithms. Our parallel implementation does worse than our se- Table 2 shows how our parallel implementation rial implementation when the transducer used is on a GPU compares against a multi-core version small (presumably due to the overhead of kernel of our serial Viterbi algorithm implemented in launches and memory copies), but the speedups MPI. We chose MPI since it supports distributed increaseasthesizeofthetransducergrows,reach- andsharedmemoryunlikeOpenMPthatsupports ingaspeedupof5x. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Our GPU implementations of the achieve higher speedups than a GPU on smaller forward and backward algorithms, and for- datasets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 860> 

<Paper ID = 861>  </Paper ID = 861> 

<Paper ID = 862>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Details of the best systems pertaining to the PBMT and NMT paradigms submitted to the WMT16newstranslationtaskforeachlanguagedirection. </Abstractive Summary>  <Extractive Summary> =  1065wasextremelylow(EN→Turkish)andhencemost System CS DE FI RO RU probably not representative of the state-of-the-art FromEN inNMT. PBMT 23.7 30.6 15.3 27.4 24.3 Table 1 shows the main characteristics of the NMT 25.9 34.2 18.0 28.9 26.0 best PBMT and NMT systems submitted to the IntoEN WMT16newstranslationtask. Itshouldbenoted PBMT 30.4 35.2 23.7 35.4 29.3 that all the NMT systems listed in the table fall NMT 31.4 38.7 - 34.1 28.2 under the encoder-decoder architecture with at- Table2: BLEUscoresofthebestNMTandPBMT tention (Bahdanau et al., 2015) and operate on systemsforeachlanguagepairatWMT16’snews subword units. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The higher the value, n5 NMT and PBMT systems submitted to each thelargeristheoverlap. language direction and checking their pairwise overlap in terms of the chrF1 (Popovic´, 2015) Table 3 shows the results. We can observe the same trends for all the language directions, 4Wereporttheofﬁcialresultsfromhttp://matrix. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Perplexity scores for the outputs of the are PBMT systems belonging to two different best NMT and PBMT systems on language mod- paradigms (pure phrase-based and hierarchical). </Abstractive Summary>  <Extractive Summary> =  That monolingual corpora, available for all the lan- manualevaluationonlycoveredlanguagedi- guagesconsidered.7 rections into English. In this experiment, we Table 4 shows the results. For all the language extendthatconclusiontolanguagedirections directionsconsideredbutone,perplexityishigher outofEnglish. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  temandthereorderingsinthereferencetranslation In order to measure the amount of reorder- can also be estimated as the distance between the ing, we used the Kendall’s tau distance between corresponding word alignments. Table 5 shows word alignments obtained from pairs of sen- thevalueofthesedistancesforthelanguagepairs tences (Birch, 2011, Sec. 5.3.2). </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Pearson correlations between sentence directionEN→FI. </Abstractive Summary>  <Extractive Summary> =  creasingwithsentencelengthandinfactwefound a strong negative Pearson correlation (-0.79) be- 54 tween sentence length and the relative improve- 52 ment(chrF1)ofthebestNMToverthebestPBMT 50 system. 48 1 Thecorrelationsforeachlanguagedirectionare F hr 46 c shown in Table 6. We observe negative corre- 44 PBMT lations for all the language directions except for 42 NMT DE→EN. </Extractive Summary>  </Table ID = 6>  </Paper ID = 862> 

<Paper ID = 863>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The data is restricted distinguishable by means of supervised and un- to sentence-pairs originally produced in English, supervised classiﬁcation (Baroni and Bernardini, French, or German. Table 1 provides statistics on 2006; Volansky et al., 2015; Rabinovich and the two datasets. We also release the full list of Wintner, 2015). </Extractive Summary>  </Table ID = 1>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  inal markers of the target language. In partic- GenderclassiﬁcationresultsintheTEDdataset ular, the markers observed in translated English are presented in Table 6. The classiﬁcation accu- mirror their original French counterparts, in the racy of English originals is 80.4%. </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The translations of the German female sen- amuchlesserextent)thetargetlanguage. tence into English, as presented in Table 7 (bot- tom),furtherhighlightthisphenomenonbychoos- Capturing the “personalization” effect Both ing the English female marker think in its person- manual- and all machine-translations of Europarl alized translation over the more neutral consider are tested on a strictly identical set of sentences; and believe in the manual and baseline versions, therefore,theperformancegapintroducedbyper- respectively. sonalizedSMTmodelscanbecapturedbyasubset of sentences misclassiﬁed by the baseline model, 8 Conclusions butclassiﬁedcorrectlywhenapplyingamoreper- sonalized approach. Thiswork els. Table 7 (top) shows manual, baseline, and leaves much room for further research and prac- personalized machine translations of examples of tical activities. Authors’ personal traits are uti- French and German sentences. </Extractive Summary>  </Table ID = 7>  </Paper ID = 863> 

<Paper ID = 864>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Comparison of different BLI systems ods from prior work with automatically learned whichuseonlyword-levelinformation. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Results of the combined model (word- sentations wechoose SGNSembeddings and the LSTM consists of 128 in memory cells in each levelSGNSplusCHAR-LSTMjoint ). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 864> 

<Paper ID = 865>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Cosine similarity for sample word vec- documents where the entity salience is above a tors. </Abstractive Summary>  <Extractive Summary> =  Althoughthetwoembeddingsproducesimi- general corpus, and embeddings trained on our larresults,theresultingwordvectorshavenotice- business-news corpus. For NEs, we use raw able differences, as can be seen in Table 1. The counts, TF-IDF and salience. </Extractive Summary>  </Table ID = 1>  </Paper ID = 865> 

<Paper ID = 866>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Steadily blocks with a certain number of ﬁlters. The best increasing the depth of the network by adding conﬁgurations we observed for depths 9, 17, 29 moreconvolutionallayersisfeasiblethankstothe and 49 are described in Table 2. We also give the limited number of parameters of very small con- numberofparametersofallconvolutionallayers. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  All ferred to (Zhang et al., 2015) for more details on experiments are performed on a single NVidia the construction of the data sets. Table 4 summa- K40 GPU. Unlike previous research on the use rizes the best published results on these corpora of ConvNets for text processing, we use temporal we are aware of. </Extractive Summary>  </Table ID = 4>  </Paper ID = 866> 

<Paper ID = 867>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Incaseof1.–4.,weevaluateallpremiseag- as it applies even when all arguments are ranked gregation methods from Section 4: (a) Minimum, equally(unlike,e.g.,thePearsoncoefﬁcient). (b)Average,(c)Maximum,and(d)Sum.Whilewe areawarethatmoresophisticatedrankingapproa- Results Table 2 shows that the highest rank cor- ches are possible, the considered selection cap- relation is clearly achieved by our PageRank ap- turesprinciplepropertiesofarguments: proach, namely, when using the Sum aggregation. WhileaKendall’sτ of0.28isnotveryhigh,itcan 1. </Extractive Summary>  </Table ID = 2>  </Paper ID = 867> 

<Paper ID = 868>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Annota- tionsareconductedusingthesessionaudiorecord- 4.2 MICoding ingalongwithitstranscriptusingNvivo,2 aquan- MITI coding was conducted by a team of three titativeanalysissuiteforbehavioralcodingthatal- counselors who have extensive experience with lowsselectingfreetextandassigningittoagiven MI.1 Priortotheannotationphase,annotatorspar- category. Table 2 presents an excerpt of a session ticipated in a coding calibration step where they transcript. As observed, a talk-turn can comprise discussedthecriteriaforsentenceparsing,thecor- multipleutterances. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  Table 10: Most discriminative syntactic features for Questions (QUEST), Reﬂections (REFL), Simple reﬂection(S-REFL)andComplexreﬂection(C-REFL). </Abstractive Summary>  <Extractive Summary> =  PersuadingwithPermission(2100). The ICC values reported in Table 1 show no- ticeable high agreement for the Question and Re- 5 LinguisticFeaturesforMIbehaviors ﬂectioncodeswithscoresrangingbetween0.89to In order to explore linguistic patterns related to 0.97,whichareconsideredexcellentagreementin counselor behaviors, we analyze their deﬁnitions theMIliterature(Jelsmaetal.,2015). Theremain- andusage. over, we observe an interesting difference in the To gain further insight into the syntactic pat- verb tenseusage for Simpleand Complex Reﬂec- terns, we extract the most predictive features for tiondetection: productionrulesforSimpleReﬂec- each classiﬁcation model. Table 10 presents a tion include present tense while production rules summary of the top ten production rules associ- forComplexReﬂectionincludepasttense. ated to Question (Quest), Reﬂection (Reﬂ), Sim- ple Reﬂection (S-Reﬂ), and Complex Reﬂection (C-Reﬂ). </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  a majority class baseline, which is the percentage 6 ExperimentsandResults ofinstancescorrectlyclassiﬁedwhenselectingby default the most frequent category in the training After the feature extraction, we explore whether data. these features can be used as predictors of coun- Table 3 summarizes the classiﬁcation perfor- selor behaviors. We ﬁrst focus on the prediction mance for each set of features in the detection of of reﬂections and questions, as they represent the reﬂections. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ance and chooses the most appropriate code ac- cording to the MITI guidelines. Hence, we focus Table 4 presents the classiﬁcation performance on the identiﬁcation of Reﬂection utterances re- for the simple (S-Reﬂ) and complex reﬂections gardlessofbeingcomplexorsimple. Thelearning (C-Reﬂ). </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The class dis- KappavaluesinTable1). tribution for each set is shown in Table 5. We ex- Finally, we investigate whether larger amounts clude 16 sessions as they correspond to miscella- of training data can be helpful to discriminate be- neouschangegoals. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Weplot usingthelinguisticfeaturesdescribedbefore. Re- thelearningcurvesofthedifferentsetsoffeatures sults are shown in Table 6. From this table, we using incremental amounts of data as shown in can derive interesting observations. </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We thus looked at the top syntactic Syntax 90.48% 0.92 0.92 0.87 Allfeatures 86.57% 0.82 0.82 0.82 features generated for each classiﬁcation model GRU 92.8% 0.89 0.92 0.90 and their corresponding terminal nodes and part of speech tags. Table 7 shows sample words Table 8: Classiﬁcation results for counselor ques- for nouns, verbs, and adjectives used by coun- tions (QUEST), other MITI codes + transition selors while formulating reﬂections for three tar- (unannotated) utterances (ALL), and other MITI get behavior changes. From this table we no- codes(OTHER). </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Classiﬁcation results for counselor ques- for nouns, verbs, and adjectives used by coun- tions (QUEST), other MITI codes + transition selors while formulating reﬂections for three tar- (unannotated) utterances (ALL), and other MITI get behavior changes. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 9>  </Paper ID = 868> 

<Paper ID = 869>  <Table ID = 1>  <Abstractive Summary> =  Table 1: An example of transforming an input text according to DV-MA and DV-SA algorithms using differentvaluesofk. </Abstractive Summary>  <Extractive Summary> =  1: TokenizeText The text distortion method described in Grana- 2: foreachtokentinTextdo dos, et al. (2011; 2012) has also been applied to 3: iflowercase(t)∈/ Wk then the input text of Table 1 for k=1,000. In com- 4: replaceeachdigitintwith# parison to that method, the proposed approach is 5: replaceeachletterintwith* differentinthefollowingpoints: 6: endif 7: endfor • We replace the occurrences of the least fre- quent words rather than the most frequent We call this method Distorted View with Mul- words since it is well known that function tiple Asterisks (DV-MA). </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Top ﬁfteen words with respect to χ2 in news). </Abstractive Summary>  <Extractive Summary> =  frequent words of the English language. For each To make this difference among the above cor- model, the appropriate parameter settings for n, pora more clear, Table 2 shows the top ﬁfteen f , and k are estimated based on grid search as t wordsofeachcorpuswithrespecttotheirχ2value describedinSection4. andatotalfrequencyofatleastﬁve. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Then,thebestmodels signiﬁcantlyreducedfeaturesetsincomparisonto are applied to the test corpus. Table 3 presents the CCAT-10 corpus indicating that in cross-topic the results of this experiment. As can be seen, conditions the least frequent features are not so the baseline models are the most effective ones useful. Notethattheresultsfor MAequallyeffectivetothebaseline. Ontheother CCAT-10 are directly comparable to Table 3. On hand,intheGuardiancross-topiccorpus,thepro- theotherhand,fortheGuardiancorpuswepresent posed DV-MA and DV-SA models are better than theaverageperformanceofallpossible12combi- the baseline for all examined k values. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Accuracy results of closed-set attribution on the Guardian corpus in cross-genre conditions. </Abstractive Summary>  <Extractive Summary> =  is again the cross-topic variation since validation and test corpora do not share thematic properties. Next, we applied the examined models on the Table 5 presents the results for all tested models. opinion articles of the cross-topic Guardian cor- Again, the proposed distortion-based models per- pus as follows: one thematic category was used form much better in comparison to the baseline as training corpus, another was used as validation models. </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Accuracy results of closed-set attribution on the Guardian corpus in cross-topic conditions. </Abstractive Summary>  <Extractive Summary> =  Since there are four thematic categories rest of the models. Note also that the feature set in total, all 12 combinations were examined and size for most models in this experiment is further the resultsare shownin Table 4. Herethe results reducedincomparisontothepreviousexperiment. 5.4 EffectofParameterk corpus and another thematic category as test cor- pus). This is not directly comparable to Table 4 Sofar,theparametersettingsoftheproposedmod- (wherethetestcorpusofeachcaseconsistsoftwo els, as well as the baseline methods, were ob- thematiccategories). tained using a validation corpus. </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: AUC-ROC scores of the examined authorship veriﬁcation models. </Abstractive Summary>  <Extractive Summary> =  Moreover,themostfre- 6 AuthorshipVeriﬁcation quent wordsfor eachlanguage areextracted from thecorrespondingtrainingcorpus. 6.1 Corpora Recently, the PAN evaluation campaigns fo- 6.3 Results cused on the authorship veriﬁcation task and sev- eral benchmark corpora were developed for this Table 6 shows the performance of the authorship task (Stamatatos et al., 2014; Stamatatos et al., veriﬁcation models on the 10 PAN benchmark 2015). Each PAN corpus consists of a number of corpora based on the area under the Receiver- veriﬁcation problems and each problem includes Operating-Characteristiccurve(AUC-ROC).This a set of known documents by the same author evaluation measure was also used in PAN evalu- andexactlyonedocumentofunknownauthorship. SA models seem more competitive than DV-MA Within each veriﬁcation problem all documents intheauthorveriﬁcationtask. belong to the same genre and fall into the same Table 6 also shows the performance of DV-Opt thematic area. Details of these corpora are pre- thatcorrespondstothebestmodel(eitherDV-MA sentedin(Stamatatosetal.,2014). </Extractive Summary>  </Table ID = 6>  </Paper ID = 869> 

<Paper ID = 870>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We also experiment with a set of global features, by which we mean features that are expressed in Yˆ = arg max S(X,Y) (5) k termsofmultiplelocallabels. Theglobalfeatures Y are speciﬁed in Table 2. Global features are de- Weuseintegerlinearprogramming(ILP)tosolve ﬁned by a feature function Φ (X,Y) → Rr global the prediction problem in Equation 5. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Temporal label dependencies expressed as integer linear programming constraints. </Abstractive Summary>  <Extractive Summary> =  Sub-sampling Constraints are expressed in terms of the binary FortheTLINKsub-task,wehaveaverylargeneg- decision variables used in the integer linear pro- ativeclass(NO LABEL)andarelativelysmallpos- gram. itiveclass(theotherTLINKlabels)oftrainingex- In Table 3, constraints CCtrans, and CBtrans amples. Tospeeduptrainingconvergenceandad- model transitivity of CONTAINS, and BEFORE re- dress class imbalance at the same time, we sub- spectively. Ourmotivationfor DCTR). However, joint learning on a document not using sentence based candidate generation is level provides the ﬂexibility to formulate con- thattheclinicalrecordscontainmanyungrammat- straints connecting the labels of both tasks, such icalphrases,bulletpointenumerations,andtables as the last four constraints in Table 3, resulting in that may result in missing cross-sentence relation a more consistent labeling over both tasks. Sim- instances(LeeuwenbergandMoens, 2016). In general it can be noticed FOREonthetrainingdata. that in our experiments using the temporal label constraints from Table 3 slightly increases DCTR 4.4 Results performance, but slightly decreases TLINK per- Our experimental results on the THYME test set formance. Areasonforthiscanbethatthemodel arereportedinTable5. Areasonforthiscanbethesequen- with random sub-sampling of negative TLINKS tialdependencyofDCTRlabels,alsoexploitedby (SPrandomsub-sampling) it can be seen that a good (Khalifa et al., 2016), using the sequential CRF. selection of negative training instances is very The second global feature, Φ , in fact models drtl important for learning a good model (again thesametypeofdependenciesasthelastfourcon- P<0.0001), and resulted in our model to im- straints in Table 3, relating the TLINK relations provethestate-of-the-artby1.4onthe CONTAINS with the DCTR labels of each TLINK argument, TLINKtask3. however as a soft dependency and not as a hard constraint. </Extractive Summary>  </Table ID = 3>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  3.5 Training 3.5.2 LocalInitialization Thetrainingprocedurefortheaveragedstructured Toreducetrainingtime,wedon’tinitializeλwith perceptron is given by Algorithm 1, for I itera- ones,butwetrainaperceptronforbothlocalsub- tions, on a set of training documents T. Notice tasks,basedonthesamelocalfeaturesmentioned that the prediction problem is also present during in Table 1, and use the trained weights to initial- training, in line 6 of the algorithm. Weight vec- ize λ for those features. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Dataset statistics for the THYME sec- ing we lowercase the tokens. </Abstractive Summary>  <Extractive Summary> =  Some statistics about TLINKSintheTHYMEcorpus. the dataset can be found in Table 4. F-measure is used as evaluation metric. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Results on the THYME test set. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 870> 

<Paper ID = 871>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Comparison of PSO with other ﬁlter (Information Gain & Correlation) and wrapper (G.A & RFE)basedfeatureselectiontechnique basedapproach. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Performance evaluation on GENIA, AiMed and GENETAG data sets using various word em- bedding(WE)featurestrainedondifferentunlabeleddata. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Comparisons (in terms of F-score) 7 Conclusions&Futurework between whole word embedding features using In this paper we have investigated the effect of WE(4)andthePSOselectedwordembeddingfea- word embedding features in addition to the hand- tures excluding handcrafted features. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 871> 

<Paper ID = 872>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Statistics for drug-gene interaction can- ments (e.g., overexpression). </Abstractive Summary>  <Extractive Summary> =  doubled the number of overall candidates, while 4.1 KnowledgeBase being reasonably small so as not to introduce too We used the Gene Drug Knowledge Database many unlikely ones. Table 1 shows the statis- (GDKD) (Dienstmann et al., 2015) for distant tics of drug-gene interaction candidates identiﬁed supervision. Figure 2 shows a snapshot of the in PubMed Central articles. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Only de- to other edge types (weight 1) that are arguably pendency and adjacency edges were included in more semantics-related. Table 2 shows the aver- these experiments. Table 3 shows the results. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Table 2 shows the aver- these experiments. Table 3 shows the results. Pe- age test accuracy for single-sentence and cross- nalizing adjacency produces large gains; a harsh sentence extraction with various edge types and penalty is particularly helpful with fewer paths. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Unique drug-gene interactions ex- tractedrelationsbyvaryingtheprobabilitythresh- tracted from PubMed Central articles, compared old. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Numbers of unique genes and drugs in ter. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Sample precision and error percent- extractionforknowledgebasepopulation. </Abstractive Summary>  <Extractive Summary> =  instances,thistimewithoutoverlap. Table 6 showed the sample precision and per- 4.6 PubMed-ScaleExtraction centageoferrorsduetoentitylinkingvs. relation Ourultimategoalistoextractknowledgefromall extraction. </Extractive Summary>  </Table ID = 6>  </Paper ID = 872> 

<Paper ID = 873>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Different MIML algorithms for entity m[i] = g(E (cid:12)H) (10) :,i:i+w−1 typing, and the aggregation function they use to getcorpus-levelprobabilities. </Abstractive Summary>  <Extractive Summary> =  Each c is represented by a vector resentationoftypet. i c~ ∈ Rh, which is our type-aware representation Table 1 summarizes the differences of our i ofcontextdescribedinSection4.1. MIML methods with respect to the aggregation TolearnP(r|z),weusethemulti-instance(MI) functiontheyusetogetcorpus-levelprobabilities. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: P@1, Micro F1 for all, head and tail en- recording ... </Abstractive Summary>  <Extractive Summary> =  Results. Table 3 shows results for P@1, micro CNN+MIML-ATT (10) performs comparable F and MAP. For F , we report separate results to EntEmb (11), with better micro F1 on all and 1 1 for all, head (frequency higher than 100) and tail tailentitiesandworseMAPandmicroF1onhead (frequencylessthan5)entities. Then, we andCNN+JOINT-TRAIN.WithWEIGHTED,the feed those embeddings into an MLP which com- relations PPL.deceased PER.place of death and putesarepresentationthatweuseforthetypepre- LOC.LOC.containedby are improved the most diction. This corresponds to the EntEmb model (from 36.13 to 53.73 and 49.04 to 64.19 F , 1 presented in Table 3 (line 11). For joint train- resp.). </Extractive Summary>  </Table ID = 3>  </Paper ID = 873> 

<Paper ID = 874>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Correlation coefﬁcients between the Dice coefﬁcient and the cosine similarity. </Abstractive Summary>  <Extractive Summary> =  cosinesimilarity. ToexaminetherelationbetweentheDicecoef- Note that although we use a parallel corpus for ﬁcientandthecosinesimilarityinmoredetail,we evaluation, it does not mean that we can simply plot these values for the bottom row in Table 1, useaparallelcorpusforﬁndingmeaningchanges i.e., where the dimensions for Japanese number in loanwords. Parallel corpora are usually much 600 and the dimensions for English number 300. abletoﬁndmeaningchangesinloanwordsthatdo notappearinaparallelcorpus. 4.3 DetailedEvaluationonKnownChange The results for different Japanese and English dimensions, dim and dim , are shown in Here, we conduct a detailed evaluation on mean- jpn eng Table 1. Pearson’s correlation coefﬁcients sug- ing changes that are already known. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  It is thus ex- icon, and together with Victoria, the eng pected that cos(w ,w ) − cos(w ,w ) > 0 couplebecame··· eng a jpn a andcos(w ,w )−cos(w ,w ) > 0holdtrue. jpn b eng b Thus, the reason of icon’s anomalous behavior is The last Japanese loanword in Table 2 is used as that the distribution over senses in Wikipedia was both pivot words w and w , but the original En- a b alotdifferentfromtheexpectedone. glish word is not used as w . It is thus expected b thatcos(w ,w )−cos(w ,w ) > 0holdstrue, jpn b eng b 4.4 NearestNeighbors butcos(w ,w )−cos(w ,w ) > 0mightnot eng a jpn a benecessarilytrue. Thedifferencesincosinesim- WeshowinTable3theEnglishnearestneighbors ilarities are shown in Table 2. As expected, al- of the English word w and the Japanese loan- eng most all the differences are positive, which sug- word w in the 300-dimensional space of En- jpn gests that the difference of the word embeddings glish. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: English words that are nearest w and w . </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Table4: Twentywordswiththelowestsimilarities Weshowthe20wordswiththelowestcosinesim- andtwentywordswiththehighestsimilarities. ilaritiesandthe20wordswithhighestcosinesim- ilarities in Table 4. First, let us take a look at the words on the right, which have high similarities. that the meanings of technical terms tend not to Wemanuallyevaluatedthe100wordsthathave change, at least for Japanese (Nishiyama, 1995). the lowest similarities to the corresponding loan- Next, let us take a look at the words on the left, words including the 20 words shown in Table 4. whichhavelowsimilarities. </Extractive Summary>  </Table ID = 4>  </Paper ID = 874> 

<Paper ID = 875>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  mean word length (measured in number of datasetsatourdisposal(cf.thedatasetscoloredin segments)ofwordsforconceptscwithinS. red in Table 1), ABVD, Central Asian, and IELex, 5. correlation coefﬁcient between PMI string fortestingandallotherdatasetsfortraining. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  introducethismeasuretotesttheaccuracyofmul- tilingualcognatedetectionalgorithms. Incontrast 5 ResultsandOutlook to pair scores such as ARI, B-Cubed scores have theadvantageofbeingindependentoftheevalua- The evaluation results are given in Table 3, and tiondataitself. Whilepair-scorestendvarygreatly the differences to the baseline are visualized in dependingondatasetsizeandcognatedensity,B- Figure 5. </Extractive Summary>  </Table ID = 3>  </Paper ID = 875> 

<Paper ID = 876>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Confusion matrix between two different deﬁni- al., 2015; LeCun et al., 2015). </Abstractive Summary>  <Extractive Summary> =  2https://www.gutenberg.org/ stonybrook.edu/˜songfeng/success/ 1218Genre Unsuccessful Successful Total cessfuldespitehavingratings≥3.5andbeingre- DetectiveMystery 60 46 106 viewed by more than 100 reviewers. Table 2 de- Drama 29 70 99 tailsthediscrepanciesbetweenthetwodeﬁnitions. Fiction 30 81 111 HistoricalFiction 16 65 81 LoveStories 20 60 80 4 Methodology Poetry 23 158 181 ScienceFiction 48 39 87 Weinvestigatedawiderangeoftextualfeaturesin ShortStories 123 135 258 an attempt to capture the topic, sentiment, writ- Total 349 654 1,003 ing style, and readability for each book. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Wethenset gories representing grammatical classes, like af- an average rating of 3.5 as the threshold for suc- ﬁxes,andstylisticclasses,likebeg-punctandmid- cess, such that books with average rating < 3.5 punct which reﬂect the position of punctuation are classiﬁed as Unsuccessful. Table 1 shows the marks in the n-gram. The purpose of these fea- datadistributionofourbooks. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Feature Combination Results for Goodreads GutenbergWord2Vec 0.672 0.673 0.140 dataset. </Abstractive Summary>  <Extractive Summary> =  resentations. Someofthebestcombinationresults The results show that the MT approach is better are shown in Table 4. Out of the different possi- than the ST approach. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  characteristicforthetask. However, theresultsin Table 3 show an unimpressive F1-score of 0.610 5.2 ResultsonGoodreadsdataset forsentimentfeatures. Ontheotherhand,thebag of sentic concepts model with average scores for Table 3 shows the results with our new proposed sensitivity, attention, pleasantness, aptitude, and feature sets for the classiﬁcation and regression polaritygaveamoreimpressiveF1-scoreof0.670, tasks. However, theresultsin Table 3 show an unimpressive F1-score of 0.610 5.2 ResultsonGoodreadsdataset forsentimentfeatures. Ontheotherhand,thebag of sentic concepts model with average scores for Table 3 shows the results with our new proposed sensitivity, attention, pleasantness, aptitude, and feature sets for the classiﬁcation and regression polaritygaveamoreimpressiveF1-scoreof0.670, tasks. IntheSTsetting,exceptforthecharactern- much higher than the baseline. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Average accuracy results with new feature and rectedcontentinthebooksused,aswellasthedif- theircombinationsonEMNLP13dataset. </Abstractive Summary>  <Extractive Summary> =  Hence,we .“,.”,said:,youngman,veryyoungman,theyoungman, ngrams boys,.i,father,hisfather,mother,hesaid,shesaid,saidNE, consider our best accuracy so far (71.25%) to be princess,lord,colonel,captain,doctor,tour,mr,miss thestate-of-the-artperformanceonthisdataset. conceive,grieve,zealous,emptiness,bitterness,corpse, hypothesis,irony,theoryofthe,wagon,deepblue, Table 5 shows the results from some of our Senticconcepts scarred,screaming,grudging,vigil,vein, beautifulplace,rural,marriage,friendship,cats,911 best feature sets. The features that worked best avgaptitude,polarity,pleasantness,attentionscores Characterand mr.,mrs.,john,thou,amor,pen,his,and,the,ing, for the Goodreads data also worked best for the typedcharacterngrams n’s,ed,gg’,pt’,d’a,t”,i-t, ,,”i,””,”say,”s,”she EMNLP13 data. </Extractive Summary>  </Table ID = 5>  </Paper ID = 876> 

<Paper ID = 877>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Comparedto cliche´s,beingmarkedasinformalandunoriginal, bag-of-word representations, tree fragments can are expected to be more prevalent in non-literary capture both syntactic and lexical elements; and texts. Table 1 shows the results of these features. thesecombinetorepresentconstructionswithopen Severalotherfeatureswerealsoevaluatedbutwere slots (e.g., to take NP into account), or sentence eithernoteffectiveordidnotachieveappreciable templates(e.g.,“Yes,but...”,hesaid). </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: The number of fragments in folds 2–5 test fold. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Barnes:Senseofanending 2.143 14.1 23.1 0.85 0.32 Voskuil:Buurman 2.117 7.66 58.0 0.89 0.28 Cf. Table 3 for the scores. The syntactic frag- Murakami:1q84 1.870 12.3 20.4 0.84 0.32 mentsperformbest,followedbychar.4-gramsand Table5: Comparisonofbaselinefeaturesfornovels word bigrams. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Itisdifﬁculttoconﬁrmthisdirectly features; cf. the scores in Table 4. Both charac- by inspecting the model, since each prediction is ter4-gramsandsyntacticfragmentsstillprovidea thesumofseveralthousandfeatures,andthecon- relativelylargeimprovementoverthepreviousfea- tributionsofthesefeaturesformalongtail. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  literaryandunderestimatedbythemodel. Forthe Table 5 gives the basic features for the top 4 othernovel(Smeets,Afrekening)theliteraryrating literarynovelswiththelargesterrorandcontrasts is overestimated by the model. Since this top 10 them with 4 literary novels which are well pre- isbasedonthemeanpredictionfrombothmodels, dicted. </Extractive Summary>  </Table ID = 5>  </Paper ID = 877> 

<Paper ID = 878>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thereare tweetsby1389users. alsoseveralcommonnouns(e.g.‘devolution’,‘bag- Note that seven of the hashtags in Table 1 pipes’)andverbs(e.g.‘canvass’,‘invade’)which (#voteyes, #bettertogether, #nothanks, #voteno, are strongly associated with the political or cul- #yes2014,#letsstaytogether,and#yesvote)areoc- turalclimateinScotland. Thesetermsoccurwith casionallyusedincontextsunrelatedtotheScottish greaterrelativefrequencyintheGSdatasetsimply Independence Referendum (e.g. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  grouptouseScottishvariantsingeneralacrossall 5.2 Results oftheirtweets. Foratweettobeincludedintheanalysis,itmust 5.1.1 Teststatistic contain at least one of the variables in Table 3. LetU bethesetofallusersingroupg ∈ {yes,no} Hencenotalluserscontributedatatotheteststatis- g who have used at least one of the variables in tic, as some have not used any of the variables Table 3. Foratweettobeincludedintheanalysis,itmust 5.1.1 Teststatistic contain at least one of the variables in Table 3. LetU bethesetofallusersingroupg ∈ {yes,no} Hencenotalluserscontributedatatotheteststatis- g who have used at least one of the variables in tic, as some have not used any of the variables Table 3. For a given user u ∈ U , let V be in their tweets. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Resultsforthesecondanalysisareshown P pˆ = 1 pˆ . Wethenaverageacrossusers in the right column of Table 5. Once again, the u V v∈V u,v P toobtainthegroupmean,pˆ = 1 pˆ . </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  somevariablesmighthavehigherratesofScottish Resultsfortheﬁrstanalysisareshownintheleft variantsthanothers. Forexampleifuserstendto column of Table 7. The difference is statistically produceScottishvariantsofvariablev atahigher signiﬁcant(p < 0.01),indicatingthatonaverage, 1 rate than for v , and use v more in tweets that individualsarelesslikelytochooseScottishvari- 2 1 don’tcontainreferendum-relatedhashtags,thenit antswhenusingreferendum-relatedhashtagsthan couldappearthatusersaresuppressingtheirScot- intheirothertweets. </Extractive Summary>  </Table ID = 7>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Number of users and tweets included in thetwoanalysesinStudy2 Wepresentedtheﬁrstlarge-scalestudyofdistinc- tivelyScottishlanguageuseonsocialmedia,show- ing that this use includes a mixture of traditional AllControls Controlsw/Hashtags Scotsvocabulary,newerScottishslang,andalter- d¯u −0.0015 −0.0010 nativespellingsthatreﬂectScottishpronunciation. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  </Paper ID = 878> 

<Paper ID = 879>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Phrase-structure parsing performance on formcompetitively. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Dependency parsing performance on beled F ), Table 2; dependency parsing, Table 3, 1 PTB §23 with Stanford Dependencies (De Marn- by converting parse output to Stanford dependen- effeandManning,2008). </Abstractive Summary>  <Extractive Summary> =  AblatedRNNG(nostack) 95.1 93.8 Stack-onlyRNNG 95.8 94.6 Experimental results. We trained each abla- GA-RNNG 95.7 94.5 tion from scratch, and compared these models on three tasks: English phrase-structure parsing (la- Table 3: Dependency parsing performance on beled F ), Table 2; dependency parsing, Table 3, 1 PTB §23 with Stanford Dependencies (De Marn- by converting parse output to Stanford dependen- effeandManning,2008). † indicatessystemsthat cies (De Marneffe et al., 2006) using the tool by useadditionalunparseddata(semisupervised). </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Phrase-structure parsing performance on LSTMLM 113.4 PTB §23. </Abstractive Summary>  <Extractive Summary> =  AblatedRNNG(nostack) 95.1 93.8 Stack-onlyRNNG 95.8 94.6 Experimental results. We trained each abla- GA-RNNG 95.7 94.5 tion from scratch, and compared these models on three tasks: English phrase-structure parsing (la- Table 3: Dependency parsing performance on beled F ), Table 2; dependency parsing, Table 3, 1 PTB §23 with Stanford Dependencies (De Marn- by converting parse output to Stanford dependen- effeandManning,2008). † indicatessystemsthat cies (De Marneffe et al., 2006) using the tool by useadditionalunparseddata(semisupervised). </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  † indicatessystemsthat cies (De Marneffe et al., 2006) using the tool by useadditionalunparseddata(semisupervised). Kong and Smith (2014); and language modeling, Table 4. The last row of each table reports the tory. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  NPs. In most simple noun phrases (representa- 5 HeadednessinPhrases tive samples in rows 6–7 of Table 5), the model pays the most attention to the rightmost noun Wenowexploittheattentionmechanismtoprobe andassignsnear-zeroattentionondeterminersand what the RNNG learns about headedness on the possessivedeterminers,whilealsopayingnontriv- WSJ§23testset(unseenbeforebythemodel). ialattentionweightstotheadjectives. </Extractive Summary>  </Table ID = 5>  </Paper ID = 879> 

<Paper ID = 880>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Inordertoavoid RL(S:), RR(S). Table 1 details the sequence of useless GAPS, we do not allow a SHIFT to follow conﬁgurationsobtainedwhenderivingthistree. aGAP. </Extractive Summary>  </Table ID = 1>  <Table ID = 7>  <Abstractive Summary> =  Table 7: List of all constraints on actions for the FifthConferenceonAppliedNaturalLanguagePro- cessingANLP-97,Washington,DC. </Abstractive Summary>  <Extractive Summary> =  lowed immediately after a SHIFT. Other con- straints on the transition system are straightfor- Itisimmediatethatif(n0,n) ∈ E∗,thennmust ward, we refer the reader to Table 7 of Appendix be reduced before n0 in a derivation. An invari- Aforthecompletelist. Whenaugmentedwithcer- Maier,2015)requiresfurtherinvestigations. tain constraints to make sure that predicted trees are unbinarizable (see Table 7 of Appendix A), 2.4 LengthofDerivations this result also holds for the set of discontinuous Any derivation produced by SR-GAP for a sen- n-ary trees (modulo binarization and unbinariza- tence of length n will contain exactly n SHIFTS tion). andn−1binaryreductions. </Extractive Summary>  </Table ID = 7>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  1263s.c[w/t] s.c[w/t] d.c[w/t] d.c[w/t] 1 0 1 0 s.lc[lw/lt] s.rc[rw/rt] s.lc[lw/lt] s.rc[rw/rt] d.lc[lw/lt] d.rc[rw/rt] d.lc[lw/lt] d.rc[rw/rt] 1 1 0 0 1 1 0 0 s.w/t s.w/t s.w/t s.w/t d.w/t d.w/t d.w/t d.w/t 1 l l 1 r r 0 l l 0 r r 1 l l 1 r r 0 l l 0 r r b0.w/t b1.w/t ... Figure 4: Schematic representation of the top-most elements of S, D and B, using the notations in- troduced in Table 2. Due to discontinuities, it is possible that both the left- and right- index of s are i generatedbythesamechildofs . i BASELINE We ﬁxed it at 30 for every experiment, and shuf- b tw b tw b tw b tw d tc ﬂedthetrainingsetbeforeeachepoch. 0 1 2 3 0 d wc s tc s wc s tc s wc 0 0 0 1 1 s tc s wc s lwlc s rwrc d lwlc 2 2 0 0 0 Features We tested three feature sets described d rwrc s wd w s wd c s cd w s cd c 0 0 0 0 0 0 0 0 0 b0wd0w b0td0w b0wd0c b0td0c b0ws0w in Table 2 and Figure 4. The BASELINE feature b ts w b ws c b ts c b wb w b wb t 0 0 0 0 0 0 0 1 0 1 set is the transposition of Maier (2015)’s baseline b tb w b tb t s cs wd c s cs cd c b ws cd c 0 1 0 1 0 1 0 0 1 0 0 0 0 b0ts0cd0c b0ws0wd0c b0ts0wd0c s0cs1cd0w b0ts0cd0w features to the GAP transition system. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Final test results. </Abstractive Summary>  <Extractive Summary> =  with gold part-of-speech tags, following a com- monpracticefordiscontinuousparsing. 3.3 Results We report results on test sets in Table 3. 3.2 Classiﬁer All the metrics were computed by DISCO-DOP with the parameters included in this package We used an averaged structured perceptron (proper.prm). The beam size controls the andMaierandLichte(2016)useashift-reduceal- tradeoffbetweenspeedandaccuracy.8 gorithmaugmentedwithaswaptransition. Interestingly, the improvement from a larger Table 3 includes recent results from these vari- beamsizeisgreaterondiscontinuousconstituents ous parsers. The most successful approach so far than overall. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Statistics on Tiger train corpus. </Abstractive Summary>  <Extractive Summary> =  transitionsystemproducesshortderivations,com- The fact that derivations are shorter with SR- paredto SR-SWAP, whilebeingabletoderiveany GAP is conﬁrmed empirically. In Table 6, we discontinuoustree. presentseveralmetricscomputedonthetrainsec- Our experiments show that it outperforms the tion of TIGERM15. </Extractive Summary>  </Table ID = 6>  </Paper ID = 880> 

<Paper ID = 881>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Hand-crafted features, based on those dow to capture the context of a mention. </Abstractive Summary>  <Extractive Summary> =  Rockta¨scheletal.(2015)demonstratedthatbyap- Foreachentitymentionm,wecreateabinaryfea- plying an attention mechanism to a textual entail- ture indicator vector f(m) ∈ {0,1}Df and feed ment model, they could attain state-of-the-art re- it to the logistic regression layer. The features sults,aswellasanalysehowtheentailingsentence used are described in Table 1, which are compa- wouldaligntotheentailedsentence. rable to those used by Gillick et al. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Training datasets used and its avail- ability. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Performance on FIGER (GOLD) for videdbythemfortrainingourmodels. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Performance on FIGER (GOLD) for used the freely available 300-dimensional cased modelsusingdifferenttrainingdata. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Performance on OntoNotes for models LSTM+Hand-crafted+Hier 50.42 69.99 64.57 usingdifferenttrainingdata. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 881> 

<Paper ID = 882>  </Paper ID = 882> 

<Paper ID = 883>  <Table ID = 1>  <Abstractive Summary> =  Table 1: The set of conversion targets. </Abstractive Summary>  <Extractive Summary> =  Therearetwomaindifferencesbetweenthefor- ward and backward algorithms. The ﬁrst is the 2 Conversionmethod relative position of a target node (one of Table 1) Let us deﬁne notations ﬁrst. For the i-th word w among the operated nodes; in the forward algo- i in a sentence, p denotes its POS tag, h the head rithm they are the target node, its parent (head), i i index,l thedependencylabel,andleft (right )the and its grandparent, while in the backward algo- i i i list of indexes of left (right) children for w . We choose the modiﬁcationssuchaschangingthemarkarcfrom intermostchildasthenewheadofthetargetword went to that in Figure 1 occur when it detects a (line17). word w (that, in this case), for which the pair i (p ,l )existsinthesetofconversiontargets,which Remarks ThetargetlistinTable1isdeveloped i i is listed in Table 1 and is denoted by T in Algo- for covering main constructions in English and rithm 1. Let w be the head of the detected word Japanese while keeping the backward conversion j w . </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  2Algorithm2Backwardconversion 3.2 Result Input:adependencytreeyandthesetoftargetsT. Attachment scores Table 2 shows the main re- Output:reconvertedyafterapplyingCONV(root(y)). sult and we can see that the improvements are re- 1: procedureCONV(j) 2: foriinleft do markable in the labeled attachment score (LAS): j 3: CONV(i) For MST, the scores increase more than 1.0 point 4: if(p ,l )∈T then 5: CjHAjNGEDEP(leftj,j) in many languages (11 out of 19), and for RBG, 6: foriinright do though the changes are smaller, more than 0.5 j 7: CONV(i) points improvements are still observed in 10 lan- 8: if(p ,l )∈T then j j guages. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: F1-scores (UD and CONV) and the av- is equally easy for both representations, but our erage ratio in the test set (Ratio) of the frequent method needs correct analysis on both functional labels. </Abstractive Summary>  <Extractive Summary> =  Recon- of each arc label. Table 3 summarizes the results verting CONV’soutputintotheUDschemebythe for the frequent labels, and interestingly we can backward algorithm, we can evaluate the outputs see that the improvements are observed for more ofbothmodelsagainstthesameUDtestset. semantically crucial, core relations such as dobj For parsers, we usetwo non-projective parsers: (+0.81), nmod (+2.34), and nsubj (+2.01).6 This second-orderMSTParser(MST)(McDonaldetal., is not surprising as these relations are involved in 2005)3 andRBGParser(RBG)(Leietal., 2014)4 most of our conversion. </Extractive Summary>  </Table ID = 3>  </Paper ID = 883> 

<Paper ID = 884>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Truncated lang2vec syntax vectors for English, Malagasy, and Kazakh, representing binary feature values converted from multi-class features in WALS (Dryer and Haspelmath, 2013) (§3.1), ex- tracted by text-mining prose descriptions in Ethnologue (Lewis et al., 2015) (§3.1), and imputed by k-nearest-neighborsclassiﬁcationfromrelated,nearby,andsimilarlanguages(§4). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Perplexity of monoglot and polyglot language models in Italian and Hindi (Tsvetkov et al., 2016), when the languages are not identiﬁed to the model (baseline), when the languages are represented as one-hot vectors (id), and when languages are represented as lang2vec vectors (id+phonology+inventory). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 884> 

<Paper ID = 885>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Negative log-likelihood after one epoch whichexplainsthenegativelog-likelihoodgapob- oftrainingwithafullvocabulary,forvariousnoise servedinﬁgure2betweenthesetwodistributions. </Abstractive Summary>  <Extractive Summary> =  The third graph shows that its log-partition function is behaving quite better, Table 1: Negative log-likelihood after one epoch whichexplainsthenegativelog-likelihoodgapob- oftrainingwithafullvocabulary,forvariousnoise servedinﬁgure2betweenthesetwodistributions. distributions and a varying number of noise sam- plesk 40 Table 1 shows the negative log-likelihood 35 reached after one epoch of training, for a vary- 30 a=0 ing number of noise samples. For the sake of 25 a=0.25 20 a=0.5 efﬁciency with context-independant noise distri- 15 a=0.75 butions, we used for these experiments the NCE 10 a=1 implementation native to Tensorﬂow, for which 5 the noise samples are re-used for all the positive 0 0 1 2 3 4 5 examples in the training batch. </Extractive Summary>  </Table ID = 1>  </Paper ID = 885> 

<Paper ID = 886>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Examples of generated sentences across tifywordswhichshouldnotbecandidatesforlex- four proposed methods. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Accuracy (%) of the CNN, in four in-domain settings, and two cross-domain settings, with word dropout (“dropout”), or linguistic corruption based on different sources of syntactic and semantic corruption. </Abstractive Summary>  <Extractive Summary> =  WORD2VEC word embeddings (Mikolov et al., 2013). Wordsnotinthepre-trainedvocabularyare 4 ExperimentalResultsandAnalysis initialized randomly using a uniform distribution Table 2 presents the results of training with dif- U([−0.25,0.25)m). ferent sources of linguistic corruption in the in- InjectingNoiseduringTraining Ourproposed domainandcross-domainsettings. </Extractive Summary>  </Table ID = 2>  </Paper ID = 886> 

<Paper ID = 887>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Comparing the Pearson r of adding language model over the residual of the control model vs. </Abstractive Summary>  <Extractive Summary> =  0.36 to 0.44 and for ‘foreclosure‘ from 0.65 to 0.69. Thus, the improvements provided by the 4.2 Results residualized control approach do not appear to be Table 1 reports the effect of building a language duetothefactthattwitterdataarenewerthancon- model over the residual of socioeconomics, de- troldata. mographics, and socioeconomics & demograph- ics by comparing them with the control models. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We see and language features in a single learning step. that, although each displayed n-gram was predic- Results are in Table 2, showing that building lan- tive beyond socioeconomics, many of them sug- guage model over residual performs signiﬁcantly gestamorenuancedeconomiccharacterizationof better than a combined model for both of the out- a community (e.g. ‘technology’, ‘media’, ‘inter- net’, and ‘marketing’), suggesting avenues of fu- 4The control and real estate datasets can be found here: tureexplorationforbetterunderstandingthehous- http://www3.cs.stonybrook.edu/˜mzamani/ datasets/eacl2017/ ingmarket. </Extractive Summary>  </Table ID = 2>  </Paper ID = 887> 

<Paper ID = 888>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  rankersinpractice,Pearsoncorrelationshowshow Each instance is composed of a sentence, a tar- welltherankerscapturesimplicityingeneral. get complex word, and a set of gold candidates Table 2 reveals that, much like our SG ap- ranked by simplicity. We use the same metrics proach,ourNeuralRankerperformswellinisola- featured in (Paetzold and Specia, 2016a), which tion, offering the highest scores among all strate- arethewellknownPrecision, Recall andF1. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  tice that, since these datasets already provide tar- TRank p get words deemed complex by human annotators, Devlin 0.596 0.614 wedonotaddressCWIinourevaluations. Biran 0.513 0.505 The results in Table 1 reveal that our SG ap- Yamamoto 0.604 0.649 proach outperforms all others in Precision and F1 Horn 0.639 0.673 by a considerable margin, and that our SS ap- Glavas 0.632 0.644 proach leads to noticeable increases in Precision Paetzold 0.653 0.677 atalmostnocostinRecall. NNLS/SR 0.658 0.677 BenchLS NNSeval P R F1 P R F1 Table2: SRbenchmarkingresults Devlin 0.133 0.153 0.143 0.092 0.093 0.092 Biran 0.130 0.144 0.136 0.084 0.079 0.081 Yamamoto 0.032 0.087 0.047 0.026 0.061 0.037 7 FullPipelineEvaluation Horn 0.235 0.131 0.168 0.134 0.088 0.106 Glavas 0.142 0.191 0.163 0.105 0.141 0.121 We then evaluate our approach in two settings: Paetzold 0.180 0.252 0.210 0.118 0.161 0.136 with (NNLS) and without (NNLS-C), the Con- NNLS/SG 0.270 0.209 0.236 0.186 0.136 0.157 NNLS/SG+SS 0.337 0.206 0.256 0.231 0.135 0.171 ﬁdence Check (Section 4.3). </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We use applicable in this form of evaluation. Table 4 re- TRank, the ofﬁcial metric of the SemEval 2012 veals that, without the conﬁdence check, our ap- task, which measures the proportion of instances proachyieldsanaverageincreaseof10.5%inAc- for which the candidate with the highest gold- curacy over the former state-of-the-art simpliﬁer. rankwasrankedﬁrst,aswellPearson(p)correla- With the conﬁdence check, it yields the highest tion. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  erything(SE),andthePerformance-OrientedSoft Votingapproach(PV),whichwontheCWItaskof Infuturework,weaimtoinvestigatenewarchi- SemEval2016(PaetzoldandSpecia,2016e). tecturesforourNeuralRankingmodel,aswellas Table 3 shows the count and proportion (in to test our approach in other NLP tasks. An im- brackets) of instances in BenchLS in which each plementation of our Substitution Generation, Se- error was made. </Extractive Summary>  </Table ID = 3>  </Paper ID = 888> 

<Paper ID = 889>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Exact and greedy oracle summarisation ROUGE-nscoresinpercentages,forn ∈ [2]. </Abstractive Summary>  <Extractive Summary> =  wouldleadtosubstantialimprovementsinsystem Farfromperfection. Forextractivesummarisa- performance.Therefore,forourthreedatasets,we tion, the perfect scores (in Table 1) are far from generategoldextractivesummariesusingbothex- 100%aswellasdiverse,accordingtodataset. act and greedy global oracle approaches. tomatically chops sentences that otherwise bring summary lengths over the limit. Table 1 gives the As illustration, consider the frequencies re- resultsforgreedyandexactoraclegoldextractive quiredbythereferencesummariesforaduc2004 summariesacrossourthreedomains. documentsetinFigure1.Thenumberof1-grams to match has increased: this was the original in- Greedy is good. </Extractive Summary>  </Table ID = 1>  </Paper ID = 889> 

<Paper ID = 890>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  the extractive phrase into the abstractive phrase. Table 1 lists each rewrite and how many align- Whenperformingthetask,Turkerswereonlycon- ments used it; we include an alignment when at cerned with one rewrite at a time, and simply least2outof3Turkersagreeditusedtherewrite. hadtoselectwhetherthepresentedalignmentem- We found that generalization was by far the most ployed that rewrite or not. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  marization system using text-to-text generation. Table 2 further displays the interactions be- We also hope that the success of our annotation tween rewrites in the form of a co-occurence ma- method,usingbothtrainedannotatorsandcrowd- trix of the ﬁve rewrites we tested on AMT, plus sourcing, will encourage other researchers to cre- fusion,whichweidentiﬁedautomatically. atesimilarcorpora. </Extractive Summary>  </Table ID = 2>  </Paper ID = 890> 

<Paper ID = 891>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Accuracies on TEST and CONTROL our modiﬁed Stanford Reader (Eq. </Abstractive Summary>  <Extractive Summary> =  We evaluate several other baseline systems in- Wealsotestsimplebaselinesthatchoosepartic- spiredbythoseofPapernoetal.(2016),butwefo- ular non-stopwords from the context, including a cusonversionsthatrestrictthechoiceofanswers randomone,theﬁrstinthecontext,thelastinthe to non-stopwords in the context.3 We found this context,andthemostfrequentinthecontext. 2This list of punctuation symbols is at https: 4 Results //raw.githubusercontent.com/ZeweiChu/ lambada-dataset/master/stopwords/ Table 1 shows our results. We report accuracies shortlist-stopwords.txt 3WeusethestopwordlistfromRichardsonetal.(2013). Table 2 shows the breakdown of these labels We note that TRAIN has similar characteristics across instances, as well as the accuracy on each to the part of CONTROL that contains the answer labeloftheGAReaderwithfeatures. in the context (the ﬁnal column of Table 1). We The GA Reader performs well on instances in- ﬁnd that the ranking of systems according to this volving shallower, more surface-level cues. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Labels derived from manual analysis of text,theaccuracyonTESTismuchhigherthanthe 100 LAMBADA DEV instances. </Abstractive Summary>  <Extractive Summary> =  for our annotator (who was correct on only 7 of the13inthissubset). Table 2 shows the breakdown of these labels We note that TRAIN has similar characteristics across instances, as well as the accuracy on each to the part of CONTROL that contains the answer labeloftheGAReaderwithfeatures. in the context (the ﬁnal column of Table 1). </Extractive Summary>  </Table ID = 2>  </Paper ID = 891> 

<Paper ID = 892>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Results for the English corpora (Sher- the proportion of negation scopes that we fully lock, SFU & BioScope) and for Chinese corpora and exactly match in the test corpus. </Abstractive Summary>  <Extractive Summary> =  alsoreportthenumberofgapsinpredictedscopes. Results are shown in Table 1, including those Data Punctuation Other on SHERLOCK for comparison.6 It is clear that Sherlock 68% 45% the LSTM system improves from joint predic- SFU 92% 23% tion,mainlybypredictingmorecontinuousspans, BioScopeAbstract 88% 51% thoughitperformspoorlyonCNeSp-SCIENTIFIC, BioScopeFull 84% 30% which we believe is due to the small size of the BioScopeClinical 98% 47% corpus. More intriguingly, the baseline results CNeSpProduct 80% 37% clearly demonstrate that punctuation alone iden- CNeSpFinancial 84% 66% tiﬁesscopeinthemajorityofcasesforSFU,Bio- CNeSpScientiﬁc 20% 41% Scope,andCNeSp. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: PCS results on the development set, NLPIR splitintocaseswherepunctuationexactlydelimits 6Unlikeallothercorporawherethescopeifalwayscon- negation scope in the gold annotation, and those tinuousandwherethejointpredictionhelpstoensurenogaps arepresent,inSherlockthegoldscopeisoftendiscontinuous; whereitdoesnot. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Training instances by corpus, showing to associate surrounding punctuation with scope total count and percentages whose scope is pre- boundaries, but when this is not sufﬁcient, it un- dictablebypunctuationboundariesonly. </Abstractive Summary>  <Extractive Summary> =  lock Holmes because of the domain of the data. Comparing the results in Table 4 with the one in 6 DiscussionandRecommendation Table 3, the Sherlock-style annotation produces We have demonstrated that in most corpora used more scopes that are not predictable by punctu- to train negation scope detection systems, scope ation boundaries than those that are. We attribute boundaries frequently correspond to punctuation thistothefactthatbycapturingellipticalconstruc- tokens. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Percentages of scope instances pre- tection easy, we should observe relatively fewer dictable (punct.) and not predictable (no punct.) instancespredictablebypunctuationaloneinthese by punctuation boundaries only on 100 randomly new annotations. </Abstractive Summary>  <Extractive Summary> =  lock Holmes because of the domain of the data. Comparing the results in Table 4 with the one in 6 DiscussionandRecommendation Table 3, the Sherlock-style annotation produces We have demonstrated that in most corpora used more scopes that are not predictable by punctu- to train negation scope detection systems, scope ation boundaries than those that are. We attribute boundaries frequently correspond to punctuation thistothefactthatbycapturingellipticalconstruc- tokens. </Extractive Summary>  </Table ID = 4>  </Paper ID = 892> 

<Paper ID = 893>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Evaluation results (BLEU) of linearized the higher precision in predicting the same predi- PredPattandlinearizedpredicates. </Abstractive Summary>  <Extractive Summary> =  2002) for evaluation. As shown in Table 1, Joint Seq2Seq achieves the best BLEU scores, with an Pipeline JointMoses JointSeq2Seq improvement 1.7 BLEU for linearized PredPatt 5965(15%) 33178(84%) 557(1%) andimprovementof4.3BLEUforlinearizedpred- icatescomparedtoPipeline. Table3: Numberofunrecoverableoutputs. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Table3: Numberofunrecoverableoutputs. PredPatt Predicates Table 4 shows an example output. While some Pipeline 17.19 17.24 arguments (e.g., “The focus of focus” in Table 4) JointMoses 18.34 16.43 arenotcorrect,theoutputofJointSeq2Seqisclos- JointSeq2Seq 18.94 21.55 esttothegoldintermsoftranslation. PredPatt Predicates Table 4 shows an example output. While some Pipeline 17.19 17.24 arguments (e.g., “The focus of focus” in Table 4) JointMoses 18.34 16.43 arenotcorrect,theoutputofJointSeq2Seqisclos- JointSeq2Seq 18.94 21.55 esttothegoldintermsoftranslation. Pipelinehas Table 1: Evaluation results (BLEU) of linearized the higher precision in predicting the same predi- PredPattandlinearizedpredicates. cateheadtokensasthegold,butitsoverallmean- ing is less close. Joint Moses often generates un- Wealsoevaluatepredicatesinthesameveinas recoverable outputs (e.g., the predicate in Table 4 event detection evaluation using the weighted F hastwoheadtokens: “focus”and“related”.) 1 score.5 Therearetotally9,535predicatetokensin thetestdata. Toenableacoarser-grainevaluation, zh sent: 重点 审计 关注 与 老百姓 生活 密切 相 wealsopartitionedthesepredicatesintokclusters 关的专项资金. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  cialfunds)] An important aspect of the auto-generated lin- Joint- [(theauditingfocus(attention)to(life)with earized PredPatt is its recoverability. Table 3 Moses: (ordinarypeople)arecloselyrelatedto(the specialfunds)] shows the number of unrecoverable outputs (in- Joint- [(Thefocusoffocus)focusedon(thespecial cludingempty ormalformedones). Since thelast Seq2Seq: collectionofthespeciﬁcfunds)[(thespecial step in Pipeline is to run PredPatt, Pipeline gen- funds)relatedto(people’slives)]] erates no malformed output. </Extractive Summary>  </Table ID = 3>  </Paper ID = 893> 

<Paper ID = 894>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Table1showstheresultsofExperiment1. Aran- Table 2 shows the results of Experiment 2. On dombaselineresultsin0.20accuracy. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  likelytobefoundinWNtrainingdata. RAEs and related architectures have been used Table 3 shows sample predictions for the in computer vision for a number of applications CCCREandFF-Concatmodels. Itcanbeseenthat includingrecognizingtransformedimages(Memi- CCCREhasmoreantonymsatthehighestranks. </Extractive Summary>  </Table ID = 3>  </Paper ID = 894> 

<Paper ID = 895>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Therefore, we create dlingdiscreteknowledge(FodorandLepore,1999; the data for our study, building two comparable Smolensky,1990),suchasinformationaboutspe- datasets around the binary semantic relations of ciﬁcinstances. Forexample,Beltagyetal.(2016) instantiation and hypernymy (see Table 2). This had to revert from a distributional to a symbolic designenablesustorelateourresultstoworkon knowledge source in an entailment task because hypernymy(seeSection5),andprovidesarichre- thedistributionalcomponentlicensedunwarranted lationalperspectiveontheinstance–conceptdivide: inferences (white man does not entail black man, Inbothcases,wearedealingwiththerelationship even though the phrases are distributionally very 1Availablefromhttp://www.ims.uni-stuttgart. previous datapointsforinstantiation(seeTable1forstatis- section). We use both a global measure of simi- tics and Table 2 for examples).4 It contains 7K larity(averagecosinetoallothermembersofthe positive cases (e.g., Vancouver-city), namely all respectiveset),andalocalmeasure(cosinetothe pairs of instances and their concepts from Word- nearest neighbor). The results, shown in Table 3, Netthatarecoveredbytheword2vecentityvector indicatethatinstancesexhibitsubstantiallyhigher 3https://code.google.com/p/word2vec similaritiesthanconcepts,bothattheglobalandat 4Each instance can belong to multiple concepts (Vancouver-city and Vancouver-port), and different in- 5Thisdoesnotreducetoco-hyponymy,becausethehy- stances/hyponymscanbelongtothesameconcept/hypernym. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Cosine similarities for within-type and Table 1: Dataset statistics. </Abstractive Summary>  <Extractive Summary> =  We use both a global measure of simi- tics and Table 2 for examples).4 It contains 7K larity(averagecosinetoallothermembersofthe positive cases (e.g., Vancouver-city), namely all respectiveset),andalocalmeasure(cosinetothe pairs of instances and their concepts from Word- nearest neighbor). The results, shown in Table 3, Netthatarecoveredbytheword2vecentityvector indicatethatinstancesexhibitsubstantiallyhigher 3https://code.google.com/p/word2vec similaritiesthanconcepts,bothattheglobalandat 4Each instance can belong to multiple concepts (Vancouver-city and Vancouver-port), and different in- 5Thisdoesnotreducetoco-hyponymy,becausethehy- stances/hyponymscanbelongtothesameconcept/hypernym. ponymisrandomlypairedwithanotherhyponym. This (seeRolleretal.(2014)andbelowfordiscussion): result extends to instantiation: The average simi- Concatenatingthetwoinputvectors(Conc),their larityofeachinstancetoitsconceptis0.110(stan- difference(Diff),andconcatenatingthedifference dard deviation: 0.12), very low compared to the vectorwiththesquareddifferencevector(DDSq). ﬁgures in Table 3. The nearest neighbors of in- Instantiation. </Extractive Summary>  </Table ID = 3>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Dataset statistics. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We report F-scores for weconsidermoreuniqueconceptsthaninstances thepositiveclassonthetestsets. (Table 1), and might thus expect the concepts to Table 4 shows the results. Rows correspond showhighersimilarities,atleastatthelocallevel. Hypernymy. Table 4 shows that,inoursetup,hypernymydetectionisconsid- 6Bothdifferencesarestatisticallysigniﬁcantatα=0.001 erablyharderthaninstantiation: Resultsare0.57- accordingtoaKruskal-Wallistest. 81INSTANCE Freq 1Vec Cap Conc Diff DDSq HYPERNYM Freq 1Vec Conc Diff DDSq NOTINST 0.49 0.32 0.67 0.79 0.77 0.78 NOTHYP 0.51 0.29 0.55 0.53 0.57 INVERSE 0.5 0.96 1.00 0.98 0.99 0.99 INVERSE 0.5 0.65 0.75 0.78 0.78 I2I 0.5 0.31 0.80 0.97 0.94 0.94 C2C 0.51 0.29 0.64 0.58 0.62 UNION 0.25 0.01 0.57 0.79 0.74 0.74 UNION 0.25 0.00 0.31 0.26 0.30 Table4: Supervisedmodelingresults(rows: datasets/tasks,columns: featuresets) 0.78fortheindividualhypernymytasks,compared clusion;Rolleretal.(2014)arguedthatthesquared tothe0.79-0.99rangeofinstantiation.7 Thediffer- difference features “identify dimensions that are ence is even more striking for UNION, with 0.31 notindicativeofhypernymy”,thusremovingnoise. </Extractive Summary>  </Table ID = 4>  </Paper ID = 895> 

<Paper ID = 896>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Evaluation of word predictors for hyper- tors,onsmall(referit)andlarge(refcoco)testset nymsinsingularandpluralonREFCOCO tiveinstances,i.e.visualobjectsthathavebeenre- classiﬁers (trained with under sampling of simi- ferredtobytheword. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Second,thisshowsthatundersamplingisnot Image and Word Embeddings Following a good way of dealing with similar objects when (Schlangen et al., 2016), we derive representa- training word predictors whereas in similarity- tions of our visual inputs with a convolutional based training the model does take advantage of neural network, “GoogLeNet” (Szegedy et al., distributionalknowledge,atleastincertaincases. 2015),thatwastrainedondatafromtheImageNet corpus (Deng et al., 2009), and extract the ﬁnal Individual Words As shown in Table 1, the fully-connected layer before the classiﬁcation similarity-based training has a strong positive ef- layer,togiveusa1024dimensionalrepresentation fect for entry-level nouns, whereas the effect on oftheregion. Weadd7featuresthatencodeinfor- theoverallvocabularyisrathersmall. (2014) (trained with 5-word context tiveornegativeeffectascomparedtobinaryclas- window,10negativesamples,400dimensions). siﬁcation(seeTables3and4showingaveragepre- cision scores, number of positive instances of the 4 Results word in the train and test set, and their seman- tic neighbours in the vocabulary, according to the Overall In Table 1, we show the means of the vector space). We also look at hypernyms (Table average precision scores achieved by the individ- 2)whicharenoteasytolearninrealisticreferring ualwordpredictors. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Top 5 improvements for SIM-WAP over Table 4: Top 5 degradations for SIM-WAP over WAP,forrareandmore-frequentwords WAP,shownforrareandfrequentwords Similarly,predictorsforhypernymsandtheirplu- be helpful for learning the referential meaning of ralversionsimprovesubstantially,seeTable2. </Abstractive Summary>  <Extractive Summary> =  Generally,thedifferencesbe- expressiondataasmorespeciﬁcnounsareusually tween the overall means for the different models morecommonornatural(Ordonezetal.,2016). aremostlysmall, butwewillseebelowthatthere aremorepronounceddifferenceswhenlookingat Where similarities help Table 3 shows results particularpartsofthevocabulary. OntheREFERIT for words where SIM-WAP improves most over test set, the simple binary classiﬁers (WAP) have the binary WAP model on REFCOCO. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Top 5 degradations for SIM-WAP over WAP,forrareandmore-frequentwords WAP,shownforrareandfrequentwords Similarly,predictorsforhypernymsandtheirplu- be helpful for learning the referential meaning of ralversionsimprovesubstantially,seeTable2. </Abstractive Summary>  <Extractive Summary> =  Incontrastto tweenthereferentsofthetwowords. abstract objects labels that are annotated consis- Where similarities do not help In Table 4, we tently in image corpora, word use in referring ex- can see results for words where similarity-based pressions is more ﬂexible, and subject to a range training does not help. For words with more of communicative factors, in such a way that e.g. </Extractive Summary>  </Table ID = 4>  </Paper ID = 896> 

<Paper ID = 897>  </Paper ID = 897> 

<Paper ID = 898>  <Table ID = 1>  <Abstractive Summary> =  Table 1: We calculate the Spearman correlation between pairwise human semantic similarity (sim) and relatedness(rel)judgements,andcosinesimilarityoftheassociatedwordembeddings,over7benchmark datasets. </Abstractive Summary>  <Extractive Summary> =  Thus the input ticrelatednessandfoursemanticsimilaritybench- graphiccoordinates. marks (listed in Table 1). In each case we calcu- 100late Spearman’s rank correlation between numer- Target Mostsimilar(GEO30) Mostsimilar(TEXT5) softball,lacrosse, ical human judgements of semantic similarity or #baseball,softball, baseball #baseball,soccer, marlins,nem,dodgers relatedness for a large set of word pairs, and the tourney #naturalhistorymuseum, cosinesimilaritybetweenthesamewordpairsun- natural,dinosaurs, smithsonian’s, history #naturalhistorymuseum, derthegeo-wordembeddingmodels. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Most similar words for target contexts, GP.furniture store, based on cosine similarity of their associated GP.store, GEO30wordandcontextvectors. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Most similar contexts, based on cosine two languages might yield translation pairs that similarity of the associated GEO30 context vec- areclosetooneanotherinvectorspace. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Weevaluatetheresultsusingprecision,recall, Acknowledgments e andF-scoreofpositivetranslationpredictions. We would like to thank our reviewers for their Table 5 gives our results, which we compare to thoughtful suggestions. We are also grateful to amodelthatmakesarandomguessforeachword theNationalPhysicalScienceConsortiumforpar- pair. </Extractive Summary>  </Table ID = 5>  </Paper ID = 898> 

<Paper ID = 899>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  withthevectorV : m The three subsets are selected in sequence start- P ing from the oldest tweets and from the training S Vm = t∈Tm t setsinceautomaticsystemsareusuallytrainedon |T | m past tweets, and need to be robust to future topic Where T are the set of tokens included in the variations. m messagem,S isthevectoroftokent intheSkip- Table 2 reports the results of the ﬁve models t gram model, and |T | is the number of tokens in and the baseline. All neural models outperform m m. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Precision, Recall, F-measure, Ranking probablybecauseoftheintendedmeaningof“hol- and occurrences in the test set of the 20 most fre- iday”and“gorgeous”.). </Abstractive Summary>  <Extractive Summary> =  The ﬁrst one represent a resulted to be the best system in the experiment smallfacecrying,andthesecondoneasmallface presented above. In Table 3 we report Precision, laughing, but the results suggest that they appear Recall, F-measure and Ranking8 of each emoji. insimilartweets. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Precision, Recall and F-Measure of hu- Figure 1: Confusion matrix of the second experi- man evaluation and the character-based B-LSTM ment. </Abstractive Summary>  <Extractive Summary> =  inthetweet). Asfuturework,weplantomakethemodelable We can see in Table 4, where the results of the to predict more than one emoji per tweet, and ex- comparison are presented, that the char-BLSTM plore the position of the emoji in the tweet, as performs better than humans, with a F1 of 0.65 closewordscanbeanimportantcluefortheemoji versus 0.50. The emojis that the char-BLSTM predictiontask. </Extractive Summary>  </Table ID = 4>  </Paper ID = 899> 

<Paper ID = 900>  <Table ID = 2>  <Abstractive Summary> =  Table 2: To evaluate the validity of projecting morpholog- ical tags from Czech onto English text, we compare these Training a system to tag English text with multi- projectedfeaturestofeaturesobtainedfromtheoriginalPTB dimensionalmorphologicaltagsrequiresacorpus tags(listedontheleft). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Uni- Morphtagsareinprincipleupto23-dimensional, 3 ValidatingProjections but tags are not positionally dependent, and not Ifwebelievethatwecanprojectsemanticdistinc- every dimension needs to be speciﬁed. Table 1 tionsoverbitext,wemustensurethattheelements shows the subset of UniMorph subtags used here. linkedbyprojectioninbothsourceandtargetlan- PTBtagshavenoformalinternalsubtagstructure. </Extractive Summary>  </Table ID = 1>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Performance of the PCEDT-trained MaxEnt clas- andpenalizesbothvaluesfromthePTBtagthatareeitherin- siﬁers on §22 of the WSJ portion of the Penn Treebank. </Abstractive Summary>  <Extractive Summary> =  martorepresentequivalentmeanings,weﬁndthat We varied the regularization constant from 0.001 English-onlylexical,contextual,andsyntacticfea- to 100 in multiples of 10, choosing the value in tures derived from off-the-shelf parsing tools en- eachsituationthatmaximizedperformanceonthe codethecomplexsemanticdistinctionspresentin dev set, PCEDT §00. Table 5 contains the re- Czech morphology. In addition to allowing this sults. </Extractive Summary>  </Table ID = 5>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  For case, especially, accu- does not rely on manually-developed syntactic racy is nearly double that of baseline 1. Table 3 heuristics. This signiﬁcantly extends the applica- shows an example English sentence, where case bilityandusabilityoftheproposedgeneraltagging and number have been tagged correctly. </Extractive Summary>  </Table ID = 3>  </Paper ID = 900> 

<Paper ID = 901>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Finally, we experimented with some nonsense 7 Acknowledgments stems, overwriting sentential instances of tran- scribe to generate context-sensitive derivational Wewouldliketothankallreviewersfortheirvalu- forms. Table 3 presents the nonsense stems, the ablecommentsandsuggestions. Thesecondauthor correctformoftranscribeforagivencontext,and was supported by a DAAD Long-Term Research the predicted derivational form of the nonsense Grant and an NDSEG fellowship. was more related to transcription. Table 3 also shows that some of the stems appear to be more DzmitryBahdanau,KyunghyunCho,andYoshuaBen- productivethanothers. gio. </Extractive Summary>  </Table ID = 3>  </Paper ID = 901> 

<Paper ID = 902>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Such a ber of units per layer. Table 1 shows the number languagemodelshouldassignahigherprobability ofparametersineachlayer: to words than to nonwords. At ﬁrst blush, it ap- pearsstraightforwardtoperformthetaskbyﬁxing Layer Parameters a probability threshold and classifying all of the stringswhoseprobabilityfallsabovethisthreshold Characterembeddinglayer VD aswordsandallofthestringsthatfallbelowitas FirstSRNlayer H(D+H +1) nonwords. </Extractive Summary>  </Table ID = 1>  </Paper ID = 902> 

<Paper ID = 903>  </Paper ID = 903> 

<Paper ID = 904>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  De- Eisenstein et al., 2011; Rao et al., 2011; Volkova partment of Justice, 2015): (1) Empathogens et al., 2015), location (Bamman et al., 2014), yet (EMP),coveringthefollowingsubstances: MDA, otheraspectsareminedaswell,amongthememo- MDAI, MDE, MBDB, MDMA; (2) Hallu- tion and sentiment (Volkova et al., 2015), per- cinogens (HAL), which include 5-MeO-DiPT, sonality types (Schwartz et al., 2013; Volkova et ayahuasca, peyote, cacti (trichocerus pachanoi, al., 2015), user political afﬁliation (Cohen and peruvianus, terschekcii, cuzcoensis, bridgesi and Ruths, 2013; Volkova and Durme, 2015), men- calea zachatechichi), mescaline, cannabis, LSD, tal health diagnosis (Coppersmith et al., 2015) belladonna, DMT, ketamine, salvia divinorum, and even lifestyle choices such as coffee prefer- hallucinogen mushrooms (psilocybe cubensis, ence(PennacchiottiandPopescu,2011). Thetask semilanceata, ‘magic mushrooms’), PCP, 2C-B is typically approached from a machine learning and its derivatives (2C-B-FLY, 2C-E, 2C-I, 2C- perspective, with data originating from a variety T-2,2C-T-7); (3) Sedatives (SED), which in- of user generated content, most often microblogs clude alcohol, barbitures, buprenorphine, heroin, (Pennacchiotti and Popescu, 2011; Coppersmith morphine, opium, oxycodone, oxymorphone, hy- et al., 2015; Volkova et al., 2015), article com- drocodone, hydromorphone, methadone, nitrous- 137oxide, DXM (dextromethorphan) and benzodi- an automatic classiﬁer (see Table 3). Not surpris- azepines(alprazolam,clonazepam,diazepam,ﬂu- ingly,thehallucinogenexperiencesaretheeasiest nitrazepam, ﬂurazepam, lorazepam, midazolam, to classify, probably due to the larger amount of phenazepam, temazepam); (4) Stimulants (STI), dataavailableforthisdrug. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Not surpris- azepines(alprazolam,clonazepam,diazepam,ﬂu- ingly,thehallucinogenexperiencesaretheeasiest nitrazepam, ﬂurazepam, lorazepam, midazolam, to classify, probably due to the larger amount of phenazepam, temazepam); (4) Stimulants (STI), dataavailableforthisdrug. includingcocaine,caffeine,khataedulis,nicotine, Table 4 shows a sample of the most informa- tobacco,methamphetamines,amphetamines. tive features for the four categories. </Extractive Summary>  </Table ID = 4>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  to avoid descriptions of the interaction of multi- 5 UnderstandingDrugUsers ple drugs, which are hard to characterize and still mostlyunknown. Table1presentsstatisticsonthe 5.1 PsycholinguisticProcesses dataset, while Table 2 shows excerpts from expe- riencesreportedforeachdrugtype.4 To gain a better understanding of the character- istics of drug users, we analyse the distribution Drugtype Numberreports Totalwords of psycholinguistic word classes according to the Linguistic Inquiry and Word Count (LIWC) lex- EMP 399 378,478 icon – a resource developed by Pennebaker and HAL 2,806 3,494,223 colleagues (Pennebaker and Francis, 1999). The SED 954 692,121 2015versionofLIWCincludes19,000wordsand STI 480 449,596 wordstemsgroupedinto73broadcategoriesrele- Table1: Corpusstatistics. </Extractive Summary>  </Table ID = 2>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Pearson correlations of the LIWC domi- sincetheycouldbecomeaddictive. </Abstractive Summary>  <Extractive Summary> =  6 Conclusions ofpsycholinguisticprocessesacrossthefourdrug types, we also calculate the Pearson correla- Automating language assessment of drug addict tion between the dominance scores for all LIWC experienceshasapotentiallylargeimpactonboth classes. As seen in Table 6, empathogens appear toxicovigilanceandprevention. Drugusersarein- to be the most dissimilar with respect to the other clined to underreport symptoms to avoid negative drugtypes. </Extractive Summary>  </Table ID = 6>  </Paper ID = 904> 

<Paper ID = 905>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Test T Time,h best implementation to operate on non-complete can- LSSVMK 56.16 54.50 – 23.06 didate graphs as it was originally designed for LSSVME 62.82 61.75 – 24.09 making inference on fully-connected graphs only LSPK 57.98 56.81 6 1.82 (HaponchykandMoschitti,2014). LSPE 63.11 61.98 49 1.62 LSPAK 58.69 57.38 3 3.50 3.6 ResultsonFilteredData LSPAE 63.28 62.11 6 1.98 Table 3 reports the results using ﬁltering corre- spondingtothesettingN = 106,d = 20. Wenote Table3: Mainresultsforthesystemsevaluatedon that (i) the training time is reduced by more than CoNLL-2012 English development and test sets, 10times;(ii)LSSVMK isoutperformedbyLSPK usingalltrainingdocumentswithﬁlteredfeatures (2points)andperformsworsethanLSSVME;(iii) (N=106)andedges(d=20). </Extractive Summary>  </Table ID = 3>  </Paper ID = 905> 

<Paper ID = 906>  <Table ID = 1>  <Abstractive Summary> =  Table 1: The distribution of training and test sets in Most-used Split and Cross Validation on level 2 relations in PDTB. </Abstractive Summary>  <Extractive Summary> =  The model also has 2005). access to the traditional features, which are con- In Table 1, we can see that the different re- catenatedtotheneuralrepresentationsoftheargu- lations’ proportions on the training and test set mentsintheoutputlayer. Inordertosimulatewhat are quite different in the most-used split. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 906> 

<Paper ID = 907>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Comparison of input and output embeddings betweenallpairsofwordsevaluatedforthedifferentembed- learned by a word2vec skip-gram model. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Word level perplexity (lower is better) on PTB fromourweighttiedvariant.Spearman’scorrelationρispre- and size (number of parameters) of models that use either sented. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Size(numberofparameters)andBLEUscoreof Small+PR 4.69M 50.8 116.0 111.7 varioustranslationmodels.TWWT–three-wayweighttying. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  <Table ID = 6>  <Abstractive Summary> =  Table 6: WordlevelperplexityonPTBandsizeformod- butionofWTisalsosigniﬁcantinthismodel. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  Table 7: Word level perplexity on the text8, IMDB and Finally,westudytheimpactofweighttyinginat- BBCdatasets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  </Paper ID = 907> 

<Paper ID = 908>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The relative gains and losses from MTL over the 9. Super-sensetagging(SEM)Weusethestan- single-task models (see Table 1) are presented in dardsplitsfortheSemcordataset,predicting Figure 1, showing improvements in 40 out of 90 coarse-grained semantic types of nouns and cases. We see that chunking and high-level se- verbs(super-senses). </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Mean performance across 100 runs of 5- foldCVlogisticregression. </Abstractive Summary>  <Extractive Summary> =  F (gain) 1 Majoritybaseline 0.555 0.615 Allfeatures 0.749 0.669 Best,datafeaturesonly 0.665 0.542 Bestcombination 0.785 0.713 Table 3: Mean performance across 100 runs of 5- foldCVlogisticregression. model for different feature combinations is listed in Table 3. The ﬁrst observation is that there is a strong signal in our meta-learning features. </Extractive Summary>  </Table ID = 3>  </Paper ID = 908> 

<Paper ID = 909>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Success rates (%) for test data with mis- to be giving best success rates. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 909> 

<Paper ID = 910>  </Paper ID = 910> 

<Paper ID = 911>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Some statistics pertaining to the re- train the decoder GRU to minimize the negative sponsesgeneratedbythemodels. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 911> 

<Paper ID = 912>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Audio normalization of three subsets: Table3: Trainingresultsonmetadata WN-withoutnormalization,EN-energynormal- ized,SR-silenceremoved. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  </Paper ID = 912> 

<Paper ID = 913>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 913> 

<Paper ID = 914>  </Paper ID = 914> 

<Paper ID = 915>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The other beneﬁt of this architecture Results. This section brieﬂy summarizes re- isthatitcanweightitsoutputu1accordingtohow sultsofourtrackerondstc2 test(1117dialogs)in many ontology values have been detected during all DSTC2 categories as can be seen in Table 1. turnt. </Extractive Summary>  </Table ID = 1>  </Paper ID = 915> 

<Paper ID = 916>  <Table ID = 1>  <Abstractive Summary> =  Table 1: An example of morphological analy- checkmarksindicatewhichoftheanalysessatisfy sis: multiplecorrectinterpretationsoftheGerman theafﬁx-matchconstraint. </Abstractive Summary>  <Extractive Summary> =  The accu- of correct analyses, which may involve not only racy of the system on German approaches that multiple inﬂections, but also distinct lemmas and of a hand-engineered FST analyzer, while having parts of speech (c.f. Table 1). Hand-built lexi- much higher coverage. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Wesaythatalemma+taganalysisgeneratedfrom 2.1 Alignment a word-form satisﬁes the afﬁx-match constraint if andonlyiftheresultingafﬁx-tagpairoccursinthe Forthetrainingofthestringtransductionmodels, alignment of the training data. Table 2 shows the we need aligned source-target pairs. Monotonic alignments of ﬁve possible analyses to the corre- alignments are inferred with a modiﬁed version spondingword-formschreibet,ofwhichthreesat- of the M2M (many-to-many) aligner of Jiampo- isfy the afﬁx-match constraint. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  These three fea- prediction. Table 3 shows ﬁve possible analyses tures follow the same pattern as the afﬁx-match oftheword-formschreibet,ofwhichonlyonesat- features. isﬁes the mirror constraint. In addition, because the lexicon of Mor- yses that clear the ﬁrst threshold. For example, phisto has a limited coverage, we report micro- the fourth analysis in Table 3, schrieben + 2PKE, averagedresultsinthissection. needs to clear both thresholds, because its lemma Table 5 shows that overall our system per- differsfromthetop-1analysis,schreiben+2PKA. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  For example, phisto has a limited coverage, we report micro- the fourth analysis in Table 3, schrieben + 2PKE, averagedresultsinthissection. needs to clear both thresholds, because its lemma Table 5 shows that overall our system per- differsfromthetop-1analysis,schreiben+2PKA. forms much better on the test sets than the hand- Boththresholdsaretunedonadevelopmentset. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  bol(e.g. canto´ →canto’),withnolossofinfor- Table 6 presents the results. We evaluate the mation. </Extractive Summary>  </Table ID = 6>  </Paper ID = 916> 

<Paper ID = 917>  <Table ID = 1>  <Abstractive Summary> =  Table 1: The origin of the ruled-based analyzers and tag- gers.ILMTstandsforIndianLanguageMachineTranslation Table2: PerlanguagebreakdownofsizeofthePOSportion Project,AMstandsforAmritaUniversity,IIIT-Hstandsfor andthemorphologicalsegmentationportionofDravMorph. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  All Dravidian lan- our corpus: Kannada, Malayalam, Tamil and Tel- guages discussed in this work have semantically ugu. Wefirst performed a full ablation study (see vacuous segments known as inflectional incre- Table 3) on our model described in §3 to validate ments that are inserted during word formation that both the higher-order models and the linguis- betweenthestemandaninflectionalending. Con- tic features have the desired effect. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The results are re- SegmentationinPOSTagging. Next,weshow ported in Table 4. We see clear gains of up to the efficacy of morphological segmentation used 1.69% with the systems that use the segments as asapreprocessingstepforPOStagging(seenasa features. </Extractive Summary>  </Table ID = 4>  </Paper ID = 917> 

<Paper ID = 918>  <Table ID = 1>  <Abstractive Summary> =  Table 1: arguablybetterhaveComputingasitstopdomain. </Abstractive Summary>  <Extractive Summary> =  importance. For notational brevity, we will refer This page contains a set of thirty-two domains to the domain whose similarity score is highest of knowledge.5 Table 1 shows the set of thirty- acrossalldomainsasitstopdomain. Forinstance, two domains. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Number of tagged synsets (new, re- least two of its hypernyms, this domain tag is re- annotatedandremoved)ineachofthedomainan- moved. </Abstractive Summary>  <Extractive Summary> =  Inourapproach, Then, we tag (or retag) all the synsets contain- in addition to WordNet, we provide annotations ingthegivenlabelprovidedthatthemostfrequent for other lexical resources such as Wikipedia or domain for that label gets a number of instances Wiktionary. Table 2 shows some statistics of the higher than 80% of the total of instances contain- synsets tagged in each step of the whole domain ingthesamelabel.11 Asanexample,beforeapply- annotation process. The largest number of an- ingthisheuristicthelabelalbumcontained14192 notated synsets were obtained in the ﬁrst distri- synsets which were pre-tagged with a given do- butional step (1.31M) and the ﬁnal propagation main. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Finally,weincludethere- sults of the distributional approach performed in In this section we describe the evaluation of our theﬁrststepofourmethodology(Section2.1). domain annotations on two different lexical re- Table 3 shows the results of our system and sources: BabelNet and WordNet. To this end, four comparison systems. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  This pre-clustering proved crucial for ﬁnd- inghypernymsinWikidata. ing accurate hypernyms in a distributional vector Table 4 shows the results of TaxoEmbed in space. Weareplanningtofurtheruseourresource the hypernym discovery task on the same ten do- for multi-source domain adaptation on other NLP mains20 evaluatedinEspinosa-Ankeetal.(2016). </Extractive Summary>  </Table ID = 4>  </Paper ID = 918> 

<Paper ID = 919>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  JFLEGwillenablethe ﬁeldtomovebeyondminimalerrorcorrections. the example in Table 1. The correction with only minimaleditsisgrammaticalbutsoundsawkward 2 corpora (unnatural to native speakers). </Extractive Summary>  </Table ID = 1>  </Paper ID = 919> 

<Paper ID = 920>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The JRC-Acquis (Steinberger et al., 2006). The corpus version we collection,ofwhichwetranslateaportion,ispub- usecontainstextsin22ofﬁcialEUlanguages(see licly available for research purposes and already Table 2). The JRC-Acquis corpus text is mostly exists in 22 languages (and others ongoing). from/to Arabic through English. We present the 4 TranslationAnalysis MT results for the European languages with En- glish in Table 2. Our results almost match those Whenanalyzingthedifferencesinthetranslations at (Koehn et al., 2009). Indeed, if we rank the 0.65andr = 0.61,respectively. language families in the order shown in Table 2 Interestingly, the BLEU scores for En→X are form 1 to 7, the correlation between this rank almost double those for Ar→En→X. This is ex- andtheX→EnBLEUandX→En→ArBLEUare pected but it highlights the need for better MT r = 0.90 and r = 0.93, respectively; while the modelsforArabictoEurope’slanguages. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ing the same data splits for training, tuning, and development used by Koehn et al. (2009), who reported their work on developing 462 machine 3.4 Arab-AcquisDataset translation systems based on the 22 languages of In Table 1, we present the ﬁnal dataset sizes for the JRC-Acquis corpus. Their paper included Arab-Acquisandtherespectivedatasetsizesfrom both direct and pivoting-based systems on multi- theJRC-AcquisEnglishandFrenchportionsused ple languages. </Extractive Summary>  </Table ID = 1>  </Paper ID = 920> 

<Paper ID = 921>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Number of gold, silver and bronze doc- accessible through a web interface.6 Stable re- uments per layer and language, as of 13-02-2017. </Abstractive Summary>  <Extractive Summary> =  (manually checked), silver standard (including at OurﬁrstresultsforDutchshowthatourmethod least one BoW) and bronze standard (no BoWs). is promising (Evang and Bos, 2016), but we still Table 1 shows how these classes are distributed needtoassesshowmuchmanualeffortisinvolved acrosslanguagesanddocuments. in other languages, such as German and Italian. </Extractive Summary>  </Table ID = 1>  </Paper ID = 921> 

<Paper ID = 922>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Wiktionary size and quality, and metrics evaluation. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 922> 

<Paper ID = 923>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Results for the test portion of the for their classiﬁcation. </Abstractive Summary>  <Extractive Summary> =  TheresultsfortheNERlevelforCur- riculumLearningarethesameasforMLP,andthe 5 Analysisofresults StanfordNERcannothandlethenumberofclasses intheYAGOlevel. The results on the test portion of our Wikipedia corpus are reported in Table 1. We show over- all accuracy, and the average recall and precision ger classes, for all levels of abstraction but most across classes other than the non-NE class. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Comparison of curriculum learning ECHRjudgments. </Abstractive Summary>  <Extractive Summary> =  Wecanseethatthedropinper- curriculum learning might work better learning formance with respect to results on Wikipedia is ﬁrst from most concrete classes, then from more veryimportant, butontheotherhandthisannota- general classes. In Table 2 we show the perfor- tor has no annotation cost, because examples are mance of curriculum learning in reverse, that is, obtained from Wikipedia, so it can be considered fromthesmallestclassestothemostgeneralones. as a preprocess for human validation / annotation oflegaltext. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Comparison of different strategies for NERC trained on Wikipedia, as they perform in Table 2: Comparison of curriculum learning ECHRjudgments. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 923> 

<Paper ID = 924>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Inter Annotator Agreement: Cohen’s theGutenbergproject2. </Abstractive Summary>  <Extractive Summary> =  In the second phase, inter-annotator agree- mentwascalculatedonasubsetoftheCTD v1(a • DESCRIPTIVE: clauses presenting tangi- total of 5,328 tokens and 526 clauses, with 2,500 ble and intangible characteristics of entities, tokens and about 250 clauses per genre). Table 1 such as objects, persons or locations; e.g., reports the Cohen’s kappa on the number of to- The road winds above, beneath, and beside kens for both text genres. With the exception ruggedcliffsofgreatheight. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  scriptionsoftheclasses,examplesforbothgenres, and priority rules discriminating when more than • OTHER: clauses containing text in foreign one CT class may apply to clauses. Table 2 illus- languages, phatic expressions, references to trates the composition of CTD v1. The two gen- thereader;e.g.,Madameestservie. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  nsubj and nsubjpass), (ii) the noun phrase in direct object position (i.e. dobj and agent), (iii) the noun phrase in any 2623.2 ClassiﬁcationExperiments Results are illustrated in Table 4. The content-based classiﬁcation experiments show We developed our models by dividing the an- that CTs are subject to the functional structure of notated data in training (80%) and test sections thesentenceand,moregenerally,ofthedocument. </Extractive Summary>  </Table ID = 4>  </Paper ID = 924> 

<Paper ID = 925>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Alldatasetsweremadeavailablebythe and learn these representations in the context of authors of their respective papers. Table 1 shows theauthorshipattributiontasksbeingconsidered. descriptivestatisticsforthedatasets. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  5 ResultsandDiscussion We follow Zhang et al. (2015) by setting the vocabularyto70mostcommoncharactersin- Table 2 presents the comparison of the proposed cluding letters, digits, and some punctuation approaches against the previous state-of-the-art marks. methodsonthefourauthorshipattributiondatasets considered. </Extractive Summary>  </Table ID = 2>  </Paper ID = 925> 

<Paper ID = 926>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Performance of the real estate entity Table 3: Performance of the three approaches on recognitionwithhyperparameterλ = 10. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Performance of the three approaches on recognitionwithhyperparameterλ = 10. </Abstractive Summary>  <Extractive Summary> =  2774.4 Pipelineapproach 1455–1465,JejuIsland,Korea,July.Associationfor ComputationalLinguistics. The bottom rows in Table 3 refer to the pipeline approach combining both sequence labeling and Danqi Chen and Christopher Manning. 2014. </Extractive Summary>  </Table ID = 3>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Asexpected, ingsubtask. Weseparatelyshowtheperformance theMTTapproachperformsbetterthantheothers, of our model for each entity type (see Table 1). becausethegloballytrainedmodellearnsdirected Overall, the CRF performs well with a score of spanningtrees. </Extractive Summary>  </Table ID = 1>  </Paper ID = 926> 

<Paper ID = 927>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Statistics of the English side of the test ent issue that has a similar motivation, which is corpora. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  So, the ﬁnal goal is to reduce the number sentence pairs. The statistics of the corpora are ofﬁnalsystems,trainedwithpooledmulti-domain presented in Table 1. All the corpora are pre- datasets,withoutdegradingtheﬁnalperformance. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Performance of the generic and adapted should be very careful in applying them in spe- systemsintermsofBLEUscore. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 927> 

<Paper ID = 928>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ForasetoftimelinesT,wesetD = ∪ D . T t∈T t Table 1 shows parts of journalist-generated 3 CurrentEvaluationMetrics timelines. Approaches for automatic timeline We now describe evaluation metrics for TLS and summarization (TLS) use such edited timelines relatedtasks. Otherwise,ascore and most popular way to apply ROUGE to TLS, of 0 is assigned. For example, in the BP oil spill which we refer to as concat, is to run ROUGE on example in Table 1, the ﬁrst timeline would get documentsobtainedbyconcatenatingtheitemsof a score of 0 when comparing it with the second the timelines (Takamura et al., 2011; Yan et al., timeline,eventhoughbothtimelinesreportonthe 2011a; Nguyen et al., 2014; Wang et al., 2016). existence and later failure of the “top kill” effort, Given a timeline t = (d ,s ),...,(d ,s ), we althoughondifferentdates. The main idea is that daily dr,f(dr) f summariesthatarecloseintimeandthatdescribe dr∈DR the same event or very similar events should be 4.3 Instantiations compared for evaluation. For example, the daily summaries that report on the “top kill” effort in We consider three instantiations of the alignment the example in Table 1 should be compared. To problem presented above. Precision should stay 1, recall should The two daily summaries referring to the “top decrease. kill” effort in Table 1 would be aligned when this • Add: for the ﬁrst date not in the reference metricisemployed. timeline, add a summary consisting of the Many-to-one Date-content Alignment. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We evaluate using variants based on ROUGE- dr,ds r s R s 1 and ROUGE-2, which are the most popu- 5 TestsforMetrics lar ROUGE-N metrics for evaluating TLS. Table 2 shows averaged results over all timelines for An evaluation metric should behave as expected ROUGE-1(ROUGE-2yieldedsimilarresults). when task-speciﬁc operations are performed on Weﬁndthatthefrequentlyusedconcat isnota output(MoosaviandStrube, 2016). </Extractive Summary>  </Table ID = 2>  </Paper ID = 928> 

<Paper ID = 929>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Results of current top systems: ‘*’: previous best score for each evaluation. </Abstractive Summary>  <Extractive Summary> =  followed the instructions of the evaluation setting usedinpreviousstudiesforafaircomparison. Ta- 4.2 Comparisontocurrenttopsystems ble1summarizesthemodelconﬁgurationandthe Table 3 lists the current top system results. Our parameterestimationsettinginourexperiments. </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Results on DUC-2004 and Gigaword data: ROUGE-x(R): recall-based ROUGE-x, ROUGE- x(F):F1-basedROUGE-x,wherex ∈ {1,2,L},respectively. </Abstractive Summary>  <Extractive Summary> =  method EncDec+WFE successfully achieved the current best scores on most evaluations. This re- 4.1 Mainresults: comparisonwithbaseline sult also supports the effectiveness of incorporat- Table 2 shows the results of the baseline EncDec ingourWFEsub-model. and our proposed EncDec+WFE. However,weemphasizethatitalreadyhas data: onlyevaluatedtruefrequency≥ 1. anenoughpowertoimprovetheoverallqualityas shown in Table 2 and Figure 3. We can expect to imum risk estimation, while we trained all the furthergaintheoverallperformancebyimproving models in our experiments with standard (point- theperformanceoftheWFEsub-model. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Confusion matrix of WFE on Gigaword mation. </Abstractive Summary>  <Extractive Summary> =  furtherevidenceoftheeffectivenessofourmethod inquality. Acknowledgement 4.4 PerformanceoftheWFEsub-model We thank three anonymous reviewers for their To evaluate the WFE sub-model alone, Table 4 helpfulcomments. shows the confusion matrix of the frequency esti- 295References In Proceedings of the 2016 Conference on Empiri- calMethodsinNaturalLanguageProcessing,pages Ayana, Shiqi Shen, Zhiyuan Liu, and Maosong Sun. </Extractive Summary>  </Table ID = 4>  </Paper ID = 929> 

<Paper ID = 930>  </Paper ID = 930> 

<Paper ID = 931>  <Table ID = 1>  <Abstractive Summary> =  Table 1: The number of the edges produced in 1- 11: iflb<k-best(chart,k)then 12: lb←k-best(chart,k) bestparsingontestingset. </Abstractive Summary>  <Extractive Summary> =  notworkwellevenforshortsentences. Table 1 shows that the number of edges pro- 5 RelatedWork ducedbytheIVPalgorithmissigniﬁcantlysmaller than standard CKY. Moreover, many of the edges Huang and Chiang (2005) presented an efﬁcient areprunedduringtheiterativeprocess. </Extractive Summary>  </Table ID = 1>  </Paper ID = 931> 

<Paper ID = 932>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  neuralnetworkmodels(NN*). 3.2 Impactofembeddingsandfeed-forward neuralnetworks tendstheNN4modelbyaddingauxiliarydistribu- In the upper half of Table 3.2, we compare the tionsusingonlytheunambiguousPPattachments. LR baseline with two variations of the proposed Althoughtheunambiguousattachmentsareasub- neural network model. These re- feature representations. Our NN1 model outper- sults are shown in the lower half of Table 3.2. forms the logistic regression baseline (LR) by By exploiting topological ﬁelds as extra features, 11.3% in terms of absolute accuracy improve- model NN3 obtains 1.8% absolute improvements ment. </Extractive Summary>  </Table ID = 3>  </Paper ID = 932> 

<Paper ID = 933>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Performance (UAS) of the standard and Wisniewski, 2016),5 with the averaged struc- and improved early-update strategies, depending tured perceptron (Collins, 2002), a beam size on the position in the sentence (French SPMRL of 8 and the ROOT placed at the end. </Abstractive Summary>  <Extractive Summary> =  tributionseenattesttime. Table 3 reports the Kullback-Leibler diver- Discussion Table 2 shows the performance im- gencesinducedbyourreﬁnementswithrespectto balancebetweenvariouspositionsinthesentence the corresponding baselines. It clearly shows that and conﬁrms that our improvements partly allevi- our ‘improved’ learning strategy considers train- ate this phenomenon: the scores on the ﬁrst half ingexamplesthatareclosertotestconﬁgurations. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Effect of our improvements on the formation(coarsetags,nolabel). </Abstractive Summary>  <Extractive Summary> =  tributionseenattesttime. Table 3 reports the Kullback-Leibler diver- Discussion Table 2 shows the performance im- gencesinducedbyourreﬁnementswithrespectto balancebetweenvariouspositionsinthesentence the corresponding baselines. It clearly shows that and conﬁrms that our improvements partly allevi- our ‘improved’ learning strategy considers train- ate this phenomenon: the scores on the ﬁrst half ingexamplesthatareclosertotestconﬁgurations. </Extractive Summary>  </Table ID = 3>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  As a point of compar- EARLY 0.350 0.280 ison, on average over the treebank, our GREEDY MAXV 0.357 0.277 DYNbaselineis2.7UAShigherthanaMaltParser trainedwith ARCEAGER andthesamekindofin- Table 3: Effect of our improvements on the formation(coarsetags,nolabel). Kullback-Leiblerdivergencebetweenthetrainand test feature distributions (French SPMRL dataset, Results Table 1 reports the performance of all withsimilarresultsinotherlanguages). training strategies evaluated by the traditional UASontheprojectivetestsets,ignoringpunctua- tiontokens. </Extractive Summary>  </Table ID = 1>  </Paper ID = 933> 

<Paper ID = 934>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We concatenate an additional 1x2 system deﬁnes a set of conﬁgurations for a sen- vector8 in the input layer of the neural network tence w ,...,w , where each conﬁguration C = (S, 1 n representingthelanguagetagofthecurrentword. B, A) consists of a stack S, a buffer B, and a Table 2 gives the POS tagging accuracies of the set of dependency arcs A. For each sentence, the twomodels. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  1). The results of our ex- 7 Conclusion periments are reported in Table 3. Table 4 shows theimpactofsententialdecodingforchoosingthe In this paper, we have evaluated different strate- best normalized and/or back-transliterated tweets gies for parsing code-mixed data that only lever- ondifferentparsingstrategies(see§4). </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The results of our ex- 7 Conclusion periments are reported in Table 3. Table 4 shows theimpactofsententialdecodingforchoosingthe In this paper, we have evaluated different strate- best normalized and/or back-transliterated tweets gies for parsing code-mixed data that only lever- ondifferentparsingstrategies(see§4). age monolingual annotated data. </Extractive Summary>  </Table ID = 4>  </Paper ID = 934> 

<Paper ID = 935>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Dependency and tagging results. </Abstractive Summary>  <Extractive Summary> =  task. Results are shown in Table 2. Our parser 3Mismatchescouldbecausedbyirreduciblestructuredif- ferencebetweenbothtreebanks(Crabbe´,2015). </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  showninTable4inAppendixA. This last model will give upper-bound accura- Results and Discussion Results on develop- ciesagainstwhichwecancomparethemodelwith ment and test sets are presented in Table 1. First all auxiliary tasks (TOK+CLSTM+M+D), which is thefocusofthepaper. </Extractive Summary>  </Table ID = 1>  </Paper ID = 935> 

<Paper ID = 936>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The Startingfromthefulllistof203conceptsandcor- two datasets were split according to their ‘com- responding images extracted by Cassani (2014), binations’, that is the mixture of targets and dis- wediscardedthoseconceptswhosecorresponding tractors in the scenario. As reported in Table 1, word had low/null frequency in the large corpus we kept 4 different combinations for each C/Q used in (Baroni et al., 2014). To get rid of issues in Train and 2 in Test. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  dinal functions’ beyond the subitizing range. If Results As reported in Table 2, nn-cos is over- so, we might observe that the models keep mak- all the best model for Qs, whereas nn-dot is the ingcognitivelyplausibleerrors,pickingitemsthat best model for Cs. In particular, mean average are close to the target one in the ordered scale. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  constitute the natural next step toward a complete they confound Cs/Qs that are close to each other modelling of the meaning of quantiﬁed expres- in the ordered scale. Table 3 reports the confu- sions. sionmatricesforthebestperformingmodels. </Extractive Summary>  </Table ID = 3>  </Paper ID = 936> 

<Paper ID = 937>  <Table ID = 1>  <Abstractive Summary> =  Table 1: The most common conj attachments in asaguidingprincipleinpreviousworkoncoordi- thePennTreeBankdependencyconversion. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The results Table4: Test-setresultsforconjlabelonly. are in Table 4. The improvement in Rel score is relatively small while there is an improvement of 1.1 points in Rel+Att F1 score, suggesting that The above table does not include the SYM fea- the parser was already effective at identifying the ture since unlike the other features there is no ab- modiﬁers in a conj relation and that our model’s solutewaytodeterminewhetherthefeaturetakes beneﬁt is mainly on attaching the correct parent placeonaspeciﬁcexample. </Extractive Summary>  </Table ID = 4>  </Paper ID = 937> 

<Paper ID = 938>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The scores marked with * in Grundkiewicz (2016). We built our APE Table 1 indicate statistically signiﬁcant improve- Sym model with a single-layer LSTM as encoder and ments(p < 0.01)asmeasuredbybootstrapresam- two-layer LSTM as decoder, using 1024 embed- pling(Koehn,2004)overthecorrespondingscore ding,1024hiddenand512alignmentdimensions. in the previous row. MT outputs by marking the translation they con- sider of better quality in terms of both adequacy 3.1 AutomaticEvaluation and ﬂuency. Each student received a set of 30 Table 1 provides a comparison of the base- sentencesforevaluation,with20sentencesdrawn line WMTB , WMTB , WMTBest, APEB2, randomly and 10 sentences being common to all 1 2 APESym andtheAPERerank system. Automatic students, allowing us to compare the distribution evaluationwascarriedoutintermsofBLEU(Pa- of decisions across all sentences and the 10 com- pineni et al., 2002), METEOR and TER. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Some mon sentences. The outcome of the evaluation is general trends can be observed across all met- presented in Table 2. Assessors preferred the MT rics. </Extractive Summary>  </Table ID = 2>  </Paper ID = 938> 

<Paper ID = 939>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  It is Graham et al., 2014). Table 1 shows numbers of thereforehighlyunlikelytoeverunfairlyexagger- judgments collected in total for each data collec- ate (or under-reward) the performance of any QE tion run on Mechanical Turk, including numbers systeminagivenevaluation. ofassessmentsbeforeandafterqualitycontrolﬁl- With regard to resources required to construct tering,whereonlydatabelongingtoworkerswith eachgoldstandard,asingleDAdatacollectionrun ap-valuebelow0.05wereretained. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Inotherwords,thecostofproducing B, repeat assessments are down-sampled to show the gold standard is 10–20 times greater for post- the increasing correspondence between scores as editingthanDA.5 ever-increasing numbers of repeat assessments are collected for a given document. Correlation 3.2 Re-evaluatingDoc-levelQEWMT-16 between scores collected in the two separate data In order to demonstrate DA’s potential as a gold collection runs reaches r = 0.901 by a minimum standard, Table 2 shows correlations for WMT- of 27 repeat assessments of the sentences of a 16 document-level QE shared task systems when given document, or by an average 107 sentence evaluatedwithDAandtheoriginalgoldstandard. assessmentsperdocument.4 Results show system rankings that diverge from Since DA scores achieve a correlation of r > the original, as the original gold standard exag- 0.9 in our self-replication experiment, we now geratedtheperformanceofthreeparticipatingsys- knowthatDAprovidesreliablehumanevaluation 5Post-editing cost estimates are based on 0.06 and 0.12 4Varianceinnumbersofrepeatassessmentsperdocument EuropersourcedocumentwordconvertedtoUSD$. </Extractive Summary>  </Table ID = 2>  </Paper ID = 939> 

<Paper ID = 940>  </Paper ID = 940> 

<Paper ID = 941>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Morphological variants of the Czech ducetheseinﬂectedwordforms. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Feature templates for the discrimina- in decoding cause a blow-up of the search tiveclassiﬁer: l(lemma), t(morphosyntactictag), space; and useful information is dropped when r(syntacticrole),p(lemmaofdependencyparent). </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Our aim is to quan- tifythepotentialimprovementthatourmethodcan We have studied the important problem of mod- bring. Table 7 shows the statistics: at 50K, the eling all morphological variants of our SMT sys- baseline OOV rate is nearly 17% and our tech- tem’s vocabulary. We showed that we can aug- nique successfully reduces it to less than 10%. </Extractive Summary>  </Table ID = 7>  </Paper ID = 941> 

<Paper ID = 942>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  For names that were unseen in thetrainingdata,weswaptwoadjacentchar- 2 ContrastiveTranslationPairs acters. Wecreateatestsetofcontrastivetranslationpairs Table 1 shows examples for each error type. fromtheEN→DEtestsetsfromtheWMTshared Mostaremotivatedbyfrequenttranslationerrors; translationtask.2 Eachcontrastivetranslationpair forEN→DE,sourceandtargetscriptarethesame, consists of a correct reference translation, and a so technically, we do not perform transliteration. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Case-sensitive BLEU scores (EN-DE) decoder gru_cond two_layer_gru_decoder on WMT newstest. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  Table 2: NMT hyperparameters. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The metric does not provide any insight into the negationinsertion negationdeletion respective strengths and weaknesses of different system nicht kein un- nicht kein un- (categoryandsize→) 1297 10219 11244 2919 538 586 textrepresentations. BPE-to-BPE 94.8 99.1 97.1 93.0 88.7 86.5 Our main result is the assessment via con- BPE-to-char 92.7 98.9 98.7 91.0 85.1 78.8 char-to-char 92.1 98.9 98.8 91.5 86.4 80.5 trastive translation pairs, shown in Table 4. We (Sennrichetal.,2016a) 97.1 99.7 98.0 93.6 92.0 88.4 ﬁnd that despite obtaining similar BLEU scores, Table 5: Accuracy (in percent) of models on dif- the models have learned different structures to a ferent categories of contrastive errors related to different degree. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Accuracy (in percent) of models on dif- the models have learned different structures to a ferent categories of contrastive errors related to different degree. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Examples where char-to-char model prefers contrastive translation (subject-verb agreement errors). </Abstractive Summary>  <Extractive Summary> =  We encour- created to analyse any type of error in a way that agetheuseofLingEval97toassessalternativear- ismodel-agnostic,automaticandreproducible. chitectures,suchashybridword-charactermodels Table 6 shows different examples of the where (Luong and Manning, 2016), or dilated convolu- thecontrastivetranslationisscoredhigherthanthe tional networks (Kalchbrenner et al., 2016). For reference by the char-to-char model, and the cor- thetestedsystems,themostchallengingerrortype responding1-besttranslation. </Extractive Summary>  </Table ID = 6>  </Paper ID = 942> 

<Paper ID = 943>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Translationqualityisevalu- atedintermsoftokenizedBLEUscores(Papineni The results of English-German and Chinese- etal.,2002)withmulti-bleu.perl. English are shown in Table 2 and 3 respectively. 385src Shewasspottedthreedayslaterbyadogwalkertrappedinthequarry ref DreiTagespa¨terwurdesievoneinemSpazierga¨ngerimSteinbruchinihrermisslichenLageentdeckt baseline SiewurdedreiTagespa¨tervoneinemHundentdeckt. </Extractive Summary>  </Table ID = 2>  </Paper ID = 943> 

<Paper ID = 944>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Evaluation results on the RW dataset (and on RG-65 as baseline). </Abstractive Summary>  <Extractive Summary> =  Also,we tion4)thathavereportedperformanceontheRW set θ in formula 1 to one in order to assign equal dataset. Results are shown in Table 2.2 To have weights to the initial embedding d(w0), when- a fair comparison, in this setting we used a 500d r ever available, and to the one induced based on setofembeddingstrainedbytheSkipgrammodel theknowledgeextractedfromthelexicalresource. (Mikolov et al., 2013) on the Wikipedia corpus We did not perform any tuning on these parame- (Shaoul and Westbury, 2010), similarly to Sori- ters. </Extractive Summary>  </Table ID = 2>  </Paper ID = 944> 

<Paper ID = 945>  <Table ID = 1>  <Abstractive Summary> =  Table 1: R2 of full model and major interactions goricalvariablesirrespectiveofthedummycoding forT[OEFL],G[EK],W[S353]andA[P]datasets schemeused. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 945> 

<Paper ID = 946>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  samples,fromeach for each model in Section 4.1. In Table 3 we can ofthetrainingsets. Eachsampleisnormalizedand see again the superior performance of the Baroni balancedto400instances.1 dataset. </Extractive Summary>  </Table ID = 3>  </Paper ID = 946> 

<Paper ID = 947>  </Paper ID = 947> 

<Paper ID = 948>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Average F scores and conﬁdence intervals of cross-language similarity detection methods 1 appliedonEN→FRsub-corpora–8foldsvalidation. </Abstractive Summary>  <Extractive Summary> =  5 ResultsandDiscussion Use of word embeddings. We can see in Table 1 that theuse of distributed representation of words instead of lexical resources improves CL-CTS (CL-CTS-WE obtains overall performance gain of (a)Distributionhistogram(ﬁngerprint)ofCL-C3G +3.83%onchunksand+3.19%onsentences). De- spitethisimprovement,CL-CTS-WEremainsless efﬁcientthanCL-C3G. </Extractive Summary>  </Table ID = 1>  </Paper ID = 948> 

<Paper ID = 949>  </Paper ID = 949> 

<Paper ID = 950>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Test accuracy [%] on sentiment datasets. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Training time for a single epoch on sentiment analysis datasets compared to char-CNN and VDCNN. </Abstractive Summary>  <Extractive Summary> =  We will release a ing 20 threads. Table 2 shows that methods us- script that recreates this dataset so that our num- 429Input Prediction Tags taiyoucon 2011 digitals: individuals digital pho- #cosplay #24mm #anime #animeconvention tosfromtheanimeconventiontaiyoucon2011in #arizona #canon #con #convention mesa,arizona. ifyouknowthemodeland/orthe #cos#cosplay#costume#mesa#play character,pleasecomment. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Examples from the validation set of YFCC100M dataset obtained with fastText with 200 hiddenunitsandbigrams. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Prec@1 on the test set for tag predic- method for text classiﬁcation. </Abstractive Summary>  <Extractive Summary> =  AlexisConneau,DuyuTangandZichaoZhangfor providing us with information about their meth- Results and training time. Table 5 presents ods. a comparison of fastText and the baselines. </Extractive Summary>  </Table ID = 5>  </Paper ID = 950> 

<Paper ID = 951>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Details of the New York Times (NYT) Table 2: Per-token log likelihood measures on and State of the Union (SOTU) corpora used for held-out data for New York Times models with topic modeling. </Abstractive Summary>  <Extractive Summary> =  We treat the use co-occurrence statistics for frequent types in full article set as a reference corpus for word co- the topic, such as topic coherence (Mimno et al., occurrence. Thedetailsofthesizeofeachcorpus 2011) and normalized pointwise mutual informa- are in Table 1. We use a standard stoplist from tion(NPMI)(AletrasandStevenson,2013;Lauet MALLETforourexperiments(McCallum,2002). </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Per-token log likelihood measures on and State of the Union (SOTU) corpora used for held-out data for New York Times models with topic modeling. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Classiﬁcation results using top terms of NYT-U 0.0554 0.1233 0.1208 50-topicmodelsonNYTandSOTUdata. </Abstractive Summary>  <Extractive Summary> =  Table 4: The average NPMI scores for New York We use 10-fold cross validation to compute accu- Times and State of the Union data. Surprisingly, racy, which we report in Table 5. Unsurprisingly, with 10 topics, post-removal of stopwords often removing stopwords at some stage improves the producesbettercoherence. </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  distinctivenessoftopicsbasedonhigh-probability terms. 4.3 Coherence Examples of topics in Table 3 provide some We report the average NPMI scores for the New depth to understanding these results. Topic 1 is York Times and State of the Union data in Table nearly identical across the two treatments, while 4. </Extractive Summary>  </Table ID = 3>  </Paper ID = 951> 

<Paper ID = 952>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The details i=2 j=1 1j ij penalty for the words which are not assigned to of the optimization formulation are presented the principal eigenvector. The penalty is equal in Table 1. We consider the following example to their similarity with the principal eigenvector. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Higher the correlation with human We proposed a novel approach TBuckets to mea- scores, better is the performance of the technique sure quality of Latent Dirichlet Allocation (LDA) atmeasuringcoherence. based topics, based on grouping of topic words Table 2 shows the Pearson’s r values obtained into buckets. TBuckets uses singular value de- from the state-of-the-art (Ro¨der et al. performance across datasets. Moreover, as com- org/palmetto-webapp/ paredtothestate-of-the-arttechniqueswhichneed As observed in Table 2, TBuckets outperforms totunemultipleparameters,TBucketsrequiresno (Ro¨deretal.,2015)on3outof4and(Nikolenko, parametertuning. 2016)onall4datasets. </Extractive Summary>  </Table ID = 2>  </Paper ID = 952> 

<Paper ID = 953>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  It was only possible to ConcatenateCNNandRNN 50.6 run some of the baseline algorithms on the Yelp GRAnot-aligned 48.8 dataset due to availability of source code and pa- GRA 51.0 rameterconﬁgurations. It can be seen from Table 1 that GRA outper- Table2: AccuracyofGRAandotherhybrids. forms the baselines on the ﬁne-grained datasets (SST5/Yelp),andisalsocomparablewiththebi- It can be seen from Table 2 that even very narycase(SST2). </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  It can be seen from Table 1 that GRA outper- Table2: AccuracyofGRAandotherhybrids. forms the baselines on the ﬁne-grained datasets (SST5/Yelp),andisalsocomparablewiththebi- It can be seen from Table 2 that even very narycase(SST2). simple ensemble methods can yield good results Next, we further investigated the effect of soft- when compared to standalone models. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Accuracy of GRA against structure de- casesbetweenGRAandGRA-without-alignment. </Abstractive Summary>  <Extractive Summary> =  timent observed by GRA is caused by the model Acknowledgments capturingphraselevelchanges. 4.2 Structure-dependentModels This material is based upon work supported in In Table 3, we compare GRA with state-of-the- whole or in part with funding from LAS. Any art structure-dependent models. </Extractive Summary>  </Table ID = 3>  </Paper ID = 953> 

<Paper ID = 954>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (8) Twitter: contains 0.98 a set of tweets, each labeled with its sentiment 0.96 (8Sdaantdaesrest,s.2011). Table 1 shows statistics of the accuracy 0.94 4.3 TextCategorization 0.92 To perform text categorization, we employed an 0.90 SVMclassiﬁer(Boseretal.,1992). Sincethepro- 0.0 0.2 0.4 0.6 0.8 1.0 α posedsimilarityfunction(Equation6)isakernel, we directly built the kernel matrices2. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  It is also important proaches, we employed two well-known evalu- to note that the approaches that use word embed- ation metrics: accuracy and macro-average F1- dings (Centroid, WMD, CNN, Gaussian) achieve score. Table 2 shows the performance of the con- an immense increase in performance on the Snip- sideredapproachesontheeighttextcategorization petsdataset. Onepossibleexplanationisthatthese datasets. </Extractive Summary>  </Table ID = 2>  </Paper ID = 954> 

<Paper ID = 955>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thesub-vectorsin 4.5 ExperimentalResults b were whitened ﬁrst (mean-zeroed and scaling m Thegenreclassiﬁcationaccuracyandtheweighted to the unit variancefor each axis) before concate- F-scoreresultsusingdifferentfeaturevectorsover nation. the three corpora are summarized in Table 3 and Table4. Table1: Valuesofvarioushyperparametersbeing tunedforthederivationofthebestPV-DM. </Extractive Summary>  </Table ID = 3>  </Paper ID = 955> 

<Paper ID = 956>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Under the ﬁrst setting and on CoreRank 0.474* 0.259* PageRank 0.469 0.250 the AMI corpus, we ﬁnally investigated how Degree 0.470* 0.245 the density term (h) of our submodular func- Frequency 0.460 0.231 tion f was inﬂuencing the performance of the RAKE 0.384 0.196 Random 0.365 0.190 graph-based systems. As shown in Table 3, h proved beneﬁcial, even though the improvements Table1:Resultsforscenario1(real-time,cosinesimilarity). *indicatesstatisticalsigniﬁcance6atp<0.05againstthe were marginal. </Extractive Summary>  </Table ID = 3>  </Paper ID = 956> 

<Paper ID = 957>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Formallythisattention In our best performing model, we augment the canbedescribedbythefollowingequations: inputs to the encoder by adding entity type fea- tures. Classes present in the knowledge base of ut = vT tanh(W h +W h˜ ) (1) i 1 i 2 t the dataset, namely the 8 distinct entity types re- at = Softmax(ut) (2) i i ferred to in Table 1, are encoded as one-hot vec- Xm tors. Whenever a token of a certain entity type h˜0 = ath (3) t i i is seen during encoding, we append the appro- i=1 priate one-hot vector to the token’s word embed- o = U[h˜ ,h˜0] (4) t t t ding before it is fed into the recurrent cell. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Evaluation on DSTC2 test (top) and dev ationproceduresbasedonbeamsearchdecoding. </Abstractive Summary>  <Extractive Summary> =  3.4 Results includingbeatingitsper-dialogueaccuracy. Italso In Table 2, we present the results of our mod- achievesthehighestentityF . els compared to the reported performance of the 1 best performing model of (Bordes and Weston, 4 DiscussionandConclusion 2016), which is a variant of an end-to-end mem- ory network (Sukhbaatar et al., 2015). </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Sample dialogue generated. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 957> 

<Paper ID = 958>  <Table ID = 1>  <Abstractive Summary> =  Table 1: UTD matches within utterances, within domlychosenforthetrainingset(independentof calls and within the corpus. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Source text (left) paired with translations by humans (gold), oracle, and UTD-based system. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The utterance-level scores are then used to compute 5% corpus-levelPrecision@K andRecall@K. 1 2 3 4 5 6 7 8 9 10 Table 4 and Fig. 2 show that even the oracle K has mediocre precision and recall, indicating the Figure2: PrecisionandRecall@K forthecalland difﬁculties of training an MT system using only utteranceleveltestsets. ter,sincetrainingandtestsharemorevocabulary. Table 4 and Fig. 2 also show a large gap be- 6 Conclusionsandfuturework tween the oracle and our system. </Extractive Summary>  </Table ID = 4>  </Paper ID = 958> 

<Paper ID = 959>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  a strategy which uses a legal trade re-ranking a chat interface to engage in conversations with mechanism trained on the human negotiation each other, involving the negotiation of trades in corpus described in (Afantenos et al., 2012) particular. Table 1 shows an annotated chat be- using supervised learning (Random Forest) tweenplayersW,T,andG;inthisdialogue,atrade (Cuaya´huitl et al., 2015a; Cuaya´huitl et al., is agreed between W and G, where W gives G a 2015b;Keizeretal.,2015);and clayinexchangeforanore. Fortrainingthedata- driven negotiation strategies, 32 annotated games 4. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The action space includes Strategy)andtheirtradenegotiationstrategy(Ne- 70actionsforofferingtradingnegotiations(in- got. Strategy), see Table 2. The game playing cluding up to two giveable resources and only strategy involves all non-linguistic moves in the one receivable resource) and 3 actions (ac- game: e.g.,whenandwheretobuildasettlement, cept,rejectandcounteroffer)forreplyingtoof- 2jsettlers2.sourceforge.net fers from opponents. cult to control. Each participant played one game The evaluation results are presented in Table 2 against three bots of the same type. The bot was and Fig. Figure2: Boxplotsrepresentingthevictorypoints (VPs) scored by humans against each bot (as trained (18.2% vs. 44.4%, using the same game shown on Table 2). Humans scored lower against playing strategy). </Extractive Summary>  </Table ID = 2>  </Paper ID = 959> 

<Paper ID = 960>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thisentropy-basedmeasure ingtheoutputfromthepreviousround. is deﬁned as harmonic mean between homogene- In addition, Table 1 provides results for the ity (HO – the precision analogue) and complete- three extreme cases: random label, majority la- ness (CM – the recall analogue). Rosenberg and bel, and distinct label for each utterance (a sin- Hirschberg(2007)presentsdeﬁnitionandcompar- gleutterancepercluster). </Extractive Summary>  </Table ID = 1>  </Paper ID = 960> 

<Paper ID = 961>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  For square (1),thiswouldresultinallwordsgettingassigned Notice that a word might be aligned with more the labels row4,col0,small,blue,square. As than one action, which means that the learning Table 2 shows, this model achieves lower results. process has to deal with potentially noisy infor- Thisindicatesthattemporalalignmentinthetrain- mation. </Extractive Summary>  </Table ID = 2>  </Paper ID = 961> 

<Paper ID = 962>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In the third step, we assign tenses to the verb phrasesmarkedinthetranslations(seestep2). For thetenselabelling,weoptedforthecategoriesdis- played in Table 1. The tenses are automatically an example outcome). Table 4 that result from mapping the language- Hovering over a point on the map directly shows speciﬁc tense labelling in step 3 to more generic youtheﬁve-tuplethepointisbasedon,andclick- tenses (e.g. simple past to PAST, see Table 1). As ingonapointwillyieldanewpageinwhichyou is commonly reported in literature (see de Swart can inspect the underlying data (see Figure 3 for (2007) and references therein), the French passe´ anexample).5 compose´ takes responsibility for a wide range of Compared to Wa¨lchli and Cysouw (2012), our PERFECT uses. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  can also choose to show other dimensions of the We ﬁrst observe the descriptive statistics in MDS algorithm, which facilitates interpretation. Table 4 that result from mapping the language- Hovering over a point on the map directly shows speciﬁc tense labelling in step 3 to more generic youtheﬁve-tuplethepointisbasedon,andclick- tenses (e.g. simple past to PAST, see Table 1). </Extractive Summary>  </Table ID = 4>  </Paper ID = 962> 

<Paper ID = 963>  </Paper ID = 963> 

<Paper ID = 964>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Spearman rank correlation on word similarity task. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Accuracy on analogical reasoning task. </Abstractive Summary>  <Extractive Summary> =  d analogy dataset (Mikolov et al., 2013b), which All embeddings are 300 dimensions. The corpus contains19,544questions, abouthalfoftheques- is the same as Table 2, but the size is 1/100. The tions are syntactic analogies and another half of a bestresultforeachdatasetishighlightedinbold. moresemanticnature,andMSRanalogydataset (Mikolovetal.,2013b),whichcontains8,000syn- 4 Conclusion tacticanalogyquestions. Table 2 shows the similar result as Word Simi- In this paper, we argue that while applying prior larityanddemonstratesourproposedmethodsare knowledgeintocontext-basedembeddings,statis- stableandcanbeappliedtodifferenttasks. tics of word occurrences should be considered, which based on the assumption that a embedding 3.4 CorpusSize with more contextual information is supposed to We also apply our models on the corpus with a have higher quality, and thus should be treated in smallersize. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Spearman rank correlation on word similarity task. </Abstractive Summary>  <Extractive Summary> =  Theresults soning. The implementation is based on RCM are shown in Table 3 and Table 4, which shows package and we have released the code for aca- ourproposedmodelsalsoimprovetheCBOWand demic use. 1 In the future, under this framework, outperform the baseline. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Accuracy on analogical reasoning task. </Abstractive Summary>  <Extractive Summary> =  Theresults soning. The implementation is based on RCM are shown in Table 3 and Table 4, which shows package and we have released the code for aca- ourproposedmodelsalsoimprovetheCBOWand demic use. 1 In the future, under this framework, outperform the baseline. </Extractive Summary>  </Table ID = 4>  </Paper ID = 964> 

<Paper ID = 965>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ingtherespectiveGerman-to-EnglishandItalian- to-Englishtranslationmatrices. Thequalityofthe 4 Evaluation obtained translations, measured in terms of P@1 and P@5 (i.e., percentage of cases in which the In line with previous work (Chen et al., 2013; translationpairwasretrievedasthemostsimilaror Socheretal.,2013),weevaluateourapproachon amongtheﬁvemostsimilarwordsfromtheother the link prediction task, namely the binary clas- language), is shown Table 2. The performance siﬁcation task of predicting the correctness of a levelsweobtainarecomparabletotranslationper- KBtriple(e ,r,e ),givenentitiese ande anda 1 2 1 2 formancesreportedintheoriginalwork(Mikolov semanticrelationr. </Extractive Summary>  </Table ID = 2>  </Paper ID = 965> 

<Paper ID = 966>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Annotated adjective–noun pairs from concepts annotated with properties and produc- TSV-TEST tion frequencies (i.e. </Abstractive Summary>  <Extractive Summary> =  TSV-TESTcontains100 cross-modal map between linguistic representa- literal and 100 metaphorical pairs, annotated by tions and property-based representations, we can 5 annotators with an inter-annotator agreement of learn to predict properties for new (unseen) con- κ = 0.76. Table 3 shows a portion of the test cepts(Figure1). set. </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Wecanviewitasabagof2526prop- erties, withthestandardco-occurrencecountsbe- 4 Experiments ingreplacedbytheproductionfrequencies. Table 2 shows a subspace of such a property-norm se- 4.1 Experimentaldata manticspace. We evaluate our method using the dataset Cross-modal maps Even though MCRAE only of adjective–noun pairs manually annotated for contains annotations for 541 concepts, cross- metaphoricity, created by Tsvetkov et al. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: System performance on Tsvetkov et al. </Abstractive Summary>  <Extractive Summary> =  We evaluated the performance of our clas- domains in the literal and metaphorical phrases. siﬁer on TSV-TEST in terms of precision, recall Our hypothesis is that attribute-based methods and F-score; the results are presented in Table 4. Both types of attribute-based vectors outperform outperformtheEMBEDandSVDbaselinesbecause the attribute-based dimensions are cognitively- their dense counterparts, which lends support to motivated and represent cognitively salient prop- ourhypothesisthatpropertynormsofferasuitable erties for concept distinctiveness. tors (ATTR-EMBED, ATTR-SVD) in the metaphor 5 Qualitativeanalysisanddiscussion identiﬁcation task is that they are interpretable, i.e. every dimension in the space has a ﬁxed The results in Table 4 show that the systems are interpretation (is_round, a_bird etc.) as op- able to reliably distinguish between metaphori- posed to the abstract dimensions of SVD and cal and literal expressions both when using dense EMBED. We can thus identify the most salient andattribute-basedsemanticrepresentations. </Extractive Summary>  </Table ID = 4>  </Paper ID = 966> 

<Paper ID = 967>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  However, eval- representations leverage the context, e.g., the fact uatingmeasuresofphrase-levelsimilaritydirectly that the target word student has occurred in the againsthumanjudgmentsofsimilarityignoresthe context folded, in different ways. Table 1 illus- problemthatitisnotalwayspossibletodetermine tratesthecontextualfeatureswhichmightbegen- meaning in a compositional manner. If we com- erated for student given different deﬁnitions of posethemeaningrepresentationsforred andher- context. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Boldface is usedtoindicatethebestperformingconﬁguration We have shown that combining traditional com- ofparametersforaparticularmodel. positional methods with state-of-the-art low- Table 2 summarises results for different pa- dimensionalwordrepresentationscanimprovere- rameter settings for the neural word embeddings. sults over the state-of-the-art. sults over the state-of-the-art. Further improve- Looking at the results in Table 2, we see that mentscanbeachievedusinganintegratedcompo- the cbow model signiﬁcantly outperforms the sitionaldistributionalapproachbasedonAPTrep- skip-grammodel. Usingthecbowmodelwith resentations. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Since compositionality de- occurrences,thisperformancegaincanbeseenas tection also provides a way of evaluating compo- theresultofimplicitparameteroptimisation. sitionalmethodswithoutconfoundingjudgements Table 3 summarises results for different com- of phrase similarity with judgements of composi- position operations and parameter settings using tionality,itappearsthattheAPTapproachtocom- position is reasonably promising. Further work 4Across all models, optimal values were in the range [0.3,0.5]. </Extractive Summary>  </Table ID = 3>  </Paper ID = 967> 

<Paper ID = 968>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  moving any of the target verb types. The maxi- The results are presented in Table 1. For this mum number of senses per verb type was set to generalsemantictask,themulti-senseembeddings 20. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Theresultsarepresented c-Means baseline with single-sense embeddings. in Table 2. CHINWHISP performs signiﬁcantly (using every possible threshold within a range of better than the baseline, while most other models [0.01,0.99]todeterminethememberships,andre- are performing equally to or even inferior to the porting the one providing the highest score). </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  evaluationmeasure wereliedon B-Cubed (Bagga Model Prediction andBaldwin,1998)andreportf-scorebetweenthe NP-MSSGR .20 softextensionofprecisionandrecall. ChinRestP .30 Table 3 presents the results. Overall, CHIN- ChinWhisp .32 HDP .19 RESTP works best, and CHINWHISP and the x-Means(NN) .19 BIRCH variants work similarly well. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The plot shows that andf-scorevaluesregardingthenon-literalclass. overall, the identiﬁed senses in the models are Table 4 shows the results. All multi-sense quite similar to each other. </Extractive Summary>  </Table ID = 4>  </Paper ID = 968> 

<Paper ID = 969>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Performance of our approach for English without clustering (k = 1) and with the optimal numberofclusterontheEVALutiondatasets(k = 30)andonthecombineddatasets(k = 25). </Abstractive Summary>  <Extractive Summary> =  the non-regularized baseline. Table 2 presents detailed results for both English datasets. Sim- Acknowledgments ilarly to the ﬁrst experiment, our approach con- sistently improves results robustly across various WeacknowledgethesupportoftheDeutscheFor- conﬁgurations. </Extractive Summary>  </Table ID = 2>  </Paper ID = 969> 

<Paper ID = 970>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Distribution across the 9 stance classes Table 4: Distribution across the 9 stance classes fortheHillaryClinton-BernieSanderstargetpair fortheTedCruz-DonaldTrumptargetpair Opinion Clinton that the words outside the context window do not Toward favor against neither have an inﬂuence on the target. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Distribution across the 9 stance classes fortheHillaryClinton-BernieSanderstargetpair fortheTedCruz-DonaldTrumptargetpair Opinion Clinton that the words outside the context window do not Toward favor against neither have an inﬂuence on the target. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Distribution across the 9 stance classes fortheDonaldTrump-HillaryClintontargetpair Tocapturedependenciesbetweenstancelabelsof related targets, one possibility is to use the pre- ers have the same position towards both entities, dicted class toward one target as an extra fea- 50% of tweeters have opposing positions towards ture in other targets’ models. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Macro-averaged F-scores of different end neural machine translation (Sutskever et al., modelsontheMulti-TargetStancedataset 2014; Cho et al., 2014b), image-to-text conver- decoder deep neural model on our dataset. </Abstractive Summary>  <Extractive Summary> =  when searching the best label sequence, based on automatically-learned input features. The atten- 4.1 ResultsandDiscussion tion mechanism has the potential of dynamically Table 5 presents the macro-averaged F-scores focusing on different words of the input text to of different models on the Multi-Target Stance generate stance labels for each target of interest. dataset. </Extractive Summary>  </Table ID = 5>  </Paper ID = 970> 

<Paper ID = 971>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Single-domain polarity classiﬁcation ac- 3 as negative. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Multi-source cross-domain polarity clas- der to analyse this fact, we perform an additional siﬁcationaccuracy(in%). </Abstractive Summary>  <Extractive Summary> =  sourcesetting. 4.4 Cross-domainPolarityClassiﬁcation 5 Conclusions Following recent works in cross-domain polar- ity classiﬁcation (Bollegala et al., 2013; Franco- In this paper we studied the single and the cross- Salvador et al., 2015), in Table 2 we compare domainpolarityclassiﬁcationtasksfromthestring withthestateoftheartusingamulti-sourcecross- kernelsperspective. Weanalysedtheperformance domain setting, i.e., we train with all the domains ofthepresence,intersection,andspectrumkernels but the one we classify. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: SK single-source cross-domain polarity classiﬁcation accuracy (in %), where each column headerfollowsthe”trainingdomain→testdomain“format. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 971> 

<Paper ID = 972>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Accuracy of word rating prediction in a 10-fold cross-validation setup. </Abstractive Summary>  <Extractive Summary> =  Furthermore, our informa- moreextremeratings. tion comes in lists rather than sets, contexts, or The results are presented in Table 1 and show patterns, which presents a problem for other ex- that our method (SNCut) consistently performs isting methods (Tang et al., 2014; Pham et al., best across both ratings – valence and arousal – 2015;Schwartzetal.,2015).AnalternativetoSSC and across all three languages. For English and – must-link / cannot-link clustering (Rangapuram Spanish, the larger margins of improvement over andHein,2012)–hasthedownsideofrequiringa the mean baseline and K-NN are obtained on va- choiceofthresholdfordeﬁningthemust-linkand lence.Thisisparticularlyintuitive,asoppositeva- cannot-link underlying graph edges. </Extractive Summary>  </Table ID = 1>  </Paper ID = 972> 

<Paper ID = 973>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  z ∝ exp(W s +U h +b ), l 2 l 2 t 2 z ∝ exp(W s +U h +b ), r 3 r 3 t 3 4.2 Parameters&Metrics where z +z +z = ~1. The linear interpolation The hyper-parameters are given in Table 24. We l r amongs,s ands isformulatedas use GloVe vectors (Pennington et al., 2014) with l r 200 dimensions as pre-trained word embeddings, s˜= z(cid:12)s+z (cid:12)s +z (cid:12)s . </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  4.4 FinalResults Wecompareourmodelswithpreviouswork. The 4.6 Examples ﬁnal results are shown in Table 4. Our ﬁnal mod- els outperform both Zhang et al. </Extractive Summary>  </Table ID = 4>  </Paper ID = 973> 

<Paper ID = 974>  </Paper ID = 974> 

<Paper ID = 975>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Memory dimensions d and total network public available Glove vectors with a 300 dimen- parameters|θ|foreverynetworkvariantevaluated sionality. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  All initialized word vec- torsareﬁnetunedduringthetrainingprocessalong 3.2 Results with every other parameter. Every matrix is ini- The evaluation results are presented in Table 2 tialized with the identity matrix multiplied by 0.5 in terms of accuracy, for several state-of-the-art except for the matrices of the softmax layer and models proposed in the literature as well as for the attention layer which are randomly initialized the TreeGRU and TreeBiGRU models proposed fromthenormalGaussiandistribution. Everybias in this work. </Extractive Summary>  </Table ID = 2>  </Paper ID = 975> 

<Paper ID = 976>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Thus, social media posts can contain in- bought a product, etc. Table 1 shows one ran- formation useful for marketing and customer re- domly picked example for each of the purchase lationship management, including user behavior, stages as well as for an artiﬁcial class ‘N’ which opinions,andpurchaseinterest. weusefortweetsnotexpressingapurchasestage. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  els: Alogisticregressionclassiﬁer(LR)andalin- 5 ExperimentsandResults earsupportvectormachine(SVM).Forbothmod- els, the tweets are represented by 1-gram, 2-gram Due to the high class imbalance in our dataset, and 3-gram bag-of-word (BOW) vectors. Table 3 we use the macro F1 of the non-artiﬁcial classes shows that the RCRNN clearly outperforms non- as our evaluation measure. We implement the neuralmodels. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The results pro- quences. Our results indicate that tweets in- vided in Table 5 show that GRU outperforms the deed contain signals indicative of purchase stages FFmodels. Thus,thereiscross-tweetinformation which can be captured by deep learning models. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Finally,weinvestigatewaysofdealingwithim- balanced data: We replace the ranking layer of References RCRNN with a cross-entropy (CE) loss with and without class weights (see Section 4.1). Table 6 Sitaram Asur and Bernardo A. Huberman. </Extractive Summary>  </Table ID = 6>  </Paper ID = 976> 

<Paper ID = 977>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ctx We found that on both language pairs 35%–36% 3.2 ResultsandDiscussion of rules are basic rules. While the proportion of Table 1 shows BLEU scores of all systems. We segmentingrulesis∼53%,selectingrulesonlyac- found that GBMT is better than PBMT across countfor11%–12%. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: BLEU scores of GBMT when differ- ctx to try soft constraints. </Abstractive Summary>  <Extractive Summary> =  In totheconventionalGBMTsystem. Theresultsin addition,thesystemTBMTdoesnotshowconsis- Table 3 suggest that both segmenting and select- tentimprovementsoverPBMTwhilebothGBMT ingrulesconsistentlyimproveGBMTonbothlan- and GBMT achieve better BLEU scores than guagepairs. However,segmentingrulesaremore ctx TBMTonbothZH–EN(+1.8BLEU,intermsof usefulthanselectingrules. </Extractive Summary>  </Table ID = 3>  </Paper ID = 977> 

<Paper ID = 978>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Characteristics of our test sets. </Abstractive Summary>  <Extractive Summary> =  of fairness, we report results of the embedding- Stringfeatures Five features recording the basedapproachesonthosetermsonly. length (counted in chars) of s and t, their differ- Themaincharacteristicsofourtestsetsarepre- ence, their ratio, and the edit-distance between sented in Table 1. As an illustration of the dif- the two. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We kept the embeddings wefoundittobelessaccurateforourtask. that yielded the best performance for the Miko 4.2 RerankingIndividualApproaches approach, and ran several conﬁgurations, varying the bilingual lexicon used, and tuning the ratio The middle column in Table 2 reports the rerank- parameteroverthevalues0.5,0.8and1.0. ing of the n-best list produced by each individ- The best performance for the variants of each ualapproach. ing of the n-best list produced by each individ- The best performance for the variants of each ualapproach. Duringcalibrationexperiments, we strategy we tested is reported in the ﬁrst column foundbetterrescoringperformanceswiththeRan- of Table 2. On Wiki , the Rapp approach dom Forest algorithm. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Inﬂuence of the features used to train the reranker when combining Rapp, Miko, and Faru. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  On Wiki averagepositionofthereferencetranslationinthe ≤25 however,thispercentageismuchlower(4%). reranker’s output is clearly improving for all test sets, as shown in Table 4. The average number Wiki Wiki Euro of positions gained by reranking is rather high, >25 ≤25 5−6k and outdoes an oracle that picks the n-best list in MORPHO 18 3 26 which the reference translation is best positioned. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Annotation of 100 translations produced reranker 5.6 4.0 4.9 (atrank1)foreachtestsetbythererankedoutput ofthe3nativeapproaches. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  </Paper ID = 978> 

<Paper ID = 979>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  tiveindecoding. Table 2 shows the performance in terms of de- Model Cs-En De-En Zh-En codingspeed. WeusethesamewrapperforHiero Phrase-based 20.32 24.71 25.68 andLR-Hierotoquerythelanguagemodelandre- +LRM 20.74 25.99 26.61 port the average on a sample set of 50 sentences Hiero 20.77 25.72 27.65 from test sets. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  threedifferentlanguagepairs. Table 3 compares the performance of different translation systems in terms of translation quality Acknowledgments (BLEU). In all language pairs the proposed lexi- The authors wish to thank the anonymous re- calizedreorderingmodelimprovesthetranslation viewers for their helpful comments. </Extractive Summary>  </Table ID = 3>  </Paper ID = 979> 

<Paper ID = 980>  <Table ID = 1>  <Abstractive Summary> =  Table 1: The size and accuracy of extracted seed ofouralgorithmisthesetoftranslationpairspro- lexicons. </Abstractive Summary>  <Extractive Summary> =  The source and target vocabularies ofthembeingnumbersandpropernouns. consist of the 2000 most frequent words from the Table 1 shows that our extraction method pro- source and target corpora, with the exception of duces seed lexicons of a reasonable size and ac- the words that are in the seed lexicons. For each curacy, with, on average, 219 translation pairs at of these 2000 source words, the task is to ﬁnd a 85%accuracy. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  problem,wheretheedgecostforanysource-target pair is the normalized edit distance between the 3.5 ComparisontoHaghighietal.(2008) twowords. The results in Table 2 show that the method Unlike most of the previous work on lexicon of Mikolov et al. (2013b) (MIK13-Auto), repre- induction, our method is fully unsupervised, sented by the ﬁrst translation matrix derived on with no dependency on additional resources or our automatically extracted the seed lexicon, per- tools. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  targettestvocabularies. Table 3 shows the results on English–Spanish and English–French. The upper rows con- References tain fully-unsupervised results. </Extractive Summary>  </Table ID = 3>  </Paper ID = 980> 

<Paper ID = 981>  </Paper ID = 981> 

<Paper ID = 982>  <Table ID = 1>  <Abstractive Summary> =  Table 1: APT scores of the baseline (BL) and the their 166 70 1 148 coreferenceawaresystem(CM).CMoutperforms Coref.(CM) its his her their BLby41pronouns. </Abstractive Summary>  <Extractive Summary> =  The language model is The APT scores of the Moses baseline (BL) and trained on an NC 2011 monolingual set with ca. our system (CM) are shown in Table 1. Our sys- 1.1Msentences. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  otherpronouns,ourmethodwillnottranslatethem When examining the translation of the deter- differentlyfromthebaselinesystem,thereforewe minersu,thecomparisonoftheﬁrsttwoconfusion donotcountthembelow. matrices in Table 2 shows that CM translates su We measure the Accuracy of Pronoun Transla- morepoorlythanBL.Inparticular,itmissesmany tion (APT) by comparing the translated pronouns occurrencesofsuthatshouldhavebeentranslated with those in the reference translation (Miculi- as its, rendering them generally by his. This is cichWerlenandPopescu-Belis,2016). </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: APT scores of CM and oracle systems. </Abstractive Summary>  <Extractive Summary> =  634Example1 ing gender-number-humanness features for each SRC: no podra´ sentirse en su-masc-sg-pers pronoun. WethentranslatedthisdatawithourCM casaenesepa´ıs system,andcompareditwiththeoutputofCMus- CM: will not be able to be in his house in the ing CorZu antecedents, in Table 3. The accuracy country whenusingoracleantecedentsis83%,andamong REF: he will scarcely be able to feel at home the11errors(translationsdifferingfromtherefer- there ence), 8 are in fact considered as correct by a hu- manjudge. </Extractive Summary>  </Table ID = 3>  </Paper ID = 982> 

<Paper ID = 983>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  1–Verywellto5–Verypoorly. Table 3 suggests that the intelligibility of both the English and German product listings are perceived to bestliststobeabletoevaluatethesetwoscenariosin- besomewherebetween“easy”and“neutral”whenim- dependently. agesarealsoavailable. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Results for re-ranking n-best lists generated for eBay’s test set with text-only and multi-modal NMT models. </Abstractive Summary>  <Extractive Summary> =  Training our text-only NMT baseline on modelNMT stillfallsbehindthePBSMTmodelboth t m this large corpus would not help shed more light on in terms of BLEU and TER, although it seems to be how multi-modality helps MT, since it has no images catchingupasthedatasizeincreases. available and thus cannot be used to train the multi- In Table 5, we show results for re-ranking 10- and modal model NMT . Rather, we report results of re- 100-bestlistsgeneratedbyeBay’sPBSMTproduction m ranking experiments using n-best lists generated by system. </Extractive Summary>  </Table ID = 5>  </Paper ID = 983> 

<Paper ID = 984>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  In additional experiments we found the overall structure of these clusterings to be relatively sta- ble across models, but for very similar languages either one of the vectors learned during training, (such as Danish and the two varieties of Norwe- or some arbitrary other point. Table 1 shows text gian) the hierarchy might differ, and the some samples from different points along the line be- holdsforlanguagesorgroupsthataresigniﬁcantly tween Modern English [eng] and Middle English differentfromthemajorgroups. Anexamplefrom [enm]. </Extractive Summary>  </Table ID = 1>  </Paper ID = 984> 

<Paper ID = 985>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Sparse lexicon with different threshold θˆ psp(f|e) = f0∈F(e) f0|e (6) valuesandbackoffmodels(λ = 0.99). </Abstractive Summary>  <Extractive Summary> =  Asaback- off model, we use uniform distribution, unigram τ ofsourcewords,orKneser-Neylowerordermodel 35 0.001 (KneserandNey,1995;Fosteretal.,2006). 0.000001 In Table 2, we illustrate the effect of the sparse 0.0 0.2 0.4 0.6 0.8 1.0 lexicon with EUTRANS Spanish→English task λ (Amengualetal.,1996),comparingtotheexisting Figure1: Relationbetweensparselexiconparam- EMdeciphermentapproach(fulllexicon). Byset- eters(EUROPARLSpanish→Englishtask). Due to the small vocabulary size, (Martinetal.,1998),followingthesesteps: we are comfortable to use higher order class LM, which yields even better accuracy, outperforming 1. Estimate word-class mappings on both sides the non-pruned results of Table 2. The mem- (C ,C ) src tgt oryandtimerequirementsareonlymarginallyaf- 2. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Sparse lexicon with word class initial- for runtime efﬁciency. </Abstractive Summary>  <Extractive Summary> =  optimuminlaterstageoftraining. Onemightsug- gesttopruneonlyforlateriterations,butforlarge Table 3 shows that translation quality is con- vocabularies,asinglenon-prunedE-stepcanblow sistently enhanced by the word class initializa- upthetotaltrainingtime. tion, which compensates the performance loss We rather stabilize the training by a proper ini- caused by harsh pruning. </Extractive Summary>  </Table ID = 3>  </Paper ID = 985> 

<Paper ID = 986>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We show the MELA co-reference metric6 mies, we use a set of heuristics, e.g. if a verb has andthepairwiseF1scoresforelidedsubjectsand two child nodes labelled as direct objects, we as- possessive pronouns in Table 1, from which we sumethatoneofthemisactuallythesubject. conclude that our adaption achieves satisfactory Furthermore, we annotate the possessive pro- performance.7 nounssuandsuswiththemorphologicalinforma- tionofthepossessoridentiﬁedbytheco-reference 3 DummySubjectsandCo-Reference system. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  lowerthanthosereportedinSemEval2010. lingfil.uu.se/ 658es en P(en|es) es en P(en|es) he 0.317 his 0.532 dummy-he NULL 0.188 su-masc-sg its 0.136 it 0.126 their 0.110 NULL 0.277 her 0.370 she 0.245 su-fem-sg his 0.179 dummy-she it 0.114 its 0.144 he 0.082 their 0.109 it 0.317 its 0.489 dummy-it is 0.168 su-nonhum-sg their 0.185 NULL 0.126 NULL 0.103 Table2: LexicalAlignmentProbabilities Table 2 illustrates the lexical translation prob- where a gendered form is translated with a neuter abilities for third person dummies and annotated form (e.g. dummy-she → it) and cases where a possessive pronouns. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: BLEU scores (average of 5 tuning runs) translationdidnotreﬂectthemorphologicalanno- withandwithoutco-referenceannotations tationinthesource. </Abstractive Summary>  <Extractive Summary> =  However, we still observed cases where the Table 4: BLEU scores (average of 5 tuning runs) translationdidnotreﬂectthemorphologicalanno- withandwithoutco-referenceannotations tationinthesource. Wedistinguishbetweencases According to the evaluation in Table 4, insert- ing co-reference annotations results in a small in- 10Hisandheoccuralmost20,000timesinthenewscom- mentary 2011 corpus, whereas the corresponding feminine crease in BLEU scores for the large random test pronounsamounttoroughly3,000. set and for some of the small test sets. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  translationonthesubjectdummiesandtheposses- 3.2 APT:AccuracyofPronounTranslation sive pronouns separately. Table 5 shows the APT APT (Werlen and Popescu-Belis, 2016) is a met- scores for the baseline and the annotated phrase- ric to assess the quality of the translation of pro- basedsystem.15 Theoraclescoresarenever100% nouns. Instead of scoring the entire translation, for two reasons: Some pronouns have no corre- APTcalculatestheaccuracyofthepronountrans- spondence in the reference translation (consider lations through word alignment of the source, the the example in Table 3: su a´nimo cambio´ → this hypothesis, andthereferencetranslation. </Extractive Summary>  </Table ID = 5>  <Table ID = 3>  <Abstractive Summary> =  Table 3: su a´nimo cambio´ → this hypothesis, andthereferencetranslation. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 986> 

<Paper ID = 987>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  traininglossincurredfortheencoding. The ﬁlters of the convolutional layer (LeCun Table 1 shows character and word level statis- andBengio,1995;Kim,2014)areconvolvedwith tics per product title in the training and test set. with a window of consecutive observations (char- It is evident from the mean word and character acters or words), and produce an encoding of the counts that, on average, Japanese product titles in window. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We compare places. the average number of characters in the words of the8000titlesinthetestsetforwhichourACNN Table 2 shows that our proposed ACNN model model provides correct predictions over the CNN –theCNNmodelaugmentedwithwordandchar- model,tothatfortheoveralltestsetfromTable1. acter based attention mechanisms, improves over Thecountfortheformercaseturnsouttobe9.245 the baseline CNN model by an absolute 0.37%, that is substantially higher than that for the latter whichtranslatestomorethan8,000testtitlesbe- case, which is 5.799. </Extractive Summary>  </Table ID = 2>  </Paper ID = 987> 

<Paper ID = 988>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Ourcharacterembeddingmoduleismo- m [500,500,500] Convolutional 3 w [3,4,5] tivated by the success of other distributed vector Pooling max representationslikewordembeddings(Mikolovet Fullyconnected 1 #ofunits Dependsonthe#ofauthors al., 2013) This module learns a continuous, non- Table1: Neuralnetworkarchitecturehyperparameters sparse d-dimensional vector representation of the character n-grams. The maximum length l of the Table 1 contains the combination of hyperpa- trainingsequencesdeterminesthesizeoftheinput rameters for the three modules that generate the and input shorter than l are padded. This module bestvalidationscore. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  This module bestvalidationscore. Additionally,wehaveadded 670a dropout layer with 25% dropout after the ﬁrst in Table 2. The results show that our CNN bi- embeddinglayerforregularization. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Accuracy comparison for increasing # of authors Table2: Accuracyfor50authorswith1000tweetseach. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Accuracy values for 35 authors with 1000 tweets eachafterbot-likeauthorsremoval(15authorswerebots). </Abstractive Summary>  <Extractive Summary> =  There are no comparable experiments in methodbytakingthemaximumsaliencyvalueper (Schwartz et al., 2013), thus we compare only character. against CHAR, LSTM-2, and CNN-W as shown We selected two authors, one bot-like and in Table 5. The accuracies for all of the methods one human, to analyze what kind of patterns are decreaseonthisdatasetasthebot-likeauthorsare learnedforspeciﬁcauthors. </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  InWorking vation values aggregated over all ﬁlters. The sec- Notes Papers of the CLEF 2015 Evaluation Labs, ond column in Table 6 shows the top 15 bigrams volume1391. from this analysis for CNN-2 models trained on RonanCollobert,JasonWeston,LéonBottou,Michael the whole dataset and on the reﬁned dataset. </Extractive Summary>  </Table ID = 6>  </Paper ID = 988> 

<Paper ID = 989>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  n·+2π−1 nt·,y=1+Vβ−1 ny·=0+Vβ0−1 For each word position in the sentence, ﬁrst sam- ple a value for switching y based on π and then pick the word based on the word distribution ϕt 4 Experiment of the topic z if y = 0, or from background word distribution ϕ otherwise. Figure 2 illustrates the Reviews from 5 categories (details in Table 1) b generative process using watches as an example, of Amazon.com Review Dataset (McAuley and showingtop3aspectsandtheirprobabilities. Leskovec, 2013) are used as the corpora. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Top topics and topic words for Laptops. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Topwordsforthetoptopicsdiscoveredineach Warranty warranty,back,service,unit,repair... general category are presented in Table 2 in the Screen screen,picture,monitor,color,bright... form of one topic per line, along with the top ranked words in this topic. </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The hit rates DavidM.Blei, AndrewY.Ng, andMichaelI.Jordan. of different products are given in Table 5. To be 2003. </Extractive Summary>  </Table ID = 5>  </Paper ID = 989> 

<Paper ID = 990>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  It is a 18- napped”byRobertLouisStevenson,whichisvery class classiﬁcation problem, which is consider- different from the other considered books by the ably more challenging. Table 1 (column ‘Accu- same author, is confused with “Treasure Island” racy Auth’) shows the performance of our model also by Stevenson, and “Great Expectations” by with 10-fold cross-validation when using the full Charles Dickens. “Pride and Prejudice” by Jane setoffeaturesanddifferentfeaturecombinations. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Performance of our model compared to TO Elaboration VC DT Elaboration TO IM CC otherparticipantsonthe“PANLiterary”dataset presentverbs OPRD sentenceSTD determiners subordinatewidth PRT parentheses PRP quotations Contrast commas discoursemarkers OBJ PRP periods VC as the best performing approach of the shared CC Manner-means stopwords VBZ task, the META-CLASSIFIER (MC), by a large sentenceSTD RBR OPRD CONJ nouns positivewords AMOD ﬁrstP margin. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: 20 features with the highest information siontheaveragedprobabilityscoresofallofthem. </Abstractive Summary>  <Extractive Summary> =  gaininalltheexperiments 4.5 FeatureAnalysis not in the other experiments. This is likely be- Table 3 displays the 20 features with the high- causetheycanserveasindicatorsofthestructural estinformationgain,orderedtop-down(upperbe- complexity of a text and thus of the idiosyncrasy ing the highest) for each of the presented experi- of a writing style of an individual – as punctua- ments.5 Syntactic features prove again to be rele- tion marks such as periods and commas, which vant in all the experiments. The table shows that are typical stylistic features. </Extractive Summary>  </Table ID = 3>  </Paper ID = 990> 

<Paper ID = 991>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Scaling performance for the positioning tweenpartiesfromthesamecountrycomesfrom regardingEuropeanintegration. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 991> 

<Paper ID = 992>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Dataset overview. </Abstractive Summary>  <Extractive Summary> =  i tiontaskusingthefollowingtwomedicalabstract Inordertomodeldependenciesbetweensubse- datasets,whereeachsentenceoftheabstractisan- quent labels, we incorporate a matrix T that con- notated with one label. Table 1 presents statistics tainsthetransitionprobabilitiesbetweentwosub- oneachdataset. sequentlabels;wedeﬁneT[i,j]astheprobability thatatokenwithlabeliisfollowedbyatokenwith NICTA-PIBOSO This dataset was introduced the label j. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: F1-scores on the test set with several baselines, Table3: Ablationanalysis. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Confusion matrix on PubMed 20k RCT obtained Total 90.1 89.9 90.0 30135 with our model. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 6>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Results for each class obtained by our model on backgroundsentenceswerepredictedasmethod. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  difﬁcultyindistinguishingmethodsentencesfrom Structured prediction: The labels for all sen- resultsentences. tences in an abstract are predicted jointly, which Table 4 presents a few examples of prediction improves the coherency between the predicted la- errors. Ourerroranalysissuggeststhatafairnum- belsinagivenabstract. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Theablationanalysispre- ber of sentence labels are debatable. For exam- sented in Table 3 shows that the sequence opti- ple, the sentence “We conducted a randomized mization layer is the most important component study comparing strategies X and Y.” belongs to oftheANNmodel. the background according to the gold target, but Joint learning: Our model learned the features mosthumanswouldclassifyitasanobjective. </Extractive Summary>  </Table ID = 3>  </Paper ID = 992> 

<Paper ID = 993>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Example of a topic and its textual and Wikipedia article titles using the cosine similarity imagelabels. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  703food,eat,cook,chicken,recipe, drive,computer,card,laptop,memory, TopicTerms cup,cheese,add,taste,tomato battery,usb,intel,processor,hard ImageLabel PredictedRating 2.53 1.87 TextualLabel Cooking DesktopComputer PredictedRating 1.98 2.20 Table3: Exampleoftwotopicsandtheirgeneratedtextualandimagelabelsandpredictedratings. dimensional input (representing the letter trigram Wepresenttheresultsofallsystems(baseline, andPageRankfeatures)intoa128-dimensionvec- joint-NN and disjoint-NN) in Table 2. Each tor and concatenating it with the 256-dimension modelistrainedusing10-foldcross-validationfor hiddenrepresentationatthethirdlayer(thusyield- 10 epochs. </Extractive Summary>  </Table ID = 2>  </Paper ID = 993> 

<Paper ID = 994>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  For the NYT corpus, we work with 19,086 Intheﬁrststage,webuildanestimatore,equal document-summarypairspublishedbetween1987 totheprobabilitythatasampleispredictedaspos- and2006fromtheBusinesssection. itive given that it is indeed positive, p(o = 1|y = Table 3 in Section 5 shows a summary from 1). Weﬁrsttrainalogisticregression(LR)classier the NYT corpus. The annotators signed-rank test is less than 0.001, indicating that are given the option to indicate that the two sum- the improvement is signiﬁcant. Table 3 shows maries are equally informative with respect to the the example of lead paragraph along with the In- content of the NYT summary. We randomize the foFilter summary with the unimportant sentence orderofsentencesinbothLeadWordsandInfoFil- removed. </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  TheDUCmodel article sentences that contribute to the summary is trained using the articles and summaries from (positiveclass)ornot(negativeclass)1. DUC2002 dataset, where 1,833 sentences in total Table 2 shows the evaluation results on the appear in the summaries. We also randomly sam- human-annotated test set. Sentences with becausewecannotoutputanemptytextasasum- Jacana alignment scores less than or equal to 10 mary. form the unlabeled set, including 20,653 (12.9%) Each row in Table 2 corresponds to a model unlabeled sentences in total. Liblinear (Fan et al., trained with one training set. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  samples to balance the training set. According to Although the unimportant class is the majority thecriteriadescribedinNYTcorpussection,there (see Table 1), predicting all test samples as not are 22,459 (14.1%) positive sentences selected summary-worthyislessusefulinrealapplications from total of 158,892 sentences. Sentences with becausewecannotoutputanemptytextasasum- Jacana alignment scores less than or equal to 10 mary. </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Performance comparison on single- Turk (AMT) and each summary pair is assigned documentsummarization(%) to 8 annotators. </Abstractive Summary>  <Extractive Summary> =  veryhigh(87%)forbothtestsets. Consistentwith Table 4 shows the ROUGE scores for all sum- theobservationabove, theDUCmodelispredict- marizers. InfoRank signiﬁcantly outperforms Ic- ing intrinsic importance more aggressively. </Extractive Summary>  </Table ID = 4>  </Paper ID = 994> 

<Paper ID = 995>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  We treat bigram features as discrete fea- withlittlelossintime. tures (Collobert et al., 2011) for our neural net- Table 3 shows results for NER on test sets. In work. In work. Ourmodelsaretrainedusingstochasticgra- the Table 3, we also show micro F1-score (Over- dientdescentwithanL2regularizer. all) and out-of-vocabulary entities (OOV) recall. We design a F-Score driven training tions for future work. We can observe all models method in our third model F-Score Driven Model in Table 3 achieve a much lower recall than pre- I . We propose an integrated training method in cision (Pink et al., 2014). </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The results of two meth- entityandnominalmention. ods are shown as Table 2. We can see that posi- Tobetterunderstandtheimpactofthefactorβ, tionalcharacterembeddingsperformbetterinneu- we show the results of our integrated model with ral network. </Extractive Summary>  </Table ID = 2>  </Paper ID = 995> 

<Paper ID = 996>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Alignment was performed via string turevectorf(q)andthencomputeitstransformed edit distance, leading to a 55% alignment to the feature vector t (q) given model parameters θ, original annotations. Table 1 dev/test reﬂects the θ formingaweightedquery. Wemodiﬁedthesimilar- subsetresultingfromthisalignment;alloftheorig- ityfunctionofLUCENEwhenexecutingmultiway inalWIKIQAtrainwasusedintraining,alongwith postingslistmergingsothatfastefﬁcientmaximum 50negativeexamplesrandomlysampledperques- inner product search can be achieved. </Extractive Summary>  </Table ID = 1>  </Paper ID = 996> 

<Paper ID = 997>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Percentage of positive examples in the Table2: ImpactofCNNvs.LSTMsentencemod- trainingdatasetsforeachtask. </Abstractive Summary>  <Extractive Summary> =  Each of the three datasets is in turn divided Weconcatenatethethreerepresentations, h = j intraining,dev.andtestsets. [x ,x ,x ],andfedthemtoahiddenlayer qnew qrel crel Table 1 reports the label distributions with re- tocreateasharedinputrepresentationforthethree specttothedifferentdatasets. ThedataforTaskC tasks, h = σ(Wh + b). </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  not observe any important improvement of tasks A and B in our MTL setting. More insights and 4.2 Results results are available in our longer version of this Table 3 shows the results of our individual and paper(Bonadimanetal.,2017). MTLmodels,incomparisonwiththeRandomand IRbaselinesofthechallenge(ﬁrsttworows),and 5 RelatedWork theSemEval2016systems(rows3–12). </Extractive Summary>  </Table ID = 3>  </Paper ID = 997> 

<Paper ID = 998>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Accuracy results of classiﬁcation methods on Set1 for selected Enron users. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Accuracy results of classiﬁcation methods on Set2 for selected Enron users. </Abstractive Summary>  <Extractive Summary> =  2004. AutomaticCategorizationofEmailintoFold- Table 3 shows that the overall trend is similar to ers:BenchmarkExperimentsonEnronandSRICor- whatisobservedinTable2. pora. </Extractive Summary>  </Table ID = 3>  </Paper ID = 998> 

<Paper ID = 999>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  max of the vectors for multi-word units. Lexical DRandCRtasksresultsarepresentedrespectively contexts are thus represented by 200-dimensional in Table 4b and Table 4c. For both tasks, we vectors. max of the vectors for multi-word units. Lexical DRandCRtasksresultsarepresentedrespectively contexts are thus represented by 200-dimensional in Table 4b and Table 4c. For both tasks, we vectors. the DR task and 0.573 (F1-Measure) for the CR Acknowledgements task,whicharecomparabletoourresults(0.87for theDRtaskand0.53fortheCRtask). TheauthorsthanktheBiomedicalInformaticsDe- Table 4a also indicates that replacing lexical partmentattheRouenUniversityHospitalforpro- formsbywordembeddingsseemstohaveanega- vidingaccesstotheLERUDIcorpusforthiswork. tiveimpactonperformanceineverycase. </Extractive Summary>  </Table ID = 4>  </Paper ID = 999> 

<Paper ID = 1000>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  To evaluate our models on the relations be- rametertuningwasﬁnalized,weevaluatedourbest tween clinical events, we ﬁltered out all general event-eventandevent-timemodelsontheheld-out events(andrelationsassociatedwiththem)usinga testset. 748Model Event-timerelations Event-eventrelations P R F1 P R F1 THYMEsystem(allevents) 0.577 0.845 0.685 0.595 0.572 0.584 CNNtokens(allevents) 0.683 0.717 0.700 0.688 0.412 0.515 THYMEsystem(medicaleventsonly) 0.230 0.851 0.362 0.215 0.703 0.330 CNNtokens(medicaleventsonly) 0.272 0.714 0.394 0.300 0.519 0.380 Table3: Event-timeandevent-eventcontainsrelationsontestset 3.3 Results formedthefeature-basedTHYMEsystemwhich includes not only tokens and part of speech tags, Table 1 shows the evaluation of different model butsyntactictreefeaturesandgoldeventandtime typesandfeaturesetsonthedevset. Forbothevent- properties. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  This neuralmodelsoutperformthefeature-basedsystem suggeststhattheneuralmodelsforevent-eventre- inbothcases. lations are not able to generalize over the token Finally, Table 3 shows the performance of the inputaswellastheywereforevent-timerelations. state-of-the-artTHYMEsystemandthebestneural This may be due in part to the difﬁculty of the systemsonthetestset. </Extractive Summary>  </Table ID = 3>  </Paper ID = 1000> 

<Paper ID = 1001>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Let n be the number of mentions, T the true i set of tags of mention i and Y the predicted set. Table 1 shows performance of PthDCode on i P Then, we deﬁne P = R = 1/n n δ(Y = T ) test, based on the interval [40000,50000]; av- P i=1 i i for strict; P = 1/n n (|Y ∩ T |)/(|Y |) and erage and standard deviation are computed for P i=1 i i i R = 1/n n (|Y ∩T |)/(|T |)forloosemacro; 2000(20 + i),0 ≤ i ≤ 5, as described above. Pi=1 i i Pi and P = ( n |Y ∩T |)/( n |Y |) and R = PthDCode achieves clearly better results than P i=1 Pi i i=1 i ( n |Y ∩T |)/( n |T |))forloosemicro. Anotherspecial ent, results are not directly comparable. An im- symbol EOL (End Of Label) is appended to short portant observation in Table 1 is that most of the paths, so that all hierarchical paths have the same improved systems (Ren et al., 2016; Yogatama et length. We use ADAM (Kingma and Ba, 2014) al.,2015)considerentityclassiﬁcationinahierar- with learning rate .001 and batch size 500. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  bilityof.1tofeedforwardweights. Theweightsof feedforward units are initialized with an isotropic Table 2 shows that for level-wise comparisons Gaussiandistributionhavingmean0andstandard on loose micro F , PthDCode improves recall 1 deviation .02 while weights of recurrent units are compared to Yogatama et al. (2015)’s precision initializedwithrandomorthogonalmatrix. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Top 5 Attention per level (L1/L2). </Abstractive Summary>  <Extractive Summary> =  tion. This introduces noise and complexities like Table 3 shows, for some examples, which ﬁve unrelated labels, redundant labels and large sizes words received the highest attention on level 1 of candidate label sets. To address these chal- (L1) and on level 2 (L2). </Extractive Summary>  </Table ID = 3>  </Paper ID = 1001> 

<Paper ID = 1002>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  X The sizes of the resulting data splits are given in q (m ) = |N(i)|−1 p(m | m ). (3) i i i j Table 2. For the development and test splits we j∈N(i) always include the lemma (as is standard) while Crucially,someofthedistributionsarestochastic sampling additional observed forms. </Extractive Summary>  </Table ID = 2>  </Paper ID = 1002> 

<Paper ID = 1003>  </Paper ID = 1003> 

<Paper ID = 1004>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  mean, likeinthe PRECISE evaluation, thatthean- swer is in the computed result set. Table 1 shows 3.2 IntegratingSemanticsandPragmatics results for two cases: full has all features turned ManyoftheMRLexpressionsgeneratedinphase on; no-equiv reduction shows results when the threearesemanticallyequivalent,butsyntactically auxiliary process described in section 3.2 is dis- distinct. The second auxiliary process uses a the- engaged. </Extractive Summary>  </Table ID = 1>  </Paper ID = 1004> 

<Paper ID = 1005>  </Paper ID = 1005> 

<Paper ID = 1006>  </Paper ID = 1006> 

<Paper ID = 1007>  </Paper ID = 1007> 

<Paper ID = 1008>  </Paper ID = 1008> 

<Paper ID = 1009>  </Paper ID = 1009> 

<Paper ID = 1010>  </Paper ID = 1010> 

<Paper ID = 1011>  </Paper ID = 1011> 

<Paper ID = 1012>  </Paper ID = 1012> 

<Paper ID = 1013>  </Paper ID = 1013> 

<Paper ID = 1014>  </Paper ID = 1014> 

<Paper ID = 1015>  </Paper ID = 1015> 

<Paper ID = 1016>  </Paper ID = 1016> 

<Paper ID = 1017>  </Paper ID = 1017> 

<Paper ID = 1018>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Specialized ﬁelds and textual genres in- “Supporting details” section includes two differ- cludedinarText. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 1018> 

<Paper ID = 1019>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Recognition results for the LF-MMI putationalconstraints. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 1019> 

<Paper ID = 1020>  </Paper ID = 1020> 

<Paper ID = 1021>  </Paper ID = 1021> 

<Paper ID = 1022>  </Paper ID = 1022> 

<Paper ID = 1023>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Hov- identiﬁcation of places or dates. Table 2 shows eringthemouseonatrajectory,thesnippetoftext thedistributionperdomainoftheindividualswith fromwhichitwasautomaticallyextractedappears associated movements. Moreover, we corrected onthebottomleft. </Extractive Summary>  </Table ID = 2>  </Paper ID = 1023> 

<Paper ID = 1024>  </Paper ID = 1024> 

<Paper ID = 1025>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  </Paper ID = 1025> 

<Paper ID = 1026>  </Paper ID = 1026> 

<Paper ID = 1027>  </Paper ID = 1027> 

<Paper ID = 1028>  </Paper ID = 1028> 

<Paper ID = 1029>  </Paper ID = 1029> 

<Paper ID = 1030>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  FortheAnd,atleastonewordhas giventothenextcomponent(yes/noarrowsinthe to exist that survived every subtraction (intersec- diagram). Table 1 shows the description of each tionofdifferencesetsshouldbenon-empty) componentincollocationextractordiagram. Performance. </Extractive Summary>  </Table ID = 1>  </Paper ID = 1030> 

<Paper ID = 1031>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  liographic information. Experimental results in Table 2 show that our model can predict relative Wang Ling, Yulia Tsvetkov, Silvio Amir, Ramon Fer- words or similar authors favorably well. We plan mandez, Chris Dyer, Alan W Black, Isabel Tran- coso, and Chu-Cheng Lin. </Extractive Summary>  </Table ID = 2>  </Paper ID = 1031> 

<Paper ID = 1032>  </Paper ID = 1032> 

<Paper ID = 1033>  </Paper ID = 1033> 

<Paper ID = 1034>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  3Fully in the spirit of Krahmer (2010), who argues that The main goal of this proposal is to better un- computationallinguistscanlearnagreatdealfrompsychol- derstandtheprocessofdescribinganimage,from ogists(andviceversa). 22.1 Whattoincludeinadescription 2.2 Marking Table 1 shows the ways in which a phrase or ex- Following Jakobson (1972) and others in linguis- pression can be related to an image.4 It can ei- tics, we will use the term marking to denote the ther refer to something inside or external to the act of signaling an entity or attribute. The differ- image, and annotators can choose whether or not encebetweenSituationAandCisoneofmarked- to useit in their description. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: A categorization of labels based on whether the label is applied on the basis of someone’s appearanceorthesituationtheyarein. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  promptshadaneffectonthelengthoftheexpres- To ﬁnd out whether there are such within- sions (with longer expressions in the CR and CT subject effects in the MS COCO and Flickr30K conditions), and on the amount of adjectives used data, it is necessary to know who provided which (with more adjectives in the CR-condition than in description, and in what order the images were the FA-condition). Table 3 shows an example de- presented. Because the raw crowdsourcing data scriptionforeachcategory. </Extractive Summary>  </Table ID = 3>  </Paper ID = 1034> 

<Paper ID = 1035>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  With this approach a slightly types of surface features did not improve the re- better F-score as with SPSim is obtaind (0.29), sults. see Table 4. However, it is outperformed by the Differing from the result obtained by Ciobanu machine learning approach. </Extractive Summary>  </Table ID = 4>  </Paper ID = 1035> 

<Paper ID = 1036>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  original if we remain unable to replicate the out- putoftheitemsetminingphase. Table 4 shows a comparison of the results we have obtained so far using our implementation of Original Ours TBAandthevaluesprovidedbyLiuetal.(2012). Corpus P R P R Oncemore,weremainunabletoreplicatetheper- ApexDVDPlayer 0.90 0.86 0.239 0.328 CreativeMP3Player 0.81 0.84 0.180 0.319 formancereportedbythepaper. </Extractive Summary>  </Table ID = 4>  </Paper ID = 1036> 

<Paper ID = 1037>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  ”atthesametime”). Thus, syntacticparsing(eitherphrase-structureor dependency) must precede identiﬁcation of sister The numbers for the three types of multi-label VPs, whether there is an explicit connective be- conjunctions can be seen in Table 1, along with tween them or not. This makes shallow discourse thenumbersforsingle-labelconjunctions. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Baseline for only implicit relations, only relationsholdbetweensisterVPswithnoexplicit explicitrelationsandthetotaldataset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Tokensnotanno- (TEMPORAL.ASYNCHRONOUS.PRECEDENCE). tatedwithatleastoneofthesesenseshavebeenre- We apply these rules on the same dataset that is moved,andmulti-labeltokenswithonlyonesense used for the classiﬁcation approach, with certain shown in Table 3 have been included as single- senses removed, as will be explained in Section label tokens. As a result 2446 conjunctions can 4.2. The contribution of negation can be seen (5) ... Deltaissued2.5millionsharesofcommonstock by comparing rows 7 and 8 in Table 3. For toSwissair andrepurchased1.1millionsharesfor explicit relations, negation improves recall while use in a company employee stock ownership plan. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  (This is a likely conse- of the experiments. Table 4 displays all of the re- quence of the content of the WSJ corpus.) An sults. While the syntactic features only increase the recall of the ’Explicit’ classiﬁer, the perfor- example where the explicit sense is EXPAN- mance of the ’Implicit’ classiﬁer is considerably SION.CONJUNCTION and the implicit sense is improved when using the PoS tags of the argu- TEMPORAL.ASYNCHRONOUS.PRECEDENCE,is: ments. </Extractive Summary>  </Table ID = 4>  <Table ID = 5>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  VerbNet and WordNet, are evaluated in combina- BrownCluster classes represent words as se- tionwiththeconnectives/discourseadverbialsfea- mantic clusters, through a hierarchical clustering tures. Table 5 shows that the ’Implicit’ classiﬁer approach using mutual information (Turian et al., proﬁts the most from the semantic features. This 2010). </Extractive Summary>  </Table ID = 5>  <Table ID = 6>  <Abstractive Summary> =  Table 6: Comparison of the two classiﬁers’ performance on features from the internal, external and combinedfeaturescope. </Abstractive Summary>  <Extractive Summary> =  isenoughorwhethersomeexternalfeaturescould contributetoabettersenseclassiﬁcation,acombi- (6) Itisalsopulling20peopleoutofPuertoRico,who nation of syntactic and semantic features is used werehelpingHurricaneHugovictims,andsending on the internal, external and combined feature- scope. The results in Table 6 indicate that the ar- 3http://nlp.stanford.edu/software/srparser.shtml 39Explicit Implicit Accuracy Prec. Recall F-m. </Extractive Summary>  </Table ID = 6>  <Table ID = 7>  <Abstractive Summary> =  Table 7: F-m. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The nal system this classiﬁer uses only these features. results from Table 8 show that the performance The ’Implicit’ classiﬁer uses the tfIdf weighted of a classiﬁer decreases in both experiments. The PoS trigrams and the tfIdf weighted Brown Clus- changesinspanandtheinclusion/exclusionofad- ter classes. </Extractive Summary>  </Table ID = 8>  </Paper ID = 1037> 

<Paper ID = 1038>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  asabasisofourclassiﬁer). A statistical analysis (see Table 2) showed that thecomputedvariationcoefﬁcientfortheselected It should be noted that in order to design the parameters ranges signiﬁcantly. The parameters models, the parameters that are normally dis- with correlation coefﬁcient over 50 % were ex- tributed in the corpus of truthful texts were em- cluded at the next stage (see Levitsky (2004); ployed. </Extractive Summary>  </Table ID = 2>  </Paper ID = 1038> 

<Paper ID = 1039>  <Table ID = 2>  <Abstractive Summary> =  Table 2: The accuracy of the resolver for ﬁnding cludefeatureswithafrequencysmallerthan0.5%, thecorrectantecedentofthepronounonthetrain- meaning that every feature should be attested at ing,developmentandtestset. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Theaccuracyandsizeforeachsubcorpus canbefoundinTable2. The weights of the logistic regression model in An important question is whether these results Table 1 predict the preferences the classiﬁer will are satisfactory. Our results are difﬁcult to com- showonexperimentaldata. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Human pronoun assignment versus the model’s predictions on Crawley et al. </Abstractive Summary>  <Extractive Summary> =  SarahvisitedCathyathomeandthenCharlesphoned heratwork. 4.1.2 Crawley’sUnambiguousItemswith SubjectAntecedent 4.2 Results Theambiguousitemshaveunambiguousversions: thereisonlyonepossibleantecedentthatmatches We can see in Table 3 that the model ﬁts human in gender. All 40 ambiguous items (see section preferences quite accurately. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Imagine that in a text the pronoun it has ity distribution from the scores we have by using to be resolved and that all preceding mentions in techniques inspired by Luo et al. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 1039> 

<Paper ID = 1040>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Preprocessing: H (set of tokens), C (set phenomena like click-baiting (Blom and Hansen, ofcontentwords),E (setofwikiﬁedentities) 2015). </Abstractive Summary>  <Extractive Summary> =  We ﬁrst extract phrases of thereforemeasurethepercentageofcontentwords following types: SUBJ-V, V-OBJ, ADV-V, ADJ- 67Table 2: Feature implementations and statistics on The Guardian. Notation is in Table 1. Measures: median and maximum values, prevalence (proportion of non-zero scores), and the Kruskall-Wallis test comparingthemanualgoldstandardtoautomaticextraction(*p<0.05,**p<0.01,***p<0.001). Times corpus. Notation is explained in Table 1. RandolphQuirk, SidneyGreenbaum, GeoffreyLeech, Reported measures: median and maximum val- andJanSvartvik. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  12http://www.theguardian.com/open-platform 68negative (Y/N) for the ﬁrst ﬁve news values (cf. lent news values and our approach using wikiﬁ- Table 3). For NV6:Uniqueness, annotators were cation proves very reliable. For exam- proves very reliable. This news values occurs in ple, comedy (E5 in Table 3) has positive senti- 35%ofheadlines. Thisisnotsurprising,consider- ment, butdoesnotevokepositiveemotion. tity might be missed (cf. E1 in Table 3, where NV3: Superlativeness is rare, but reliably ex- EE/Orange was missed and consequently both tracted. It is the least prevalent news value (be- Prominence and Proximity scores are zero). </Extractive Summary>  </Table ID = 3>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  how many times in a year an entity had ual annotation labels (Y/N) were signiﬁcant (cf. pageviewssigniﬁcantlyhigherthanitsaverage)is column KW in Table 2). These results indicate areliablefeature,currentburstsize(i.e. </Extractive Summary>  </Table ID = 2>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Feature extraction statistics on New York 172. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 1040> 

<Paper ID = 1041>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Theworstpredictionwas GroupV avg.no.ofwordspersentence; 50-55% avg.sentencelengthratio; as low as 48% for the percentage4 of nouns in an percentageofdiscoursemarkers; argument(numberofnouns/numberofwords)and no.ofrudewords; capscount;digits;percentsigns; the highest one was 75% for the length ratio (in NEratio;percentageofMCnouns, words). lemmas,stemsandbigrams; ISMCbigramsratio;percentageof Thefeatureswerethendividedinto7groupsas unusualwords shown in Table 1, grouping the ones with sim- GroupVI avg.lengthofword;readability; 50% no.ofhyperlinks;percentageof ilar accuracy together within 5% ranges, start- adjectivesandadverbs; ing at 45%. In the three highest groups, rang- avg.rarityofwords GroupVII punctuationcount; <50% ing from 60% to 75%, were six features, namely: percentageofnounsandpronouns; the length ratios (in words and in sentences), the percentageofMCtrigrams; ISMCtrigramsratio IS MC lemmas, stems and nouns ratios, and per- centage of long words (minimum 10 characters). </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  gallonsofbottledwaterwereshippedinfromFiji in 2006, producing about 2,500 tons of global 4 Results warmingpollution. 4.1 Evaluation (2) Bottled water is not strictly regulated Table 2 shows that combining features that inde- while tap water is, so you have no idea what you pendentlyhaveasimilaraccuracy, canachievean aredrinkingwhenyoudrinkbottledwater. upto9%higheraccuracywhenusedtogether. </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Result Comparison between our Feed- occurwithoneanotherinagivencorpus. </Abstractive Summary>  <Extractive Summary> =  As a consequence that this method is not domain inde- II increases accuracy even further, seemingly be- pendentanddependsonthefeaturesofthecorpora cause arguments that are longer, contain the most thatwereusedforobtainingthewordembeddings. common words and nouns of the debate, as well Table 3 shows our 5-feature8 vector results us- as long words are the most convincing. The aver- ingourFFNNcomparedtotheSVMandBLSTM ageresultoftheGroupsI,IIandIIIis76.57%. </Extractive Summary>  </Table ID = 3>  </Paper ID = 1041> 

<Paper ID = 1042>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Hashtags and users employed in the datasetElecciones. </Abstractive Summary>  <Extractive Summary> =  In the following cess. Table 2 shows the exact terms that were ex- points we summarise the information about these ploredforextractingthetweets. Summingup,this datasets. </Extractive Summary>  </Table ID = 2>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  is available in the following url: https:// Forstudyingdistributionsofhashtagsmentions s3.amazonaws.com/cosmos.datasets/ in Twitter conversations, it is important if we are hispatweets-populated.zip. abletodetectandcorrectinsomewaythiskindof In Table 1 we include some information about problems in hashtag identiﬁcation. One possibil- thisdataset. </Extractive Summary>  </Table ID = 1>  <Table ID = 3>  <Abstractive Summary> =  Table 3: String metrics between hashtags for dif- hashtagswithrespecttoZipf’slaw. </Abstractive Summary>  <Extractive Summary> =  thatarenotverysimilaramongthem. Table 3 shows an example of the measures 4 Experiments of the string distances applied to some hashtags. Notethatameasureof1indicatesclosenesssimi- Afterthecorrectidentiﬁcationofhashtags,inthis larityand0meansnosimilarityatall. </Extractive Summary>  </Table ID = 3>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Percentage of each FSD First Signiﬁcant Digit for the seven countries of the dataset His- patweets. </Abstractive Summary>  <Extractive Summary> =  4.2 Benford’slaw AfteranalysingtheZipf’slawonthetwodatasets with succesful results, here we study if the distri- butions of the frequency of hashtags follow Ben- ford’slaw. 4.2.1 Dataset Hispatweets Table 5 shows the percentage of each FSD (First Signiﬁcant Digit) for the seven countries of the dataset. We also include in the ﬁrst row the the- oreticalpercentageforeachFSDaccordingtothe Benford’s law. </Extractive Summary>  </Table ID = 5>  <Table ID = 7>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  4.2.2 Dataset Elecciones Accordingto(Nigrini,2012),wecanassume that a distribution does not follow Benford’s Wealsohaveasimilarresultinthecaseofdataset law for the ﬁrst digit (FSD) if χ2 > 15.507 Elecciones. Table 7 includes the computed distri- (conﬁdence 95%), and if χ2 > 20.090 (con- bution of FSD without ﬁltering hashtags, and ap- ﬁdence99%). plyingtheﬁlterbasedonJaro-Winklerdistancefor different values of α. </Extractive Summary>  </Table ID = 7>  <Table ID = 8>  <Abstractive Summary> =  Table 8: Range of critical values and the corre- thecomparisonwithrespecttoBenford’sLaw. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 8>  <Table ID = 9>  <Abstractive Summary> =  Table 9: Pearson Correlation, χ2 statistics and hashtags achieve a high number of mentions, and Mean absolute deviation (MAD) between ob- mostofthemlackofimpactwithfewrepetitions. </Abstractive Summary>  <Extractive Summary> =  In fact, we detected an irregular number of hashtags with just one mention. Many of these If we analyse the results of Table 9, we can ob- hashtags are spelling mistakes of Twitter users. servethatforallcasescorrelationobtainhighval- In order to mitigate this dispersion, we deﬁned a ues (greater than0.92). </Extractive Summary>  </Table ID = 9>  </Paper ID = 1042> 

<Paper ID = 1043>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Average Score for each aspect and ﬁnal written BP (Martins, 2000); then we use the gradeinUOLDataset number of demonstrative pronouns and the Aspect AverageScore number of demonstrative pronouns per num- FormalLanguage 1.1 beroftokens. </Abstractive Summary>  <Extractive Summary> =  Both features were also divided by Table1,andtheﬁnalscoreisthesumofallaspects the number of tokens in an essay; then scores. Table 2 depicts the average score assign weemployedfourfeaturesforgrammar by humans for each aspect and ﬁnal grade in our and spelling errors. To evaluate style in dataset. </Extractive Summary>  </Table ID = 2>  <Table ID = 5>  <Abstractive Summary> =  Table 5: Kappa values for each grade aspect after the more important is f for this model. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 5>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Then the strategy randomly lowestkappavaluebetweenaspects. However,we selectsinstancesfromeveryclassG 6= G and can draw some conclusions from Table 3.2. For max replicatessuchinstancesintotrainingdatasets,un- instance, Flesh score affected expressively kappa til the size of every class G 6= G be equal the value. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  Also, we observe that current features are max sizeofG . not enough for Understanding the task model, max Table 4 describes the results using quadratic therefore we will implement new features related weightedkappabeforetheoversampling. Weexe- tothisaspect. </Extractive Summary>  </Table ID = 4>  </Paper ID = 1043> 

<Paper ID = 1044>  <Table ID = 1>  <Abstractive Summary> =  Table 1: Instances of MWEs pulled form the cor- perceivedsuperiorityofword2vecovertraditional pus. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 1>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Results for the word2vec vector space sions: IdentifyingandExploitingUnderlyingProp- withacontextwindowofsize10. </Abstractive Summary>  <Extractive Summary> =  Thensin- search engines seems to be more restricted than a gular value decomposition (SVD)13 was applied fewyearsago. tothematrixtoreducethedimensionsoftheword Table 4 shows the results for the three vector vectorsto300. lexicons with a context window of 6. </Extractive Summary>  </Table ID = 4>  <Table ID = 3>  <Abstractive Summary> =  Table 3: Results for the word2vec vector space table 3 suggest they are not. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 3>  </Paper ID = 1044> 

<Paper ID = 1045>  <Table ID = 1>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The reason for this unex- DS + + weirdnessratio,corpComLL,TFITF pectedclassiﬁcationmightresultfromthecontext AM - + PMI,LocalMI,Chi2 in which the study is conducted: there might be Count + + frequency,wordlength Ling + + POSpattern papers which are limited to Czech data or only to Comp - + 0-uni-POS,1-bi-tf-idf newspapertexts. The construction of the whole decision tree re- Table1: OverviewofFeatureClasses veals that the classiﬁer tries to identify clear-cut sets of terms using decision thresholds with ex- An overview of the classes is given in Table 1. treme values. </Extractive Summary>  </Table ID = 1>  <Table ID = 2>  <Abstractive Summary> =  Table 2: Precision, Recall and F1-Scores for Fea- ond or the third component (in case of trigrams) tureClasses appears in both classes. </Abstractive Summary>  <Extractive Summary> =  The perfor- 2). All these improvements are signiﬁcant, 5 mances of the different classes for unigrams, bi- except for the comparison of the overall model grams and trigrams are shown in Table 2. When for trigrams to the model of its best-working considering the overall results (F1-score), it is class (features of components). </Extractive Summary>  </Table ID = 2>  <Table ID = 3>  <Abstractive Summary> =  </Abstractive Summary>  <Extractive Summary> =  The termhood,resultsdecrease. results are shown in Table 3. All systems which combine features outperform the baselines. </Extractive Summary>  </Table ID = 3>  <Table ID = 4>  <Abstractive Summary> =  Table 4: Results for identical elements for differ- ﬁer to model term recognition with focus on the entcomponentsinterm-andnon-termclass distributionoftermsandofitscomponentsintext. </Abstractive Summary>  <Extractive Summary> =  </Extractive Summary>  </Table ID = 4>  </Paper ID = 1045> 

<Paper ID = 1046>  </Paper ID = 1046> 

<Paper ID = 1047>  </Paper ID = 1047> 

<Paper ID = 1048>  </Paper ID = 1048> 

<Paper ID = 1049>  </Paper ID = 1049> 

<Paper ID = 1050>  </Paper ID = 1050> 

<Paper ID = 1051>  </Paper ID = 1051> 

